{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02f84fe0-f591-48d2-a81f-3a57bc2c4fad",
   "metadata": {},
   "source": [
    "# Exemple consumer Kafka avec PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dd96ef-0f0e-4dde-8be7-8620a81d9709",
   "metadata": {},
   "source": [
    " ## Consumer avec Spark\n",
    "\n",
    "Apache Spark est un moteur de traitement distribué écrit en Scala, langage s’exécutant sur la JVM (Java Virtual Machine).\n",
    "Il permet de manipuler de grands volumes de données en mémoire, en mode batch ou streaming.\n",
    "\n",
    "PySpark est l’interface Python officielle de Spark.\n",
    "Elle agit comme un pont : les instructions Python sont traduites en requêtes Spark exécutées sur la JVM.\n",
    "Ainsi, le développeur Python bénéficie de la puissance de Spark sans écrire de code Scala ou Java."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a40e615-fcbc-4cc1-94db-d179533d212c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import time\n",
    "\n",
    "# ... (Constantes et dépendances) ...\n",
    "SPARK_VERSION = \"4.0.1\" \n",
    "BROKER_ADDRESS = \"broker:29092\"\n",
    "TOPIC = \"test\" \n",
    "STARTING_OFFSET_MODE = \"earliest\" \n",
    "TABLE_NAME = \"kafka_data_stream\" \n",
    "KAFKA_CONNECTOR_VERSION = \"4.0.0\" \n",
    "KAFKA_CONNECTOR_PACKAGE = f\"org.apache.spark:spark-sql-kafka-0-10_2.13:{KAFKA_CONNECTOR_VERSION}\"\n",
    "COMMON_KAFKA_PACKAGE = \"org.apache.kafka:kafka-clients:3.7.0\" \n",
    "\n",
    "# -------- Dépendances --------------------\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaConsumerStream\") \\\n",
    "    .config(\"spark.jars.packages\", f\"{KAFKA_CONNECTOR_PACKAGE},{COMMON_KAFKA_PACKAGE}\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# *** AJOUT DE LA LOGIQUE D'ARRÊT ***\n",
    "for s in spark.streams.active:\n",
    "    if s.name == TABLE_NAME and s.isActive:\n",
    "        print(f\"Arrêt du stream existant: {TABLE_NAME}\")\n",
    "        s.stop()\n",
    "        # Optionnel: attendre un court instant pour la propreté\n",
    "        time.sleep(1) \n",
    "# *** FIN DE LA LOGIQUE D'ARRÊT ***\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# -------- Connexion à Kafka ----------------\n",
    "print(f\"Démarrage de la lecture depuis le topic '{TOPIC}' avec l'offset: {STARTING_OFFSET_MODE}\")\n",
    "\n",
    "# ... (Définition du DataFrame df) ...\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", BROKER_ADDRESS) \\\n",
    "    .option(\"startingOffsets\", STARTING_OFFSET_MODE) \\\n",
    "    .option(\"subscribe\", TOPIC) \\\n",
    "    .load() \\\n",
    "    .selectExpr(\n",
    "        \"CAST(key AS STRING) AS key\", \n",
    "        \"CAST(value AS STRING) AS value\",\n",
    "        \"topic\",\n",
    "        \"partition\",\n",
    "        \"offset\",\n",
    "        \"timestamp\"\n",
    "    )\n",
    "\n",
    "# Affichage des résultats dans une TABLE EN MÉMOIRE\n",
    "query = df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(TABLE_NAME) \\\n",
    "    .start()\n",
    "\n",
    "print(f\"✅ Le stream est démarré et actif. Nom: {query.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a151684-28ad-49a2-a457-7b1b1d1718cf",
   "metadata": {},
   "source": [
    "## Boucle pour affichage\n",
    "\n",
    "Affichage du dataframe précédent.\n",
    "\n",
    "5 boucles de 3 secondes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3237bf-9347-4eec-b386-eb04853409be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule pour afficher les résultats\n",
    "# La commande 'display' est souvent disponible dans les environnements Jupyter/Databricks\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "\n",
    "# Boucle pour rafraîchir l'affichage\n",
    "for x in range(5): # Affichera pendant 30 cycles (ex: 90 secondes)\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # Requête pour lire la table mise à jour par le stream\n",
    "    results_df = spark.sql(f\"SELECT * FROM {TABLE_NAME}\")\n",
    "    \n",
    "    # Affichage des résultats (utilise show() pour une sortie texte simple)\n",
    "    results_df.show(truncate=False) \n",
    "    \n",
    "    # Si vous voulez afficher sous forme de tableau interactif (selon votre environnement)\n",
    "    # display(results_df) \n",
    "    \n",
    "    time.sleep(3) # Attendre 3 secondes\n",
    "\n",
    "# Affichage de fin\n",
    "print(\"Affichage du flux terminé ou arrêté.\")\n",
    "query.stop() # Arrête le stream proprement"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
