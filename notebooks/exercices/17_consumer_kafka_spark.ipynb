{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c756abf",
   "metadata": {},
   "source": [
    "# Exercice 17 - Consumer Kafka avec Spark\n",
    "\n",
    "## Objectifs\n",
    "- Lire des messages Kafka avec Spark Structured Streaming\n",
    "- Parser des messages JSON\n",
    "- Traiter les donnees en streaming\n",
    "- Ecrire les resultats dans MinIO\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbc2e4c",
   "metadata": {},
   "source": [
    "## 1. Architecture Consumer Spark\n",
    "\n",
    "```\n",
    "+------------------------------------------------------------------+\n",
    "|               SPARK STRUCTURED STREAMING + KAFKA                 |\n",
    "+------------------------------------------------------------------+\n",
    "|                                                                  |\n",
    "|   KAFKA                  SPARK                   DESTINATION    |\n",
    "|                                                                  |\n",
    "|  +--------+         +-------------+            +----------+     |\n",
    "|  | Topic  |         |             |            |          |     |\n",
    "|  |--------|  read   |  DataFrame  |   write    |  MinIO   |     |\n",
    "|  | Part 0 |-------->|  Streaming  |----------->|  Parquet |     |\n",
    "|  | Part 1 |         |             |            |          |     |\n",
    "|  | Part 2 |         +------+------+            +----------+     |\n",
    "|  +--------+                |                                    |\n",
    "|                            v                                    |\n",
    "|                     +------+------+            +----------+     |\n",
    "|                     |             |            |          |     |\n",
    "|                     | Aggregation |----------->|  Console |     |\n",
    "|                     |             |            |          |     |\n",
    "|                     +-------------+            +----------+     |\n",
    "|                                                                  |\n",
    "+------------------------------------------------------------------+\n",
    "\n",
    "Modes de sortie :\n",
    "- append   : Ajoute uniquement les nouvelles lignes\n",
    "- complete : Reecrit toute la table (pour aggregations)\n",
    "- update   : Met a jour les lignes modifiees\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db09dd02",
   "metadata": {},
   "source": [
    "## 2. Configuration Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dcb36c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m      7\u001b[39m packages = \u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m.join([\n\u001b[32m      8\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33morg.apache.spark:spark-sql-kafka-0-10_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSCALA_SUFFIX\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSPARK_VER\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33morg.apache.spark:spark-token-provider-kafka-0-10_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSCALA_SUFFIX\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSPARK_VER\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     10\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33morg.apache.hadoop:hadoop-aws:3.4.1\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     11\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcom.amazonaws:aws-java-sdk-bundle:1.12.262\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     12\u001b[39m ])\n\u001b[32m     14\u001b[39m os.environ[\u001b[33m\"\u001b[39m\u001b[33mPYSPARK_SUBMIT_ARGS\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m--packages \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpackages\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m pyspark-shell\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     16\u001b[39m spark = (\n\u001b[32m     17\u001b[39m     \u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mKafkaConsumer\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m )\n\u001b[32m     22\u001b[39m spark.sparkContext.setLogLevel(\u001b[33m\"\u001b[39m\u001b[33mWARN\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSpark:\u001b[39m\u001b[33m\"\u001b[39m, spark.version)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/spark/python/pyspark/sql/session.py:556\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    554\u001b[39m     sparkConf.set(key, value)\n\u001b[32m    555\u001b[39m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m sc = \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    558\u001b[39m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m    559\u001b[39m session = SparkSession(sc, options=\u001b[38;5;28mself\u001b[39m._options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/spark/python/pyspark/core/context.py:523\u001b[39m, in \u001b[36mSparkContext.getOrCreate\u001b[39m\u001b[34m(cls, conf)\u001b[39m\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    522\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    525\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext._active_spark_context\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/spark/python/pyspark/core/context.py:205\u001b[39m, in \u001b[36mSparkContext.__init__\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway.gateway_parameters.auth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    201\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    202\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is not allowed as it is a security risk.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    203\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    207\u001b[39m     \u001b[38;5;28mself\u001b[39m._do_init(\n\u001b[32m    208\u001b[39m         master,\n\u001b[32m    209\u001b[39m         appName,\n\u001b[32m   (...)\u001b[39m\u001b[32m    219\u001b[39m         memory_profiler_cls,\n\u001b[32m    220\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/spark/python/pyspark/core/context.py:444\u001b[39m, in \u001b[36mSparkContext._ensure_initialized\u001b[39m\u001b[34m(cls, instance, gateway, conf)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    443\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext._gateway:\n\u001b[32m--> \u001b[39m\u001b[32m444\u001b[39m         SparkContext._gateway = gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    445\u001b[39m         SparkContext._jvm = SparkContext._gateway.jvm\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/spark/python/pyspark/java_gateway.py:108\u001b[39m, in \u001b[36mlaunch_gateway\u001b[39m\u001b[34m(conf, popen_kwargs)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[39;00m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m proc.poll() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(conn_info_file):\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(conn_info_file):\n\u001b[32m    111\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[32m    112\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mJAVA_GATEWAY_EXITED\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    113\u001b[39m         messageParameters={},\n\u001b[32m    114\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "SPARK_VER = \"4.0.1\"\n",
    "SCALA_SUFFIX = \"2.13\"\n",
    "\n",
    "packages = \",\".join([\n",
    "    f\"org.apache.spark:spark-sql-kafka-0-10_{SCALA_SUFFIX}:{SPARK_VER}\",\n",
    "    f\"org.apache.spark:spark-token-provider-kafka-0-10_{SCALA_SUFFIX}:{SPARK_VER}\",\n",
    "    \"org.apache.hadoop:hadoop-aws:3.4.1\",\n",
    "    \"com.amazonaws:aws-java-sdk-bundle:1.12.262\"\n",
    "])\n",
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = f\"--packages {packages} pyspark-shell\"\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"KafkaConsumer\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(\"Spark:\", spark.version)\n",
    "print(\"Scala:\", spark.sparkContext._jvm.scala.util.Properties.versionNumberString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b81723a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration prete\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "KAFKA_BROKER = \"broker:29092\"\n",
    "MINIO_BUCKET = \"s3a://bronze\"\n",
    "\n",
    "print(\"Configuration prete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ac27c5",
   "metadata": {},
   "source": [
    "## 3. Lecture batch depuis Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74ffdba9",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o36.count.\n: java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.\n\tat java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:396)\n\tat java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2073)\n\tat org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)\n\tat org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:65)\n\tat org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:64)\n\tat org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:101)\n\tat org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:112)\n\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchPartitionOffsets(KafkaOffsetReaderAdmin.scala:133)\n\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.getOffsetRangesFromUnresolvedOffsets(KafkaOffsetReaderAdmin.scala:382)\n\tat org.apache.spark.sql.kafka010.KafkaRelation.buildScan(KafkaRelation.scala:69)\n\tat org.apache.spark.sql.execution.datasources.DataSourceStrategy$.apply(DataSourceStrategy.scala:396)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$1(QueryPlanner.scala:63)\n\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:72)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n\tat scala.collection.IterableOnceOps.foldLeft(IterableOnce.scala:727)\n\tat scala.collection.IterableOnceOps.foldLeft$(IterableOnce.scala:721)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1306)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:72)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n\tat scala.collection.IterableOnceOps.foldLeft(IterableOnce.scala:727)\n\tat scala.collection.IterableOnceOps.foldLeft$(IterableOnce.scala:721)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1306)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:72)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n\tat scala.collection.IterableOnceOps.foldLeft(IterableOnce.scala:727)\n\tat scala.collection.IterableOnceOps.foldLeft$(IterableOnce.scala:721)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1306)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:72)\n\tat org.apache.spark.sql.execution.QueryExecution$.createSparkPlan(QueryExecution.scala:593)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazySparkPlan$2(QueryExecution.scala:223)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazySparkPlan$1(QueryExecution.scala:223)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:227)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyExecutedPlan$2(QueryExecution.scala:238)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyExecutedPlan$1(QueryExecution.scala:238)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:248)\n\tat org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:297)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:344)\n\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:312)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:149)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n\tat org.apache.spark.sql.classic.Dataset.count(Dataset.scala:1499)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:396)\n\t\tat java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2073)\n\t\tat org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)\n\t\tat org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:65)\n\t\tat org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:64)\n\t\tat org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:101)\n\t\tat org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:112)\n\t\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchPartitionOffsets(KafkaOffsetReaderAdmin.scala:133)\n\t\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.getOffsetRangesFromUnresolvedOffsets(KafkaOffsetReaderAdmin.scala:382)\n\t\tat org.apache.spark.sql.kafka010.KafkaRelation.buildScan(KafkaRelation.scala:69)\n\t\tat org.apache.spark.sql.execution.datasources.DataSourceStrategy$.apply(DataSourceStrategy.scala:396)\n\t\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$1(QueryPlanner.scala:63)\n\t\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\n\t\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\n\t\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\n\t\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\t\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:72)\n\t\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n\t\tat scala.collection.IterableOnceOps.foldLeft(IterableOnce.scala:727)\n\t\tat scala.collection.IterableOnceOps.foldLeft$(IterableOnce.scala:721)\n\t\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1306)\n\t\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n\t\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\n\t\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\n\t\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\t\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:72)\n\t\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n\t\tat scala.collection.IterableOnceOps.foldLeft(IterableOnce.scala:727)\n\t\tat scala.collection.IterableOnceOps.foldLeft$(IterableOnce.scala:721)\n\t\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1306)\n\t\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n\t\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\n\t\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\n\t\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\t\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:72)\n\t\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n\t\tat scala.collection.IterableOnceOps.foldLeft(IterableOnce.scala:727)\n\t\tat scala.collection.IterableOnceOps.foldLeft$(IterableOnce.scala:721)\n\t\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1306)\n\t\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n\t\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\n\t\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\n\t\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\t\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:72)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.createSparkPlan(QueryExecution.scala:593)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazySparkPlan$2(QueryExecution.scala:223)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazySparkPlan$1(QueryExecution.scala:223)\n\t\tat scala.util.Try$.apply(Try.scala:217)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\t\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:227)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyExecutedPlan$2(QueryExecution.scala:238)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyExecutedPlan$1(QueryExecution.scala:238)\n\t\tat scala.util.Try$.apply(Try.scala:217)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 32 more\nCaused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Lire les messages existants (mode batch)\u001b[39;00m\n\u001b[32m      2\u001b[39m df_kafka = spark.read \\\n\u001b[32m      3\u001b[39m     .format(\u001b[33m\"\u001b[39m\u001b[33mkafka\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m      4\u001b[39m     .option(\u001b[33m\"\u001b[39m\u001b[33mkafka.bootstrap.servers\u001b[39m\u001b[33m\"\u001b[39m, KAFKA_BROKER) \\\n\u001b[32m      5\u001b[39m     .option(\u001b[33m\"\u001b[39m\u001b[33msubscribe\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcommandes-json\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m      6\u001b[39m     .option(\u001b[33m\"\u001b[39m\u001b[33mstartingOffsets\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mearliest\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m      7\u001b[39m     .load()\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMessages lus: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdf_kafka\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m df_kafka.printSchema()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/spark/python/pyspark/sql/classic/dataframe.py:439\u001b[39m, in \u001b[36mDataFrame.count\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    438\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:282\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpy4j\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    284\u001b[39m     converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o36.count.\n: java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.\n\tat java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:396)\n\tat java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2073)\n\tat org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)\n\tat org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:65)\n\tat org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:64)\n\tat org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:101)\n\tat org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:112)\n\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchPartitionOffsets(KafkaOffsetReaderAdmin.scala:133)\n\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.getOffsetRangesFromUnresolvedOffsets(KafkaOffsetReaderAdmin.scala:382)\n\tat org.apache.spark.sql.kafka010.KafkaRelation.buildScan(KafkaRelation.scala:69)\n\tat org.apache.spark.sql.execution.datasources.DataSourceStrategy$.apply(DataSourceStrategy.scala:396)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$1(QueryPlanner.scala:63)\n\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:72)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n\tat scala.collection.IterableOnceOps.foldLeft(IterableOnce.scala:727)\n\tat scala.collection.IterableOnceOps.foldLeft$(IterableOnce.scala:721)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1306)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:72)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n\tat scala.collection.IterableOnceOps.foldLeft(IterableOnce.scala:727)\n\tat scala.collection.IterableOnceOps.foldLeft$(IterableOnce.scala:721)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1306)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:72)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n\tat scala.collection.IterableOnceOps.foldLeft(IterableOnce.scala:727)\n\tat scala.collection.IterableOnceOps.foldLeft$(IterableOnce.scala:721)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1306)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:72)\n\tat org.apache.spark.sql.execution.QueryExecution$.createSparkPlan(QueryExecution.scala:593)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazySparkPlan$2(QueryExecution.scala:223)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazySparkPlan$1(QueryExecution.scala:223)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:227)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyExecutedPlan$2(QueryExecution.scala:238)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyExecutedPlan$1(QueryExecution.scala:238)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:248)\n\tat org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:297)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:344)\n\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:312)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:149)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n\tat org.apache.spark.sql.classic.Dataset.count(Dataset.scala:1499)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:396)\n\t\tat java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2073)\n\t\tat org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)\n\t\tat org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:65)\n\t\tat org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:64)\n\t\tat org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:101)\n\t\tat org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:112)\n\t\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchPartitionOffsets(KafkaOffsetReaderAdmin.scala:133)\n\t\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.getOffsetRangesFromUnresolvedOffsets(KafkaOffsetReaderAdmin.scala:382)\n\t\tat org.apache.spark.sql.kafka010.KafkaRelation.buildScan(KafkaRelation.scala:69)\n\t\tat org.apache.spark.sql.execution.datasources.DataSourceStrategy$.apply(DataSourceStrategy.scala:396)\n\t\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$1(QueryPlanner.scala:63)\n\t\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\n\t\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\n\t\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\n\t\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\t\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:72)\n\t\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n\t\tat scala.collection.IterableOnceOps.foldLeft(IterableOnce.scala:727)\n\t\tat scala.collection.IterableOnceOps.foldLeft$(IterableOnce.scala:721)\n\t\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1306)\n\t\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n\t\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\n\t\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\n\t\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\t\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:72)\n\t\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n\t\tat scala.collection.IterableOnceOps.foldLeft(IterableOnce.scala:727)\n\t\tat scala.collection.IterableOnceOps.foldLeft$(IterableOnce.scala:721)\n\t\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1306)\n\t\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n\t\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\n\t\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\n\t\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\t\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:72)\n\t\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n\t\tat scala.collection.IterableOnceOps.foldLeft(IterableOnce.scala:727)\n\t\tat scala.collection.IterableOnceOps.foldLeft$(IterableOnce.scala:721)\n\t\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1306)\n\t\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n\t\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\n\t\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\n\t\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\t\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:72)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.createSparkPlan(QueryExecution.scala:593)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazySparkPlan$2(QueryExecution.scala:223)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazySparkPlan$1(QueryExecution.scala:223)\n\t\tat scala.util.Try$.apply(Try.scala:217)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\t\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:227)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyExecutedPlan$2(QueryExecution.scala:238)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyExecutedPlan$1(QueryExecution.scala:238)\n\t\tat scala.util.Try$.apply(Try.scala:217)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 32 more\nCaused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.\n"
     ]
    }
   ],
   "source": [
    "# Lire les messages existants (mode batch)\n",
    "df_kafka = spark.read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BROKER) \\\n",
    "    .option(\"subscribe\", \"commandes-json\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "print(f\"Messages lus: {df_kafka.count()}\")\n",
    "df_kafka.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fff336b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure des donnees Kafka\n",
    "# - key: cle du message (binaire)\n",
    "# - value: contenu du message (binaire)\n",
    "# - topic: nom du topic\n",
    "# - partition: numero de partition\n",
    "# - offset: position dans la partition\n",
    "# - timestamp: horodatage Kafka\n",
    "\n",
    "df_kafka.select(\n",
    "    col(\"key\").cast(\"string\").alias(\"key\"),\n",
    "    col(\"value\").cast(\"string\").alias(\"value\"),\n",
    "    \"topic\",\n",
    "    \"partition\",\n",
    "    \"offset\",\n",
    "    \"timestamp\"\n",
    ").show(5, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984354ee",
   "metadata": {},
   "source": [
    "## 4. Parser les messages JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a1fc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir le schema des commandes\n",
    "item_schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"unit_price\", DoubleType(), True),\n",
    "    StructField(\"subtotal\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "commande_schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"items\", ArrayType(item_schema), True),\n",
    "    StructField(\"total\", DoubleType(), True),\n",
    "    StructField(\"status\", StringType(), True)\n",
    "])\n",
    "\n",
    "print(\"Schema defini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3874579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parser le JSON\n",
    "df_commandes = df_kafka \\\n",
    "    .select(\n",
    "        col(\"key\").cast(\"string\").alias(\"customer_key\"),\n",
    "        from_json(col(\"value\").cast(\"string\"), commande_schema).alias(\"data\"),\n",
    "        \"partition\",\n",
    "        \"offset\",\n",
    "        col(\"timestamp\").alias(\"kafka_timestamp\")\n",
    "    ) \\\n",
    "    .select(\n",
    "        \"customer_key\",\n",
    "        \"data.*\",\n",
    "        \"partition\",\n",
    "        \"offset\",\n",
    "        \"kafka_timestamp\"\n",
    "    )\n",
    "\n",
    "df_commandes.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d9f79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les commandes\n",
    "df_commandes.select(\n",
    "    \"order_id\",\n",
    "    \"customer_id\",\n",
    "    \"total\",\n",
    "    \"status\",\n",
    "    \"partition\"\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d30103e",
   "metadata": {},
   "source": [
    "## 5. Analyser les items des commandes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171dec49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploser le tableau items\n",
    "df_items = df_commandes \\\n",
    "    .select(\n",
    "        \"order_id\",\n",
    "        \"customer_id\",\n",
    "        \"timestamp\",\n",
    "        explode(\"items\").alias(\"item\")\n",
    "    ) \\\n",
    "    .select(\n",
    "        \"order_id\",\n",
    "        \"customer_id\",\n",
    "        \"timestamp\",\n",
    "        col(\"item.product_id\"),\n",
    "        col(\"item.product_name\"),\n",
    "        col(\"item.quantity\"),\n",
    "        col(\"item.unit_price\"),\n",
    "        col(\"item.subtotal\")\n",
    "    )\n",
    "\n",
    "df_items.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010a3afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques par produit\n",
    "df_stats_produit = df_items.groupBy(\"product_name\").agg(\n",
    "    count(\"*\").alias(\"nb_ventes\"),\n",
    "    spark_sum(\"quantity\").alias(\"quantite_totale\"),\n",
    "    spark_sum(\"subtotal\").alias(\"ca_total\")\n",
    ").orderBy(col(\"ca_total\").desc())\n",
    "\n",
    "df_stats_produit.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c12c68",
   "metadata": {},
   "source": [
    "## 6. Sauvegarder dans MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c8e10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder les commandes en Parquet\n",
    "df_commandes \\\n",
    "    .drop(\"items\") \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(f\"{MINIO_BUCKET}/kafka/commandes\")\n",
    "\n",
    "print(\"Commandes sauvegardees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ea35f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder les items\n",
    "df_items \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(f\"{MINIO_BUCKET}/kafka/items\")\n",
    "\n",
    "print(\"Items sauvegardes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ab066a",
   "metadata": {},
   "source": [
    "## 7. Streaming en temps reel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e94c104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lire en mode streaming\n",
    "df_stream = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BROKER) \\\n",
    "    .option(\"subscribe\", \"commandes-json\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "print(\"Stream configure\")\n",
    "print(f\"Is streaming: {df_stream.isStreaming}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c179d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parser le stream\n",
    "df_stream_parsed = df_stream \\\n",
    "    .select(\n",
    "        from_json(col(\"value\").cast(\"string\"), commande_schema).alias(\"data\"),\n",
    "        \"timestamp\"\n",
    "    ) \\\n",
    "    .select(\n",
    "        \"data.order_id\",\n",
    "        \"data.customer_id\",\n",
    "        \"data.total\",\n",
    "        \"data.status\",\n",
    "        col(\"timestamp\").alias(\"kafka_time\")\n",
    "    )\n",
    "\n",
    "print(\"Stream parse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5125b0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ecrire dans la console (pour debug)\n",
    "# Attention: Executez ce code puis envoyez des messages Kafka depuis un autre notebook\n",
    "\n",
    "query = df_stream_parsed \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .trigger(processingTime=\"5 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"Stream demarre - envoyez des messages Kafka pour les voir\")\n",
    "print(\"Executez query.stop() pour arreter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb5d13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attendre quelques secondes puis arreter\n",
    "import time\n",
    "time.sleep(30)  # Attendre 30 secondes\n",
    "query.stop()\n",
    "print(\"Stream arrete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1311d635",
   "metadata": {},
   "source": [
    "## 8. Aggregations en streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3e20a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lire a nouveau le stream\n",
    "df_stream2 = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BROKER) \\\n",
    "    .option(\"subscribe\", \"commandes-json\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Parser\n",
    "df_agg = df_stream2 \\\n",
    "    .select(\n",
    "        from_json(col(\"value\").cast(\"string\"), commande_schema).alias(\"data\"),\n",
    "        \"timestamp\"\n",
    "    ) \\\n",
    "    .select(\n",
    "        \"data.customer_id\",\n",
    "        \"data.total\",\n",
    "        col(\"timestamp\").alias(\"event_time\")\n",
    "    )\n",
    "\n",
    "print(\"Stream prepare pour aggregation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d60a859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation par fenetre de temps\n",
    "df_windowed = df_agg \\\n",
    "    .groupBy(\n",
    "        window(col(\"event_time\"), \"1 minute\"),\n",
    "        \"customer_id\"\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"nb_commandes\"),\n",
    "        spark_sum(\"total\").alias(\"total_commandes\")\n",
    "    )\n",
    "\n",
    "print(\"Aggregation definie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78df2593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ecrire les aggregations\n",
    "query_agg = df_windowed \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"Stream aggregation demarre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164415c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attendre puis arreter\n",
    "time.sleep(20)\n",
    "query_agg.stop()\n",
    "print(\"Stream arrete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e097b3",
   "metadata": {},
   "source": [
    "## 9. Lire les logs applicatifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd0087e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema des logs\n",
    "log_schema = StructType([\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"level\", StringType(), True),\n",
    "    StructField(\"module\", StringType(), True),\n",
    "    StructField(\"message\", StringType(), True),\n",
    "    StructField(\"request_id\", StringType(), True),\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"duration_ms\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Lire les logs\n",
    "df_logs = spark.read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BROKER) \\\n",
    "    .option(\"subscribe\", \"logs-application\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load() \\\n",
    "    .select(\n",
    "        from_json(col(\"value\").cast(\"string\"), log_schema).alias(\"log\")\n",
    "    ) \\\n",
    "    .select(\"log.*\")\n",
    "\n",
    "print(f\"Logs lus: {df_logs.count()}\")\n",
    "df_logs.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37345c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques par niveau de log\n",
    "df_logs.groupBy(\"level\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d83d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques par module\n",
    "df_logs.groupBy(\"module\").agg(\n",
    "    count(\"*\").alias(\"nb_logs\"),\n",
    "    avg(\"duration_ms\").alias(\"duree_moyenne_ms\")\n",
    ").orderBy(col(\"nb_logs\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc31137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fermer Spark\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a48b56d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercice\n",
    "\n",
    "**Objectif** : Analyser les metriques Kafka\n",
    "\n",
    "**Consigne** :\n",
    "1. Lisez le topic \"metrics\" depuis Kafka\n",
    "2. Parsez les messages JSON\n",
    "3. Calculez les moyennes CPU et memoire par serveur\n",
    "4. Identifiez les serveurs les plus charges\n",
    "\n",
    "A vous de jouer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e7dc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Definir le schema des metriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5e54ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Lire le topic metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a9b199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculer les statistiques par serveur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee3f95d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Resume\n",
    "\n",
    "Dans ce notebook, vous avez appris :\n",
    "- Comment **lire des messages Kafka** avec Spark\n",
    "- Comment **parser des messages JSON** avec un schema\n",
    "- Comment utiliser le **mode batch** et le **mode streaming**\n",
    "- Comment faire des **aggregations en streaming** avec des fenetres de temps\n",
    "- Comment **sauvegarder les donnees** dans MinIO\n",
    "\n",
    "### Prochaine etape\n",
    "Dans le prochain notebook, nous approfondirons le streaming Spark avec des concepts avances."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
