{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c756abf",
   "metadata": {},
   "source": [
    "# Exercice 17 - Consumer Kafka avec Spark\n",
    "\n",
    "## Objectifs\n",
    "- Lire des messages Kafka avec Spark Structured Streaming\n",
    "- Parser des messages JSON\n",
    "- Traiter les donnees en streaming\n",
    "- Ecrire les resultats dans MinIO\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbc2e4c",
   "metadata": {},
   "source": [
    "## 1. Architecture Consumer Spark\n",
    "\n",
    "```\n",
    "+------------------------------------------------------------------+\n",
    "|               SPARK STRUCTURED STREAMING + KAFKA                 |\n",
    "+------------------------------------------------------------------+\n",
    "|                                                                  |\n",
    "|   KAFKA                  SPARK                   DESTINATION    |\n",
    "|                                                                  |\n",
    "|  +--------+         +-------------+            +----------+     |\n",
    "|  | Topic  |         |             |            |          |     |\n",
    "|  |--------|  read   |  DataFrame  |   write    |  MinIO   |     |\n",
    "|  | Part 0 |-------->|  Streaming  |----------->|  Parquet |     |\n",
    "|  | Part 1 |         |             |            |          |     |\n",
    "|  | Part 2 |         +------+------+            +----------+     |\n",
    "|  +--------+                |                                    |\n",
    "|                            v                                    |\n",
    "|                     +------+------+            +----------+     |\n",
    "|                     |             |            |          |     |\n",
    "|                     | Aggregation |----------->|  Console |     |\n",
    "|                     |             |            |          |     |\n",
    "|                     +-------------+            +----------+     |\n",
    "|                                                                  |\n",
    "+------------------------------------------------------------------+\n",
    "\n",
    "Modes de sortie :\n",
    "- append   : Ajoute uniquement les nouvelles lignes\n",
    "- complete : Reecrit toute la table (pour aggregations)\n",
    "- update   : Met a jour les lignes modifiees\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db09dd02",
   "metadata": {},
   "source": [
    "## 2. Configuration Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dcb36c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 4.0.1\n",
      "Scala: 2.13.16\n",
      "Packages: org.apache.spark:spark-sql-kafka-0-10_2.13:4.0.1,org.apache.spark:spark-token-provider-kafka-0-10_2.13:4.0.1,org.apache.hadoop:hadoop-aws:3.4.1,com.amazonaws:aws-java-sdk-bundle:1.12.262\n",
      "Configuration prête\n"
     ]
    }
   ],
   "source": [
    "# === Setup (Kafka + MinIO + Spark) ===\n",
    "# IMPORTANT: If you change packages below, restart the kernel before re-running.\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, to_timestamp, window, count, sum as spark_sum, avg, max as spark_max, explode, current_timestamp, round\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType\n",
    "\n",
    "# Align versions with the Docker image (Spark 4.x uses Scala 2.13)\n",
    "SPARK_VER = os.environ.get(\"SPARK_VER\", \"4.0.1\")\n",
    "SCALA_SUFFIX = os.environ.get(\"SCALA_SUFFIX\", \"2.13\")\n",
    "\n",
    "packages = \",\".join([\n",
    "    f\"org.apache.spark:spark-sql-kafka-0-10_{SCALA_SUFFIX}:{SPARK_VER}\",\n",
    "    f\"org.apache.spark:spark-token-provider-kafka-0-10_{SCALA_SUFFIX}:{SPARK_VER}\",\n",
    "    # S3A / MinIO\n",
    "    \"org.apache.hadoop:hadoop-aws:3.4.1\",\n",
    "    \"com.amazonaws:aws-java-sdk-bundle:1.12.262\",\n",
    "])\n",
    "\n",
    "# Ensure Spark loads the right connector JARs (avoids Scala/Spark mismatch errors)\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = f\"--packages {packages} pyspark-shell\"\n",
    "\n",
    "# (Re)create Spark session\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"KafkaConsumer\")\n",
    "    # MinIO (S3A) configuration\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin123\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    .config(\n",
    "        \"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
    "        \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\",\n",
    "    )\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(\"Spark version:\", spark.version)\n",
    "print(\"Scala:\", spark.sparkContext._jvm.scala.util.Properties.versionNumberString())\n",
    "print(\"Packages:\", packages)\n",
    "\n",
    "# Docker-internal Kafka listener (use this from inside the Jupyter container)\n",
    "KAFKA_BROKER = \"broker:29092\"\n",
    "\n",
    "# Default MinIO base path for this lab\n",
    "MINIO_BUCKET = \"s3a://bronze\"\n",
    "print(\"Configuration prête\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b81723a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration prete\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "KAFKA_BROKER = \"broker:29092\"\n",
    "MINIO_BUCKET = \"s3a://bronze\"\n",
    "\n",
    "print(\"Configuration prete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ac27c5",
   "metadata": {},
   "source": [
    "## 3. Lecture batch depuis Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74ffdba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages lus: 66\n",
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lire les messages existants (mode batch)\n",
    "df_kafka = spark.read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BROKER) \\\n",
    "    .option(\"subscribe\", \"commandes-json\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "print(f\"Messages lus: {df_kafka.count()}\")\n",
    "df_kafka.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fff336b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------------------------------------+--------------+---------+------+-----------------------+\n",
      "|     key|                                             value|         topic|partition|offset|              timestamp|\n",
      "+--------+--------------------------------------------------+--------------+---------+------+-----------------------+\n",
      "|CUST-016|{\"order_id\": 35924, \"customer_id\": \"CUST-016\", ...|commandes-json|        0|     0|2026-01-18 10:08:23.143|\n",
      "|CUST-013|{\"order_id\": 29067, \"customer_id\": \"CUST-013\", ...|commandes-json|        0|     1|2026-01-18 10:08:23.146|\n",
      "|CUST-017|{\"order_id\": 91880, \"customer_id\": \"CUST-017\", ...|commandes-json|        0|     2|2026-01-18 10:08:23.147|\n",
      "|CUST-013|{\"order_id\": 34621, \"customer_id\": \"CUST-013\", ...|commandes-json|        0|     3|2026-01-18 10:08:23.147|\n",
      "|CUST-020|{\"order_id\": 11525, \"customer_id\": \"CUST-020\", ...|commandes-json|        0|     4|2026-01-18 10:08:23.148|\n",
      "+--------+--------------------------------------------------+--------------+---------+------+-----------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Structure des donnees Kafka\n",
    "# - key: cle du message (binaire)\n",
    "# - value: contenu du message (binaire)\n",
    "# - topic: nom du topic\n",
    "# - partition: numero de partition\n",
    "# - offset: position dans la partition\n",
    "# - timestamp: horodatage Kafka\n",
    "\n",
    "df_kafka.select(\n",
    "    col(\"key\").cast(\"string\").alias(\"key\"),\n",
    "    col(\"value\").cast(\"string\").alias(\"value\"),\n",
    "    \"topic\",\n",
    "    \"partition\",\n",
    "    \"offset\",\n",
    "    \"timestamp\"\n",
    ").show(5, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984354ee",
   "metadata": {},
   "source": [
    "## 4. Parser les messages JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01a1fc64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema defini\n"
     ]
    }
   ],
   "source": [
    "# Definir le schema des commandes\n",
    "item_schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"unit_price\", DoubleType(), True),\n",
    "    StructField(\"subtotal\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "commande_schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"items\", ArrayType(item_schema), True),\n",
    "    StructField(\"total\", DoubleType(), True),\n",
    "    StructField(\"status\", StringType(), True)\n",
    "])\n",
    "\n",
    "print(\"Schema defini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3874579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_key: string (nullable = true)\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- items: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- product_id: integer (nullable = true)\n",
      " |    |    |-- product_name: string (nullable = true)\n",
      " |    |    |-- quantity: integer (nullable = true)\n",
      " |    |    |-- unit_price: double (nullable = true)\n",
      " |    |    |-- subtotal: double (nullable = true)\n",
      " |-- total: double (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- kafka_timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parser le JSON\n",
    "df_commandes = df_kafka \\\n",
    "    .select(\n",
    "        col(\"key\").cast(\"string\").alias(\"customer_key\"),\n",
    "        from_json(col(\"value\").cast(\"string\"), commande_schema).alias(\"data\"),\n",
    "        \"partition\",\n",
    "        \"offset\",\n",
    "        col(\"timestamp\").alias(\"kafka_timestamp\")\n",
    "    ) \\\n",
    "    .select(\n",
    "        \"customer_key\",\n",
    "        \"data.*\",\n",
    "        \"partition\",\n",
    "        \"offset\",\n",
    "        \"kafka_timestamp\"\n",
    "    )\n",
    "\n",
    "df_commandes.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8d9f79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+-------+---------+\n",
      "|order_id|customer_id|  total| status|partition|\n",
      "+--------+-----------+-------+-------+---------+\n",
      "|   35924|   CUST-016|1409.91|created|        0|\n",
      "|   29067|   CUST-013| 539.94|created|        0|\n",
      "|   91880|   CUST-017|5239.92|created|        0|\n",
      "|   34621|   CUST-013| 359.96|created|        0|\n",
      "|   11525|   CUST-020|1599.92|created|        0|\n",
      "|   64269|   CUST-002|3249.92|created|        0|\n",
      "|   89777|   CUST-016|2239.95|created|        0|\n",
      "|   20996|   CUST-013| 399.92|created|        0|\n",
      "|   56230|   CUST-013|2199.95|created|        0|\n",
      "|   24810|   CUST-002|3179.92|created|        0|\n",
      "+--------+-----------+-------+-------+---------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# Afficher les commandes\n",
    "df_commandes.select(\n",
    "    \"order_id\",\n",
    "    \"customer_id\",\n",
    "    \"total\",\n",
    "    \"status\",\n",
    "    \"partition\"\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d30103e",
   "metadata": {},
   "source": [
    "## 5. Analyser les items des commandes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "171dec49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------------------+----------+------------+--------+----------+--------+\n",
      "|order_id|customer_id|           timestamp|product_id|product_name|quantity|unit_price|subtotal|\n",
      "+--------+-----------+--------------------+----------+------------+--------+----------+--------+\n",
      "|   35924|   CUST-016|2026-01-18T10:08:...|         6|      Webcam|       3|     89.99|  269.97|\n",
      "|   35924|   CUST-016|2026-01-18T10:08:...|         2|      Souris|       3|     29.99|   89.97|\n",
      "|   35924|   CUST-016|2026-01-18T10:08:...|         4|       Ecran|       3|    349.99| 1049.97|\n",
      "|   29067|   CUST-013|2026-01-18T10:08:...|         6|      Webcam|       2|     89.99|  179.98|\n",
      "|   29067|   CUST-013|2026-01-18T10:08:...|         2|      Souris|       2|     29.99|   59.98|\n",
      "|   29067|   CUST-013|2026-01-18T10:08:...|         5|      Casque|       2|    149.99|  299.98|\n",
      "|   91880|   CUST-017|2026-01-18T10:08:...|         1|      Laptop|       3|    999.99| 2999.97|\n",
      "|   91880|   CUST-017|2026-01-18T10:08:...|         1|      Laptop|       2|    999.99| 1999.98|\n",
      "|   91880|   CUST-017|2026-01-18T10:08:...|         3|     Clavier|       1|     79.99|   79.99|\n",
      "|   91880|   CUST-017|2026-01-18T10:08:...|         3|     Clavier|       2|     79.99|  159.98|\n",
      "+--------+-----------+--------------------+----------+------------+--------+----------+--------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# Exploser le tableau items\n",
    "df_items = df_commandes \\\n",
    "    .select(\n",
    "        \"order_id\",\n",
    "        \"customer_id\",\n",
    "        \"timestamp\",\n",
    "        explode(\"items\").alias(\"item\")\n",
    "    ) \\\n",
    "    .select(\n",
    "        \"order_id\",\n",
    "        \"customer_id\",\n",
    "        \"timestamp\",\n",
    "        col(\"item.product_id\"),\n",
    "        col(\"item.product_name\"),\n",
    "        col(\"item.quantity\"),\n",
    "        col(\"item.unit_price\"),\n",
    "        col(\"item.subtotal\")\n",
    "    )\n",
    "\n",
    "df_items.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "010a3afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+---------------+------------------+\n",
      "|product_name|nb_ventes|quantite_totale|          ca_total|\n",
      "+------------+---------+---------------+------------------+\n",
      "|      Laptop|       28|             62|          61999.38|\n",
      "|       Ecran|       16|             35|12249.650000000001|\n",
      "|      Casque|       27|             56|           8399.44|\n",
      "|     Clavier|       20|             42|3359.5799999999995|\n",
      "|      Webcam|       17|             36|3239.6400000000003|\n",
      "|         SSD|       10|             17|           2209.83|\n",
      "|     USB Hub|       26|             44|           1759.56|\n",
      "|      Souris|       26|             50|            1499.5|\n",
      "+------------+---------+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Statistiques par produit\n",
    "df_stats_produit = df_items.groupBy(\"product_name\").agg(\n",
    "    count(\"*\").alias(\"nb_ventes\"),\n",
    "    spark_sum(\"quantity\").alias(\"quantite_totale\"),\n",
    "    spark_sum(\"subtotal\").alias(\"ca_total\")\n",
    ").orderBy(col(\"ca_total\").desc())\n",
    "\n",
    "df_stats_produit.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c12c68",
   "metadata": {},
   "source": [
    "## 6. Sauvegarder dans MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9c8e10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commandes sauvegardees\n"
     ]
    }
   ],
   "source": [
    "# Sauvegarder les commandes en Parquet\n",
    "df_commandes \\\n",
    "    .drop(\"items\") \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(f\"{MINIO_BUCKET}/kafka/commandes\")\n",
    "\n",
    "print(\"Commandes sauvegardees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44ea35f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items sauvegardes\n"
     ]
    }
   ],
   "source": [
    "# Sauvegarder les items\n",
    "df_items \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(f\"{MINIO_BUCKET}/kafka/items\")\n",
    "\n",
    "print(\"Items sauvegardes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ab066a",
   "metadata": {},
   "source": [
    "## 7. Streaming en temps reel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e94c104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream configure\n",
      "Is streaming: True\n"
     ]
    }
   ],
   "source": [
    "# Lire en mode streaming\n",
    "df_stream = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BROKER) \\\n",
    "    .option(\"subscribe\", \"commandes-json\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "print(\"Stream configure\")\n",
    "print(f\"Is streaming: {df_stream.isStreaming}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c179d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream parse\n"
     ]
    }
   ],
   "source": [
    "# Parser le stream\n",
    "df_stream_parsed = df_stream \\\n",
    "    .select(\n",
    "        from_json(col(\"value\").cast(\"string\"), commande_schema).alias(\"data\"),\n",
    "        \"timestamp\"\n",
    "    ) \\\n",
    "    .select(\n",
    "        \"data.order_id\",\n",
    "        \"data.customer_id\",\n",
    "        \"data.total\",\n",
    "        \"data.status\",\n",
    "        col(\"timestamp\").alias(\"kafka_time\")\n",
    "    )\n",
    "\n",
    "print(\"Stream parse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5125b0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream demarre - envoyez des messages Kafka pour les voir\n",
      "Executez query.stop() pour arreter\n"
     ]
    }
   ],
   "source": [
    "# Ecrire dans la console (pour debug)\n",
    "# Attention: Executez ce code puis envoyez des messages Kafka depuis un autre notebook\n",
    "\n",
    "query = df_stream_parsed \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .trigger(processingTime=\"5 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"Stream demarre - envoyez des messages Kafka pour les voir\")\n",
    "print(\"Executez query.stop() pour arreter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fb5d13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream arrete\n"
     ]
    }
   ],
   "source": [
    "# Attendre quelques secondes puis arreter\n",
    "import time\n",
    "time.sleep(30)  # Attendre 30 secondes\n",
    "query.stop()\n",
    "print(\"Stream arrete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1311d635",
   "metadata": {},
   "source": [
    "## 8. Aggregations en streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e3e20a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream prepare pour aggregation\n"
     ]
    }
   ],
   "source": [
    "# Lire a nouveau le stream\n",
    "df_stream2 = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BROKER) \\\n",
    "    .option(\"subscribe\", \"commandes-json\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Parser\n",
    "df_agg = df_stream2 \\\n",
    "    .select(\n",
    "        from_json(col(\"value\").cast(\"string\"), commande_schema).alias(\"data\"),\n",
    "        \"timestamp\"\n",
    "    ) \\\n",
    "    .select(\n",
    "        \"data.customer_id\",\n",
    "        \"data.total\",\n",
    "        col(\"timestamp\").alias(\"event_time\")\n",
    "    )\n",
    "\n",
    "print(\"Stream prepare pour aggregation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d60a859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregation definie\n"
     ]
    }
   ],
   "source": [
    "# Aggregation par fenetre de temps\n",
    "df_windowed = df_agg \\\n",
    "    .groupBy(\n",
    "        window(col(\"event_time\"), \"1 minute\"),\n",
    "        \"customer_id\"\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"nb_commandes\"),\n",
    "        spark_sum(\"total\").alias(\"total_commandes\")\n",
    "    )\n",
    "\n",
    "print(\"Aggregation definie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78df2593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream aggregation demarre\n"
     ]
    }
   ],
   "source": [
    "# Ecrire les aggregations\n",
    "query_agg = df_windowed \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"Stream aggregation demarre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "164415c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream arrete\n"
     ]
    }
   ],
   "source": [
    "# Attendre puis arreter\n",
    "time.sleep(20)\n",
    "query_agg.stop()\n",
    "print(\"Stream arrete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e097b3",
   "metadata": {},
   "source": [
    "## 9. Lire les logs applicatifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7bd0087e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logs lus: 15\n",
      "+--------------------------+-------+--------+---------------------+----------+-------+-----------+\n",
      "|timestamp                 |level  |module  |message              |request_id|user_id|duration_ms|\n",
      "+--------------------------+-------+--------+---------------------+----------+-------+-----------+\n",
      "|2026-01-18T10:08:42.711494|WARNING|api     |Rate limit exceeded  |req-95901 |user-75|413        |\n",
      "|2026-01-18T10:08:42.724452|WARNING|api     |Invalid request      |req-28650 |user-35|397        |\n",
      "|2026-01-18T10:08:42.729874|ERROR  |database|Connection closed    |req-58629 |user-3 |312        |\n",
      "|2026-01-18T10:08:42.734416|WARNING|auth    |Session expired      |req-63481 |user-49|66         |\n",
      "|2026-01-18T10:08:42.739303|INFO   |cache   |Cache hit            |req-60579 |user-95|168        |\n",
      "|2026-01-18T10:08:42.744905|INFO   |auth    |Authentication failed|req-35880 |user-46|464        |\n",
      "|2026-01-18T10:08:42.752232|WARNING|payment |Payment failed       |req-46375 |user-92|134        |\n",
      "|2026-01-18T10:08:42.759420|INFO   |auth    |User login           |req-70606 |user-30|170        |\n",
      "|2026-01-18T10:08:42.765970|ERROR  |api     |Invalid request      |req-20739 |user-18|221        |\n",
      "|2026-01-18T10:08:42.774219|INFO   |auth    |Session expired      |req-89030 |user-39|208        |\n",
      "+--------------------------+-------+--------+---------------------+----------+-------+-----------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# Schema des logs\n",
    "log_schema = StructType([\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"level\", StringType(), True),\n",
    "    StructField(\"module\", StringType(), True),\n",
    "    StructField(\"message\", StringType(), True),\n",
    "    StructField(\"request_id\", StringType(), True),\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"duration_ms\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Lire les logs\n",
    "df_logs = spark.read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BROKER) \\\n",
    "    .option(\"subscribe\", \"logs-application\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load() \\\n",
    "    .select(\n",
    "        from_json(col(\"value\").cast(\"string\"), log_schema).alias(\"log\")\n",
    "    ) \\\n",
    "    .select(\"log.*\")\n",
    "\n",
    "print(f\"Logs lus: {df_logs.count()}\")\n",
    "df_logs.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f37345c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|  level|count|\n",
      "+-------+-----+\n",
      "|   INFO|    7|\n",
      "|  ERROR|    4|\n",
      "|WARNING|    4|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Statistiques par niveau de log\n",
    "df_logs.groupBy(\"level\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4d83d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+----------------+\n",
      "|  module|nb_logs|duree_moyenne_ms|\n",
      "+--------+-------+----------------+\n",
      "|    auth|      5|           211.2|\n",
      "|     api|      4|           342.0|\n",
      "|database|      2|           226.0|\n",
      "|   cache|      2|           186.0|\n",
      "| payment|      2|           198.5|\n",
      "+--------+-------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Statistiques par module\n",
    "df_logs.groupBy(\"module\").agg(\n",
    "    count(\"*\").alias(\"nb_logs\"),\n",
    "    avg(\"duration_ms\").alias(\"duree_moyenne_ms\")\n",
    ").orderBy(col(\"nb_logs\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4fc31137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fermer Spark\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a48b56d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercice\n",
    "\n",
    "**Objectif** : Analyser les metriques Kafka\n",
    "\n",
    "**Consigne** :\n",
    "1. Lisez le topic \"metrics\" depuis Kafka\n",
    "2. Parsez les messages JSON\n",
    "3. Calculez les moyennes CPU et memoire par serveur\n",
    "4. Identifiez les serveurs les plus charges\n",
    "\n",
    "A vous de jouer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77e7dc17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration : Broker=broker:29092 | Topic=metrics\n"
     ]
    }
   ],
   "source": [
    "# TODO: Definir le schema des metriques\n",
    "\n",
    "# Configuration\n",
    "KAFKA_BROKER = \"broker:29092\"\n",
    "TOPIC_METRICS = \"metrics\"\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType, StructType\n",
    "\n",
    "# Schéma imbriqué pour le champ \"metrics\"\n",
    "metrics_inner_schema = StructType([\n",
    "    StructField(\"cpu_percent\", DoubleType()),\n",
    "    StructField(\"memory_percent\", DoubleType()),\n",
    "    StructField(\"disk_percent\", DoubleType()),\n",
    "    StructField(\"response_time_ms\", DoubleType())\n",
    "])\n",
    "\n",
    "# Schéma global du message JSON\n",
    "schema_json = StructType([\n",
    "    StructField(\"timestamp\", StringType()),\n",
    "    StructField(\"host\", StringType()),\n",
    "    StructField(\"metrics\", metrics_inner_schema)\n",
    "])\n",
    "\n",
    "print(f\"Configuration : Broker={KAFKA_BROKER} | Topic={TOPIC_METRICS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c5e54ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream initialisé. Schéma :\n",
      "root\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- host: string (nullable = true)\n",
      " |-- metrics: struct (nullable = true)\n",
      " |    |-- cpu_percent: double (nullable = true)\n",
      " |    |-- memory_percent: double (nullable = true)\n",
      " |    |-- disk_percent: double (nullable = true)\n",
      " |    |-- response_time_ms: double (nullable = true)\n",
      " |-- event_time: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Lire le topic metrics\n",
    "df_raw = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BROKER) \\\n",
    "    .option(\"subscribe\", TOPIC_METRICS) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Conversion du JSON (colonne 'value') en colonnes structurées\n",
    "df_parsed = df_raw.select(\n",
    "    from_json(col(\"value\").cast(\"string\"), schema_json).alias(\"data\"),\n",
    "    col(\"timestamp\").alias(\"kafka_timestamp\")\n",
    ").select(\"data.*\")\n",
    "\n",
    "# Conversion du timestamp string en vrai timestamp pour les fenêtres temporelles\n",
    "df_clean = df_parsed.withColumn(\"event_time\", to_timestamp(\"timestamp\"))\n",
    "\n",
    "print(\"Stream initialisé. Schéma :\")\n",
    "df_clean.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "18a9b199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming terminé.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Calculer les statistiques par serveur\n",
    "\n",
    "# Agrégation avec fenêtre glissante (Tumbling Window) de 30 secondes\n",
    "df_stats = df_clean \\\n",
    "    .withWatermark(\"event_time\", \"1 minute\") \\\n",
    "    .groupBy(\n",
    "        window(\"event_time\", \"30 seconds\"),\n",
    "        \"host\"\n",
    "    ) \\\n",
    "    .agg(\n",
    "        round(avg(\"metrics.cpu_percent\"), 2).alias(\"avg_cpu\"),\n",
    "        round(spark_max(\"metrics.memory_percent\"), 2).alias(\"max_mem\"),\n",
    "        count(\"*\").alias(\"nb_mesures\")\n",
    "    )\n",
    "\n",
    "# Affichage des résultats en console (Output Mode : Update ou Complete)\n",
    "query = df_stats.writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .queryName(\"stats_serveurs\") \\\n",
    "    .start()\n",
    "\n",
    "import time\n",
    "time.sleep(20)\n",
    "query.stop()\n",
    "print(\"Streaming terminé.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee3f95d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Resume\n",
    "\n",
    "Dans ce notebook, vous avez appris :\n",
    "- Comment **lire des messages Kafka** avec Spark\n",
    "- Comment **parser des messages JSON** avec un schema\n",
    "- Comment utiliser le **mode batch** et le **mode streaming**\n",
    "- Comment faire des **aggregations en streaming** avec des fenetres de temps\n",
    "- Comment **sauvegarder les donnees** dans MinIO\n",
    "\n",
    "### Prochaine etape\n",
    "Dans le prochain notebook, nous approfondirons le streaming Spark avec des concepts avances."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
