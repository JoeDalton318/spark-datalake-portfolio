{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b825df71",
   "metadata": {},
   "source": [
    "# Exercice 18 - Streaming Spark Avance\n",
    "\n",
    "## Objectifs\n",
    "- Comprendre les watermarks et le traitement tardif\n",
    "- Maitriser les differents modes de trigger\n",
    "- Implementer des jointures en streaming\n",
    "- Gerer les checkpoints pour la tolerance aux pannes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cc61d5",
   "metadata": {},
   "source": [
    "## 1. Concepts avances du streaming\n",
    "\n",
    "```\n",
    "+------------------------------------------------------------------+\n",
    "|                  STREAMING SPARK AVANCE                          |\n",
    "+------------------------------------------------------------------+\n",
    "|                                                                  |\n",
    "|  WATERMARK : Gestion des donnees tardives                        |\n",
    "|  +---------------------------------------------------------+    |\n",
    "|  |                                                         |    |\n",
    "|  |  Temps reel    Watermark         Donnees acceptees      |    |\n",
    "|  |      |            |                    |                |    |\n",
    "|  |   12:05        12:00               >= 12:00             |    |\n",
    "|  |      |<-- 5min -->|                                     |    |\n",
    "|  |                                                         |    |\n",
    "|  |  Si delai = 5 min, les donnees avec timestamp < 12:00   |    |\n",
    "|  |  seront ignorees (arrivees trop tard)                   |    |\n",
    "|  +---------------------------------------------------------+    |\n",
    "|                                                                  |\n",
    "|  TRIGGERS : Frequence de traitement                              |\n",
    "|  +---------------------------------------------------------+    |\n",
    "|  |  - processingTime(\"10 seconds\") : toutes les 10 sec     |    |\n",
    "|  |  - once()                       : une seule fois        |    |\n",
    "|  |  - continuous(\"1 second\")       : latence minimale      |    |\n",
    "|  |  - availableNow()               : traite tout dispo     |    |\n",
    "|  +---------------------------------------------------------+    |\n",
    "|                                                                  |\n",
    "|  CHECKPOINTS : Tolerance aux pannes                              |\n",
    "|  +---------------------------------------------------------+    |\n",
    "|  |  Sauvegarde :                                           |    |\n",
    "|  |  - Offsets Kafka traites                                |    |\n",
    "|  |  - Etat des aggregations                                |    |\n",
    "|  |  - Metadata du stream                                   |    |\n",
    "|  +---------------------------------------------------------+    |\n",
    "|                                                                  |\n",
    "+------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bb8207",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e491bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des librairies : org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.0,org.apache.hadoop:hadoop-aws:3.4.1,com.amazonaws:aws-java-sdk-bundle:1.12.262\n",
      "Session prête (Kafka + S3 supportés)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, from_json, to_timestamp, window, expr,\n",
    "    count, sum as spark_sum, avg, max as spark_max, min as spark_min,\n",
    "    current_timestamp, lit, struct, to_json\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType,\n",
    "    DoubleType, ArrayType, TimestampType\n",
    ")\n",
    "\n",
    "# --- CONFIGURATION COMPLETE (KAFKA + S3) ---\n",
    "SPARK_VER = os.environ.get(\"SPARK_VER\", \"3.5.0\")\n",
    "SCALA_SUFFIX = os.environ.get(\"SCALA_SUFFIX\", \"2.13\")\n",
    "\n",
    "# 1. Le paquet Kafka (Dynamique)\n",
    "kafka_package = f\"org.apache.spark:spark-sql-kafka-0-10_{SCALA_SUFFIX}:{SPARK_VER}\"\n",
    "\n",
    "# 2. Les paquets S3/MinIO (Indispensables pour s3a://)\n",
    "s3_packages = \"org.apache.hadoop:hadoop-aws:3.4.1,com.amazonaws:aws-java-sdk-bundle:1.12.262\"\n",
    "\n",
    "# 3. On combine le tout\n",
    "all_packages = f\"{kafka_package},{s3_packages}\"\n",
    "\n",
    "print(f\"Chargement des librairies : {all_packages}\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"StreamingAvance\") \\\n",
    "    .config(\"spark.jars.packages\", all_packages) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.sql.streaming.checkpointLocation\", \"/tmp/checkpoints\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "KAFKA_BROKER = \"broker:29092\"\n",
    "CHECKPOINT_PATH = \"s3a://bronze/checkpoints\"\n",
    "\n",
    "print(f\"Session prête (Kafka + S3 supportés)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a4853e",
   "metadata": {},
   "source": [
    "## 3. Watermarks - Gestion des donnees tardives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3285c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream configure\n"
     ]
    }
   ],
   "source": [
    "# Schema des commandes\n",
    "commande_schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"total\", DoubleType(), True),\n",
    "    StructField(\"status\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Lire le stream Kafka\n",
    "df_stream = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BROKER) \\\n",
    "    .option(\"subscribe\", \"commandes-json\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "print(\"Stream configure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "852c85e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Watermark defini : 5 minutes\n"
     ]
    }
   ],
   "source": [
    "# Parser et ajouter watermark\n",
    "df_parsed = df_stream \\\n",
    "    .select(\n",
    "        from_json(col(\"value\").cast(\"string\"), commande_schema).alias(\"data\"),\n",
    "        col(\"timestamp\").alias(\"kafka_time\")\n",
    "    ) \\\n",
    "    .select(\n",
    "        \"data.order_id\",\n",
    "        \"data.customer_id\",\n",
    "        \"data.total\",\n",
    "        to_timestamp(\"data.timestamp\").alias(\"event_time\"),\n",
    "        \"kafka_time\"\n",
    "    ) \\\n",
    "    .withWatermark(\"event_time\", \"5 minutes\")  # Tolere 5 min de retard\n",
    "\n",
    "print(\"Watermark defini : 5 minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7382955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregation fenetre glissante configuree\n"
     ]
    }
   ],
   "source": [
    "# Aggregation avec fenetre glissante et watermark\n",
    "df_windowed = df_parsed \\\n",
    "    .groupBy(\n",
    "        window(col(\"event_time\"), \"2 minutes\", \"1 minute\"),  # Fenetre 2min, glisse 1min\n",
    "        \"customer_id\"\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"nb_commandes\"),\n",
    "        spark_sum(\"total\").alias(\"total_ventes\"),\n",
    "        spark_max(\"total\").alias(\"max_commande\")\n",
    "    )\n",
    "\n",
    "print(\"Aggregation fenetre glissante configuree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6833ae1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream avec watermark demarre\n"
     ]
    }
   ],
   "source": [
    "# Lancer le stream avec mode update\n",
    "query_watermark = df_windowed \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"Stream avec watermark demarre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0d1a141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream arrete\n"
     ]
    }
   ],
   "source": [
    "# Arreter apres 30 secondes\n",
    "time.sleep(30)\n",
    "query_watermark.stop()\n",
    "print(\"Stream arrete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68aac9b6",
   "metadata": {},
   "source": [
    "## 4. Differents modes de trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f95429ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement once() termine\n"
     ]
    }
   ],
   "source": [
    "# Mode once() - Traitement unique\n",
    "# Utile pour le traitement batch incremental\n",
    "\n",
    "df_once = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BROKER) \\\n",
    "    .option(\"subscribe\", \"commandes-json\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load() \\\n",
    "    .select(\n",
    "        from_json(col(\"value\").cast(\"string\"), commande_schema).alias(\"data\")\n",
    "    ) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "query_once = df_once \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(once=True) \\\n",
    "    .start()\n",
    "\n",
    "query_once.awaitTermination()\n",
    "print(\"Traitement once() termine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f61747b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement availableNow() termine\n"
     ]
    }
   ],
   "source": [
    "# Mode availableNow() - Traite tout ce qui est disponible\n",
    "# Similaire a once() mais avec meilleure gestion des partitions\n",
    "\n",
    "df_available = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BROKER) \\\n",
    "    .option(\"subscribe\", \"commandes-json\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load() \\\n",
    "    .select(\n",
    "        from_json(col(\"value\").cast(\"string\"), commande_schema).alias(\"data\")\n",
    "    ) \\\n",
    "    .select(\"data.order_id\", \"data.customer_id\", \"data.total\")\n",
    "\n",
    "query_available = df_available \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .start()\n",
    "\n",
    "query_available.awaitTermination()\n",
    "print(\"Traitement availableNow() termine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae9b68d",
   "metadata": {},
   "source": [
    "## 5. Checkpoints - Tolerance aux pannes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b1a94bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream avec checkpoint prepare\n"
     ]
    }
   ],
   "source": [
    "# Stream avec checkpoint\n",
    "df_checkpoint = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BROKER) \\\n",
    "    .option(\"subscribe\", \"commandes-json\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load() \\\n",
    "    .select(\n",
    "        from_json(col(\"value\").cast(\"string\"), commande_schema).alias(\"data\"),\n",
    "        col(\"timestamp\").alias(\"kafka_time\")\n",
    "    ) \\\n",
    "    .select(\n",
    "        \"data.order_id\",\n",
    "        \"data.customer_id\",\n",
    "        \"data.total\",\n",
    "        \"kafka_time\"\n",
    "    )\n",
    "\n",
    "print(\"Stream avec checkpoint prepare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1763043b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream avec checkpoint demarre\n",
      "Checkpoint: s3a://bronze/checkpoints/commandes\n"
     ]
    }
   ],
   "source": [
    "# Ecrire avec checkpoint dans MinIO\n",
    "query_ckpt = df_checkpoint \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", \"s3a://bronze/streaming/commandes\") \\\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_PATH}/commandes\") \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"Stream avec checkpoint demarre\")\n",
    "print(f\"Checkpoint: {CHECKPOINT_PATH}/commandes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12bf2c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statut du stream:\n",
      "  ID: 24681f96-cd02-4ba4-855e-e663c2ab6b57\n",
      "  Run ID: 1e0e3b27-5d9f-4165-bb57-48d4675811bb\n",
      "  Actif: True\n",
      "  Status: {'message': 'Initializing sources', 'isDataAvailable': False, 'isTriggerActive': False}\n"
     ]
    }
   ],
   "source": [
    "# Verifier le statut\n",
    "print(\"Statut du stream:\")\n",
    "print(f\"  ID: {query_ckpt.id}\")\n",
    "print(f\"  Run ID: {query_ckpt.runId}\")\n",
    "print(f\"  Actif: {query_ckpt.isActive}\")\n",
    "print(f\"  Status: {query_ckpt.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b8e53a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream arrete - checkpoint sauvegarde\n"
     ]
    }
   ],
   "source": [
    "# Attendre et arreter\n",
    "time.sleep(20)\n",
    "query_ckpt.stop()\n",
    "print(\"Stream arrete - checkpoint sauvegarde\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6154de",
   "metadata": {},
   "source": [
    "## 6. Jointure Stream-Static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dadf1121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+---------+--------+\n",
      "|customer_id|         nom|    ville| segment|\n",
      "+-----------+------------+---------+--------+\n",
      "|   CUST-001|Alice Martin|    Paris|     VIP|\n",
      "|   CUST-002|  Bob Dupont|     Lyon|Standard|\n",
      "|   CUST-003|Claire Leroy|Marseille| Premium|\n",
      "|   CUST-004|David Moreau| Toulouse|Standard|\n",
      "|   CUST-005|  Emma Petit|     Nice|     VIP|\n",
      "+-----------+------------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creer un DataFrame statique de reference\n",
    "# Simule une table de clients\n",
    "\n",
    "clients_data = [\n",
    "    (\"CUST-001\", \"Alice Martin\", \"Paris\", \"VIP\"),\n",
    "    (\"CUST-002\", \"Bob Dupont\", \"Lyon\", \"Standard\"),\n",
    "    (\"CUST-003\", \"Claire Leroy\", \"Marseille\", \"Premium\"),\n",
    "    (\"CUST-004\", \"David Moreau\", \"Toulouse\", \"Standard\"),\n",
    "    (\"CUST-005\", \"Emma Petit\", \"Nice\", \"VIP\"),\n",
    "]\n",
    "\n",
    "df_clients = spark.createDataFrame(\n",
    "    clients_data,\n",
    "    [\"customer_id\", \"nom\", \"ville\", \"segment\"]\n",
    ")\n",
    "\n",
    "df_clients.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a65c0403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream de commandes pret\n"
     ]
    }
   ],
   "source": [
    "# Stream de commandes\n",
    "df_commandes_stream = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BROKER) \\\n",
    "    .option(\"subscribe\", \"commandes-json\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load() \\\n",
    "    .select(\n",
    "        from_json(col(\"value\").cast(\"string\"), commande_schema).alias(\"data\")\n",
    "    ) \\\n",
    "    .select(\n",
    "        \"data.order_id\",\n",
    "        \"data.customer_id\",\n",
    "        \"data.total\"\n",
    "    )\n",
    "\n",
    "print(\"Stream de commandes pret\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce05c46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jointure stream-static configuree\n"
     ]
    }
   ],
   "source": [
    "# Jointure stream-static\n",
    "df_enrichi = df_commandes_stream.join(\n",
    "    df_clients,\n",
    "    on=\"customer_id\",\n",
    "    how=\"left\"\n",
    ").select(\n",
    "    \"order_id\",\n",
    "    \"customer_id\",\n",
    "    \"nom\",\n",
    "    \"ville\",\n",
    "    \"segment\",\n",
    "    \"total\"\n",
    ")\n",
    "\n",
    "print(\"Jointure stream-static configuree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0069247c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream avec jointure demarre\n"
     ]
    }
   ],
   "source": [
    "# Executer la jointure\n",
    "query_join = df_enrichi \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .trigger(processingTime=\"5 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"Stream avec jointure demarre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9631851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream arrete\n"
     ]
    }
   ],
   "source": [
    "time.sleep(20)\n",
    "query_join.stop()\n",
    "print(\"Stream arrete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e094c912",
   "metadata": {},
   "source": [
    "## 7. Ecriture vers Kafka (Stream to Stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f8b4963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation preparee\n"
     ]
    }
   ],
   "source": [
    "# Lire, transformer et ecrire vers un autre topic\n",
    "df_transform = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BROKER) \\\n",
    "    .option(\"subscribe\", \"commandes-json\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load() \\\n",
    "    .select(\n",
    "        from_json(col(\"value\").cast(\"string\"), commande_schema).alias(\"data\")\n",
    "    ) \\\n",
    "    .select(\n",
    "        col(\"data.customer_id\").alias(\"key\"),\n",
    "        struct(\n",
    "            col(\"data.order_id\"),\n",
    "            col(\"data.customer_id\"),\n",
    "            col(\"data.total\"),\n",
    "            (col(\"data.total\") * 0.2).alias(\"tva\"),\n",
    "            current_timestamp().alias(\"processed_at\")\n",
    "        ).alias(\"value\")\n",
    "    ) \\\n",
    "    .select(\n",
    "        col(\"key\"),\n",
    "        to_json(col(\"value\")).alias(\"value\")\n",
    "    )\n",
    "\n",
    "print(\"Transformation preparee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f70ef9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream Kafka -> Kafka demarre\n"
     ]
    }
   ],
   "source": [
    "# Ecrire vers un nouveau topic Kafka\n",
    "query_kafka = df_transform \\\n",
    "    .writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BROKER) \\\n",
    "    .option(\"topic\", \"commandes-enrichies\") \\\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_PATH}/kafka-to-kafka\") \\\n",
    "    .trigger(processingTime=\"5 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"Stream Kafka -> Kafka demarre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4aabfad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream arrete\n"
     ]
    }
   ],
   "source": [
    "time.sleep(20)\n",
    "query_kafka.stop()\n",
    "print(\"Stream arrete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44456f9f",
   "metadata": {},
   "source": [
    "## 8. Gestion de plusieurs streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0488426c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de streams actifs: 0\n"
     ]
    }
   ],
   "source": [
    "# Lister tous les streams actifs\n",
    "streams = spark.streams.active\n",
    "\n",
    "print(f\"Nombre de streams actifs: {len(streams)}\")\n",
    "for stream in streams:\n",
    "    print(f\"  - {stream.name}: {stream.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7f588bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tous les streams arretes\n"
     ]
    }
   ],
   "source": [
    "# Arreter tous les streams\n",
    "for stream in spark.streams.active:\n",
    "    stream.stop()\n",
    "    print(f\"Stream {stream.id} arrete\")\n",
    "\n",
    "print(\"Tous les streams arretes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5281d4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercice\n",
    "\n",
    "**Objectif** : Creer un pipeline streaming complet\n",
    "\n",
    "**Consigne** :\n",
    "1. Lisez le topic \"logs-application\" en streaming\n",
    "2. Ajoutez un watermark de 2 minutes\n",
    "3. Calculez le nombre de logs par niveau (INFO, WARNING, ERROR) par fenetre de 1 minute\n",
    "4. Ecrivez les resultats dans la console\n",
    "\n",
    "A vous de jouer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12046a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schéma logs défini.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Definir le schema des logs\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "schema_logs = StructType([\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"level\", StringType(), True),\n",
    "    StructField(\"service\", StringType(), True),\n",
    "    StructField(\"message\", StringType(), True)\n",
    "])\n",
    "\n",
    "print(\"Schéma logs défini.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1b01580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream logs initialisé avec Watermark.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Lire le stream avec watermark\n",
    "\n",
    "# 1. Lecture brute\n",
    "df_logs_raw = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"broker:29092\") \\\n",
    "    .option(\"subscribe\", \"logs-application\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# 2. Parsing et Watermark\n",
    "df_logs = df_logs_raw.select(\n",
    "    from_json(col(\"value\").cast(\"string\"), schema_logs).alias(\"data\")\n",
    ").select(\n",
    "    \"data.level\",\n",
    "    \"data.service\",\n",
    "    \"data.message\",\n",
    "    to_timestamp(\"data.timestamp\").alias(\"event_time\")\n",
    ").withWatermark(\"event_time\", \"2 minutes\")\n",
    "\n",
    "print(\"Stream logs initialisé avec Watermark.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7696bcd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming terminé.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Aggreger par niveau et fenetre\n",
    "\n",
    "# Agrégation\n",
    "df_logs_count = df_logs \\\n",
    "    .groupBy(\n",
    "        window(\"event_time\", \"1 minute\"),\n",
    "        \"level\"\n",
    "    ) \\\n",
    "    .count() \\\n",
    "    .orderBy(\"window\")\n",
    "\n",
    "# Affichage Console\n",
    "query_logs = df_logs_count.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .trigger(processingTime=\"5 seconds\") \\\n",
    "    .queryName(\"logs_monitoring\") \\\n",
    "    .start()\n",
    "\n",
    "# Laissez tourner quelques secondes pour voir le résultat du générateur\n",
    "import time\n",
    "time.sleep(15)\n",
    "query_logs.stop()\n",
    "print(\"Streaming terminé.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32345eb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Resume\n",
    "\n",
    "Dans ce notebook, vous avez appris :\n",
    "- Comment utiliser les **watermarks** pour gerer les donnees tardives\n",
    "- Les differents **modes de trigger** (processingTime, once, availableNow)\n",
    "- Comment configurer les **checkpoints** pour la tolerance aux pannes\n",
    "- Comment faire des **jointures stream-static**\n",
    "- Comment **ecrire vers Kafka** depuis un stream\n",
    "\n",
    "### Prochaine etape\n",
    "Dans le prochain notebook, nous construirons un pipeline complet Bronze/Silver/Gold."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
