{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b81fd6c6",
   "metadata": {},
   "source": [
    "# Exercice 04 - Lecture et ecriture de fichiers avec Spark\n",
    "\n",
    "## Objectifs\n",
    "- Lire des fichiers CSV, JSON et Parquet avec Spark\n",
    "- Ecrire des donnees dans differents formats\n",
    "- Comprendre les avantages du format Parquet\n",
    "- Manipuler des fichiers locaux\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6fb8fa",
   "metadata": {},
   "source": [
    "## 1. Les formats de fichiers en Big Data\n",
    "\n",
    "```\n",
    "+------------------------------------------------------------------+\n",
    "|                    FORMATS DE FICHIERS                           |\n",
    "+------------------------------------------------------------------+\n",
    "|                                                                  |\n",
    "|  CSV (Comma Separated Values)                                    |\n",
    "|  +----------------------------------------------------------+    |\n",
    "|  | nom,age,ville                                            |    |\n",
    "|  | Alice,30,Paris                                           |    |\n",
    "|  | Bob,25,Lyon                                              |    |\n",
    "|  +----------------------------------------------------------+    |\n",
    "|  + Simple, lisible                                               |\n",
    "|  - Pas de typage, lent a lire                                    |\n",
    "|                                                                  |\n",
    "|  JSON (JavaScript Object Notation)                               |\n",
    "|  +----------------------------------------------------------+    |\n",
    "|  | {\"nom\": \"Alice\", \"age\": 30, \"ville\": \"Paris\"}            |    |\n",
    "|  | {\"nom\": \"Bob\", \"age\": 25, \"ville\": \"Lyon\"}               |    |\n",
    "|  +----------------------------------------------------------+    |\n",
    "|  + Flexible, structure imbriquee                                 |\n",
    "|  - Verbose, pas optimal pour gros volumes                        |\n",
    "|                                                                  |\n",
    "|  PARQUET (format colonne)                                        |\n",
    "|  +----------------------------------------------------------+    |\n",
    "|  | [binaire compresse avec schema integre]                  |    |\n",
    "|  +----------------------------------------------------------+    |\n",
    "|  + Tres rapide, compresse, typage fort                           |\n",
    "|  + Standard du Big Data                                          |\n",
    "|  - Non lisible par un humain                                     |\n",
    "|                                                                  |\n",
    "+------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e679348a",
   "metadata": {},
   "source": [
    "## 2. Initialisation de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "187e129e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark pret !\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Creer la SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Lecture Ecriture Fichiers\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark pret !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a54ea1",
   "metadata": {},
   "source": [
    "## 3. Creer des donnees de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30f88425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+---------+-------+\n",
      "| id|    nom|age|    ville|salaire|\n",
      "+---+-------+---+---------+-------+\n",
      "|  1|  Alice| 30|    Paris|55000.0|\n",
      "|  2|    Bob| 25|     Lyon|42000.0|\n",
      "|  3|Charlie| 35|Marseille|65000.0|\n",
      "|  4|  Diana| 28| Bordeaux|48000.0|\n",
      "|  5|    Eve| 32| Toulouse|52000.0|\n",
      "+---+-------+---+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creer un DataFrame de test\n",
    "donnees = [\n",
    "    (1, \"Alice\", 30, \"Paris\", 55000.0),\n",
    "    (2, \"Bob\", 25, \"Lyon\", 42000.0),\n",
    "    (3, \"Charlie\", 35, \"Marseille\", 65000.0),\n",
    "    (4, \"Diana\", 28, \"Bordeaux\", 48000.0),\n",
    "    (5, \"Eve\", 32, \"Toulouse\", 52000.0)\n",
    "]\n",
    "\n",
    "colonnes = [\"id\", \"nom\", \"age\", \"ville\", \"salaire\"]\n",
    "\n",
    "df = spark.createDataFrame(donnees, colonnes)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3871ebe",
   "metadata": {},
   "source": [
    "## 4. Ecriture en CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fe9d406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier CSV ecrit dans : /home/jovyan/work/exercices/data/employes_csv\n"
     ]
    }
   ],
   "source": [
    "# Ecrire en CSV\n",
    "# - mode(\"overwrite\") : ecrase si le fichier existe\n",
    "# - header=True : inclut les noms de colonnes\n",
    "\n",
    "chemin_csv = \"/home/jovyan/work/exercices/data/employes_csv\"\n",
    "\n",
    "df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .csv(chemin_csv)\n",
    "\n",
    "print(f\"Fichier CSV ecrit dans : {chemin_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56b9fd3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".part-00000-d88cd15d-249f-4077-adbd-7083d8e7dbd8-c000.csv.crc\n",
      ".part-00002-d88cd15d-249f-4077-adbd-7083d8e7dbd8-c000.csv.crc\n",
      ".part-00004-d88cd15d-249f-4077-adbd-7083d8e7dbd8-c000.csv.crc\n",
      ".part-00007-d88cd15d-249f-4077-adbd-7083d8e7dbd8-c000.csv.crc\n",
      ".part-00009-d88cd15d-249f-4077-adbd-7083d8e7dbd8-c000.csv.crc\n",
      ".part-00011-d88cd15d-249f-4077-adbd-7083d8e7dbd8-c000.csv.crc\n",
      "._SUCCESS.crc\n",
      "part-00000-d88cd15d-249f-4077-adbd-7083d8e7dbd8-c000.csv\n",
      "part-00002-d88cd15d-249f-4077-adbd-7083d8e7dbd8-c000.csv\n",
      "part-00004-d88cd15d-249f-4077-adbd-7083d8e7dbd8-c000.csv\n",
      "part-00007-d88cd15d-249f-4077-adbd-7083d8e7dbd8-c000.csv\n",
      "part-00009-d88cd15d-249f-4077-adbd-7083d8e7dbd8-c000.csv\n",
      "part-00011-d88cd15d-249f-4077-adbd-7083d8e7dbd8-c000.csv\n",
      "_SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# Verifier les fichiers crees\n",
    "import os\n",
    "\n",
    "for fichier in os.listdir(chemin_csv):\n",
    "    print(fichier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34686b1f",
   "metadata": {},
   "source": [
    "### Note importante\n",
    "\n",
    "Spark cree un **dossier** contenant plusieurs fichiers :\n",
    "- `part-00000-xxx.csv` : les donnees (peut y en avoir plusieurs)\n",
    "- `_SUCCESS` : indique que l'ecriture s'est bien passee\n",
    "\n",
    "C'est normal ! Spark est concu pour le traitement distribue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98992ea",
   "metadata": {},
   "source": [
    "## 5. Lecture du CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f36e3742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+---------+-------+\n",
      "| id|    nom|age|    ville|salaire|\n",
      "+---+-------+---+---------+-------+\n",
      "|  3|Charlie| 35|Marseille|65000.0|\n",
      "|  4|  Diana| 28| Bordeaux|48000.0|\n",
      "|  5|    Eve| 32| Toulouse|52000.0|\n",
      "|  1|  Alice| 30|    Paris|55000.0|\n",
      "|  2|    Bob| 25|     Lyon|42000.0|\n",
      "+---+-------+---+---------+-------+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- nom: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- ville: string (nullable = true)\n",
      " |-- salaire: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lire le fichier CSV\n",
    "df_csv = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .csv(chemin_csv)\n",
    "\n",
    "df_csv.show()\n",
    "df_csv.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9449e6",
   "metadata": {},
   "source": [
    "## 6. Ecriture et lecture en JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d326444c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier JSON ecrit dans : /home/jovyan/work/exercices/data/employes_json\n"
     ]
    }
   ],
   "source": [
    "# Ecrire en JSON\n",
    "chemin_json = \"/home/jovyan/work/exercices/data/employes_json\"\n",
    "\n",
    "df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .json(chemin_json)\n",
    "\n",
    "print(f\"Fichier JSON ecrit dans : {chemin_json}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a446b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+-------+---------+\n",
      "|age| id|    nom|salaire|    ville|\n",
      "+---+---+-------+-------+---------+\n",
      "| 35|  3|Charlie|65000.0|Marseille|\n",
      "| 28|  4|  Diana|48000.0| Bordeaux|\n",
      "| 32|  5|    Eve|52000.0| Toulouse|\n",
      "| 30|  1|  Alice|55000.0|    Paris|\n",
      "| 25|  2|    Bob|42000.0|     Lyon|\n",
      "+---+---+-------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lire le JSON\n",
    "df_json = spark.read.json(chemin_json)\n",
    "df_json.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d673da4",
   "metadata": {},
   "source": [
    "## 7. Ecriture et lecture en Parquet (recommande)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fa20595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier Parquet ecrit dans : /home/jovyan/work/exercices/data/employes_parquet\n"
     ]
    }
   ],
   "source": [
    "# Ecrire en Parquet\n",
    "chemin_parquet = \"/home/jovyan/work/exercices/data/employes_parquet\"\n",
    "\n",
    "df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(chemin_parquet)\n",
    "\n",
    "print(f\"Fichier Parquet ecrit dans : {chemin_parquet}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb9c7fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+---------+-------+\n",
      "| id|    nom|age|    ville|salaire|\n",
      "+---+-------+---+---------+-------+\n",
      "|  3|Charlie| 35|Marseille|65000.0|\n",
      "|  4|  Diana| 28| Bordeaux|48000.0|\n",
      "|  5|    Eve| 32| Toulouse|52000.0|\n",
      "|  1|  Alice| 30|    Paris|55000.0|\n",
      "|  2|    Bob| 25|     Lyon|42000.0|\n",
      "+---+-------+---+---------+-------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- nom: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- ville: string (nullable = true)\n",
      " |-- salaire: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lire le Parquet\n",
    "df_parquet = spark.read.parquet(chemin_parquet)\n",
    "df_parquet.show()\n",
    "df_parquet.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c01c2b0",
   "metadata": {},
   "source": [
    "### Avantages de Parquet\n",
    "\n",
    "```\n",
    "+------------------------------------------------------------------+\n",
    "|              POURQUOI PARQUET EST LE STANDARD                    |\n",
    "+------------------------------------------------------------------+\n",
    "|                                                                  |\n",
    "|  1. COMPRESSION                                                  |\n",
    "|     Fichiers 10x plus petits que CSV                             |\n",
    "|                                                                  |\n",
    "|  2. SCHEMA INTEGRE                                               |\n",
    "|     Pas besoin de deviner les types                              |\n",
    "|                                                                  |\n",
    "|  3. LECTURE SELECTIVE                                            |\n",
    "|     Peut lire seulement certaines colonnes                       |\n",
    "|                                                                  |\n",
    "|  4. PERFORMANT                                                   |\n",
    "|     Optimise pour le traitement analytique                       |\n",
    "|                                                                  |\n",
    "+------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e94aec9",
   "metadata": {},
   "source": [
    "## 8. Comparaison des tailles de fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc90df39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille CSV     : 362 octets\n",
      "Taille JSON    : 413 octets\n",
      "Taille Parquet : 8,563 octets\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def taille_dossier(chemin):\n",
    "    \"\"\"Calcule la taille totale d'un dossier en octets\"\"\"\n",
    "    total = 0\n",
    "    for fichier in os.listdir(chemin):\n",
    "        chemin_fichier = os.path.join(chemin, fichier)\n",
    "        if os.path.isfile(chemin_fichier):\n",
    "            total += os.path.getsize(chemin_fichier)\n",
    "    return total\n",
    "\n",
    "taille_csv = taille_dossier(chemin_csv)\n",
    "taille_json = taille_dossier(chemin_json)\n",
    "taille_parquet = taille_dossier(chemin_parquet)\n",
    "\n",
    "print(f\"Taille CSV     : {taille_csv:,} octets\")\n",
    "print(f\"Taille JSON    : {taille_json:,} octets\")\n",
    "print(f\"Taille Parquet : {taille_parquet:,} octets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0d00ec",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercice\n",
    "\n",
    "**Objectif** : Pratiquer la lecture et l'ecriture de fichiers\n",
    "\n",
    "**Consigne** :\n",
    "1. Creez un DataFrame avec des donnees de commandes (id, produit, quantite, prix_unitaire)\n",
    "2. Ecrivez-le en format CSV dans `data/commandes_csv`\n",
    "3. Ecrivez-le en format Parquet dans `data/commandes_parquet`\n",
    "4. Relisez les deux fichiers et affichez-les\n",
    "5. Comparez les tailles\n",
    "\n",
    "A vous de jouer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ef9f395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Donnees de commandes\n",
    "commandes = [\n",
    "    (1, \"Laptop\", 2, 999.99),\n",
    "    (2, \"Souris\", 5, 29.99),\n",
    "    (3, \"Clavier\", 3, 79.99),\n",
    "    (4, \"Ecran\", 1, 299.99),\n",
    "    (5, \"Casque\", 4, 149.99)\n",
    "]\n",
    "\n",
    "colonnes = [\"id\", \"produit\", \"quantite\", \"prix_unitaire\"]\n",
    "\n",
    "# TODO: Creez le DataFrame\n",
    "df_commandes = spark.createDataFrame(commandes, colonnes)\n",
    "\n",
    "# TODO: Ecrivez en CSV\n",
    "chemin_csv_exo = \"data/commandes_csv\"\n",
    "\n",
    "df_commandes.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .csv(chemin_csv_exo)\n",
    "\n",
    "# TODO: Ecrivez en Parquet\n",
    "chemin_parquet_exo = \"data/commandes_parquet\"\n",
    "\n",
    "df_commandes.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(chemin_parquet_exo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e468d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+-------------+\n",
      "| id|produit|quantite|prix_unitaire|\n",
      "+---+-------+--------+-------------+\n",
      "|  1| Laptop|       2|       999.99|\n",
      "|  3|Clavier|       3|        79.99|\n",
      "|  5| Casque|       4|       149.99|\n",
      "|  2| Souris|       5|        29.99|\n",
      "|  4|  Ecran|       1|       299.99|\n",
      "+---+-------+--------+-------------+\n",
      "\n",
      "+---+-------+--------+-------------+\n",
      "| id|produit|quantite|prix_unitaire|\n",
      "+---+-------+--------+-------------+\n",
      "|  3|Clavier|       3|        79.99|\n",
      "|  1| Laptop|       2|       999.99|\n",
      "|  2| Souris|       5|        29.99|\n",
      "|  5| Casque|       4|       149.99|\n",
      "|  4|  Ecran|       1|       299.99|\n",
      "+---+-------+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Relisez et affichez les fichiers\n",
    "df_commandes_csv = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .csv(chemin_csv_exo)\n",
    "df_commandes_csv.show()\n",
    "\n",
    "df_commandes_parquet = spark.read.parquet(chemin_parquet_exo)\n",
    "df_commandes_parquet.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dee8e00",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Resume\n",
    "\n",
    "Dans ce notebook, vous avez appris :\n",
    "- Les 3 formats principaux : **CSV**, **JSON**, **Parquet**\n",
    "- Comment **ecrire** des donnees avec `df.write`\n",
    "- Comment **lire** des donnees avec `spark.read`\n",
    "- **Parquet** est le format recommande pour le Big Data\n",
    "\n",
    "### Prochaine etape\n",
    "Dans le prochain notebook, nous apprendrons a stocker nos fichiers dans **MinIO** (notre Data Lake)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
