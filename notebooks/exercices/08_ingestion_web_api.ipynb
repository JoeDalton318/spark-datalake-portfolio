{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66946b72",
   "metadata": {},
   "source": [
    "# Exercice 08 - Ingestion depuis le Web (API)\n",
    "\n",
    "## Objectifs\n",
    "- Appeler une API REST avec Python\n",
    "- Transformer les donnees JSON en DataFrame\n",
    "- Ingerer les donnees web dans Bronze\n",
    "- Gerer les erreurs et les retries\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f0ba96",
   "metadata": {},
   "source": [
    "## 1. Architecture d'ingestion Web\n",
    "\n",
    "```\n",
    "+------------------+                    +------------------------+\n",
    "|                  |                    |        MinIO           |\n",
    "|    INTERNET      |     PYTHON         |                        |\n",
    "|                  |    + SPARK         |  +------------------+  |\n",
    "|  +------------+  |  =============>    |  |     BRONZE       |  |\n",
    "|  | API REST   |  |                    |  +------------------+  |\n",
    "|  | (JSON)     |  |                    |  | /api_users/      |  |\n",
    "|  +------------+  |                    |  |   /2024-01-15/   |  |\n",
    "|                  |                    |  | /api_posts/      |  |\n",
    "|  +------------+  |                    |  |   /2024-01-15/   |  |\n",
    "|  | Fichiers   |  |                    |  +------------------+  |\n",
    "|  | CSV/JSON   |  |                    |                        |\n",
    "|  +------------+  |                    +------------------------+\n",
    "|                  |\n",
    "+------------------+\n",
    "\n",
    "Etapes :\n",
    "1. Appeler l'API avec requests\n",
    "2. Parser le JSON\n",
    "3. Creer un DataFrame Spark\n",
    "4. Sauvegarder dans Bronze\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f94b3c",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6e8b0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark pret - Date : 2026-01-14\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Creer la SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Ingestion Web\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.4.1,com.amazonaws:aws-java-sdk-bundle:1.12.262\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "date_ingestion = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "print(f\"Spark pret - Date : {date_ingestion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412e4e7a",
   "metadata": {},
   "source": [
    "## 3. Appeler une API REST simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cab5c050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code : 200\n",
      "Content-Type : application/json; charset=utf-8\n"
     ]
    }
   ],
   "source": [
    "# API de test : JSONPlaceholder (API publique gratuite)\n",
    "url_users = \"https://jsonplaceholder.typicode.com/users\"\n",
    "\n",
    "# Appeler l'API\n",
    "response = requests.get(url_users)\n",
    "\n",
    "print(f\"Status code : {response.status_code}\")\n",
    "print(f\"Content-Type : {response.headers.get('Content-Type')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "979e26b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type : <class 'list'>\n",
      "Nombre d'elements : 10\n",
      "\n",
      "Premier element :\n",
      "{\n",
      "  \"id\": 1,\n",
      "  \"name\": \"Leanne Graham\",\n",
      "  \"username\": \"Bret\",\n",
      "  \"email\": \"Sincere@april.biz\",\n",
      "  \"address\": {\n",
      "    \"street\": \"Kulas Light\",\n",
      "    \"suite\": \"Apt. 556\",\n",
      "    \"city\": \"Gwenborough\",\n",
      "    \"zipcode\": \"92998-3874\",\n",
      "    \"geo\": {\n",
      "      \"lat\": \"-37.3159\",\n",
      "      \"lng\": \"81.1496\"\n",
      "    }\n",
      "  },\n",
      "  \"phone\": \"1-770-736-8031 x56442\",\n",
      "  \"website\": \"hildegard.org\",\n",
      "  \"company\": {\n",
      "    \"name\": \"Romaguera-Crona\",\n",
      "    \"catchPhrase\": \"Multi-layered client-server neural-net\",\n",
      "    \"bs\": \"harness real-time e-markets\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Parser le JSON\n",
    "data = response.json()\n",
    "\n",
    "print(f\"Type : {type(data)}\")\n",
    "print(f\"Nombre d'elements : {len(data)}\")\n",
    "print(\"\\nPremier element :\")\n",
    "print(json.dumps(data[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40a1626",
   "metadata": {},
   "source": [
    "## 4. Convertir en DataFrame Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a11aae2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema :\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- username: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- address: struct (nullable = true)\n",
      " |    |-- street: string (nullable = true)\n",
      " |    |-- suite: string (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |    |-- zipcode: string (nullable = true)\n",
      " |    |-- geo: struct (nullable = true)\n",
      " |    |    |-- lat: string (nullable = true)\n",
      " |    |    |-- lng: string (nullable = true)\n",
      " |-- phone: string (nullable = true)\n",
      " |-- website: string (nullable = true)\n",
      " |-- company: struct (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- catchPhrase: string (nullable = true)\n",
      " |    |-- bs: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creer un DataFrame depuis une liste Python\n",
    "# Les donnees de l'API ont des structures imbriquees (address, company, geo)\n",
    "# On definit le schema explicitement pour eviter les erreurs d'inference\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "# Schema pour les donnees JSONPlaceholder /users\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"username\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"address\", StructType([\n",
    "        StructField(\"street\", StringType(), True),\n",
    "        StructField(\"suite\", StringType(), True),\n",
    "        StructField(\"city\", StringType(), True),\n",
    "        StructField(\"zipcode\", StringType(), True),\n",
    "        StructField(\"geo\", StructType([\n",
    "            StructField(\"lat\", StringType(), True),\n",
    "            StructField(\"lng\", StringType(), True)\n",
    "        ]), True)\n",
    "    ]), True),\n",
    "    StructField(\"phone\", StringType(), True),\n",
    "    StructField(\"website\", StringType(), True),\n",
    "    StructField(\"company\", StructType([\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"catchPhrase\", StringType(), True),\n",
    "        StructField(\"bs\", StringType(), True)\n",
    "    ]), True)\n",
    "])\n",
    "\n",
    "df_users = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "print(\"Schema :\")\n",
    "df_users.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "662ef1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+\n",
      "| id|                name|               email|               phone|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "|  1|       Leanne Graham|   Sincere@april.biz|1-770-736-8031 x5...|\n",
      "|  2|        Ervin Howell|   Shanna@melissa.tv| 010-692-6593 x09125|\n",
      "|  3|    Clementine Bauch|  Nathan@yesenia.net|      1-463-123-4447|\n",
      "|  4|    Patricia Lebsack|Julianne.OConner@...|   493-170-9623 x156|\n",
      "|  5|    Chelsey Dietrich|Lucio_Hettinger@a...|       (254)954-1289|\n",
      "|  6|Mrs. Dennis Schulist|Karley_Dach@jaspe...|1-477-935-8478 x6430|\n",
      "|  7|     Kurtis Weissnat|Telly.Hoeger@bill...|        210.067.6132|\n",
      "|  8|Nicholas Runolfsd...|Sherwood@rosamond.me|   586.493.6943 x140|\n",
      "|  9|     Glenna Reichert|Chaim_McDermott@d...|(775)976-6794 x41206|\n",
      "| 10|  Clementina DuBuque|Rey.Padberg@karin...|        024-648-3804|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Afficher les donnees\n",
    "df_users.select(\"id\", \"name\", \"email\", \"phone\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "084d2de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------+----------------+-------------------------+---------------------+-------------+-----------------+--------------+----------+------------------+\n",
      "|id |name                    |username        |email                    |phone                |website      |street           |city          |zipcode   |company_name      |\n",
      "+---+------------------------+----------------+-------------------------+---------------------+-------------+-----------------+--------------+----------+------------------+\n",
      "|1  |Leanne Graham           |Bret            |Sincere@april.biz        |1-770-736-8031 x56442|hildegard.org|Kulas Light      |Gwenborough   |92998-3874|Romaguera-Crona   |\n",
      "|2  |Ervin Howell            |Antonette       |Shanna@melissa.tv        |010-692-6593 x09125  |anastasia.net|Victor Plains    |Wisokyburgh   |90566-7771|Deckow-Crist      |\n",
      "|3  |Clementine Bauch        |Samantha        |Nathan@yesenia.net       |1-463-123-4447       |ramiro.info  |Douglas Extension|McKenziehaven |59590-4157|Romaguera-Jacobson|\n",
      "|4  |Patricia Lebsack        |Karianne        |Julianne.OConner@kory.org|493-170-9623 x156    |kale.biz     |Hoeger Mall      |South Elvis   |53919-4257|Robel-Corkery     |\n",
      "|5  |Chelsey Dietrich        |Kamren          |Lucio_Hettinger@annie.ca |(254)954-1289        |demarco.info |Skiles Walks     |Roscoeview    |33263     |Keebler LLC       |\n",
      "|6  |Mrs. Dennis Schulist    |Leopoldo_Corkery|Karley_Dach@jasper.info  |1-477-935-8478 x6430 |ola.org      |Norberto Crossing|South Christy |23505-1337|Considine-Lockman |\n",
      "|7  |Kurtis Weissnat         |Elwyn.Skiles    |Telly.Hoeger@billy.biz   |210.067.6132         |elvis.io     |Rex Trail        |Howemouth     |58804-1099|Johns Group       |\n",
      "|8  |Nicholas Runolfsdottir V|Maxime_Nienow   |Sherwood@rosamond.me     |586.493.6943 x140    |jacynthe.com |Ellsworth Summit |Aliyaview     |45169     |Abernathy Group   |\n",
      "|9  |Glenna Reichert         |Delphine        |Chaim_McDermott@dana.io  |(775)976-6794 x41206 |conrad.com   |Dayna Park       |Bartholomebury|76495-3109|Yost and Sons     |\n",
      "|10 |Clementina DuBuque      |Moriah.Stanton  |Rey.Padberg@karina.biz   |024-648-3804         |ambrose.net  |Kattie Turnpike  |Lebsackbury   |31428-2261|Hoeger LLC        |\n",
      "+---+------------------------+----------------+-------------------------+---------------------+-------------+-----------------+--------------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aplatir les structures imbriquees\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_users_flat = df_users.select(\n",
    "    \"id\",\n",
    "    \"name\",\n",
    "    \"username\",\n",
    "    \"email\",\n",
    "    \"phone\",\n",
    "    \"website\",\n",
    "    F.col(\"address.street\").alias(\"street\"),\n",
    "    F.col(\"address.city\").alias(\"city\"),\n",
    "    F.col(\"address.zipcode\").alias(\"zipcode\"),\n",
    "    F.col(\"company.name\").alias(\"company_name\")\n",
    ")\n",
    "\n",
    "df_users_flat.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b9122b",
   "metadata": {},
   "source": [
    "## 5. Sauvegarder dans Bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55c553c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sauvegarde reussie : s3a://bronze/api_users/2026-01-14\n",
      "Lignes ingerees : 10\n"
     ]
    }
   ],
   "source": [
    "# Ajouter les metadonnees\n",
    "df_final = df_users_flat \\\n",
    "    .withColumn(\"_source\", F.lit(\"api_jsonplaceholder\")) \\\n",
    "    .withColumn(\"_endpoint\", F.lit(\"users\")) \\\n",
    "    .withColumn(\"_ingestion_date\", F.lit(date_ingestion))\n",
    "\n",
    "# Sauvegarder\n",
    "chemin_bronze = f\"s3a://bronze/api_users/{date_ingestion}\"\n",
    "df_final.write.mode(\"overwrite\").parquet(chemin_bronze)\n",
    "\n",
    "print(f\"Sauvegarde reussie : {chemin_bronze}\")\n",
    "print(f\"Lignes ingerees : {df_final.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061a0b2c",
   "metadata": {},
   "source": [
    "## 6. Fonction d'ingestion API generique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25e1ded0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingerer_api(url, nom_dataset, date=None):\n",
    "    \"\"\"\n",
    "    Ingere des donnees depuis une API REST vers Bronze.\n",
    "    \n",
    "    Args:\n",
    "        url: URL de l'API\n",
    "        nom_dataset: Nom du dataset pour le chemin Bronze\n",
    "        date: Date d'ingestion (optionnel)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Statistiques d'ingestion\n",
    "    \"\"\"\n",
    "    if date is None:\n",
    "        date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    try:\n",
    "        # Appeler l'API\n",
    "        response = requests.get(url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parser le JSON\n",
    "        data = response.json()\n",
    "        \n",
    "        # Creer le DataFrame\n",
    "        if isinstance(data, list):\n",
    "            df = spark.createDataFrame(data)\n",
    "        else:\n",
    "            df = spark.createDataFrame([data])\n",
    "        \n",
    "        # Ajouter les metadonnees\n",
    "        df = df.withColumn(\"_source\", F.lit(url)) \\\n",
    "               .withColumn(\"_ingestion_date\", F.lit(date))\n",
    "        \n",
    "        # Sauvegarder\n",
    "        chemin = f\"s3a://bronze/{nom_dataset}/{date}\"\n",
    "        df.write.mode(\"overwrite\").parquet(chemin)\n",
    "        \n",
    "        nb_lignes = df.count()\n",
    "        print(f\"[OK] {nom_dataset} : {nb_lignes} lignes -> {chemin}\")\n",
    "        \n",
    "        return {\"status\": \"OK\", \"lignes\": nb_lignes, \"chemin\": chemin}\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"[ERREUR] {nom_dataset} : Erreur API - {e}\")\n",
    "        return {\"status\": \"ERREUR\", \"erreur\": str(e)}\n",
    "    except Exception as e:\n",
    "        print(f\"[ERREUR] {nom_dataset} : {e}\")\n",
    "        return {\"status\": \"ERREUR\", \"erreur\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ba3fe8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion multi-endpoints :\n",
      "==================================================\n",
      "[OK] api_posts : 100 lignes -> s3a://bronze/api_posts/2026-01-14\n",
      "[OK] api_comments : 500 lignes -> s3a://bronze/api_comments/2026-01-14\n",
      "[OK] api_todos : 200 lignes -> s3a://bronze/api_todos/2026-01-14\n"
     ]
    }
   ],
   "source": [
    "# Tester avec d'autres endpoints\n",
    "endpoints = [\n",
    "    (\"https://jsonplaceholder.typicode.com/posts\", \"api_posts\"),\n",
    "    (\"https://jsonplaceholder.typicode.com/comments\", \"api_comments\"),\n",
    "    (\"https://jsonplaceholder.typicode.com/todos\", \"api_todos\")\n",
    "]\n",
    "\n",
    "print(\"Ingestion multi-endpoints :\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for url, nom in endpoints:\n",
    "    ingerer_api(url, nom, date_ingestion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e693928",
   "metadata": {},
   "source": [
    "## 7. Gestion des erreurs et retries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "441f9125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def appeler_api_avec_retry(url, max_retries=3, delay=2):\n",
    "    \"\"\"\n",
    "    Appelle une API avec retry en cas d'erreur.\n",
    "    \n",
    "    Args:\n",
    "        url: URL de l'API\n",
    "        max_retries: Nombre maximum de tentatives\n",
    "        delay: Delai entre les tentatives (secondes)\n",
    "    \n",
    "    Returns:\n",
    "        Donnees JSON ou None\n",
    "    \"\"\"\n",
    "    for tentative in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"  Tentative {tentative + 1}/{max_retries} echouee : {e}\")\n",
    "            if tentative < max_retries - 1:\n",
    "                print(f\"  Attente de {delay} secondes...\")\n",
    "                time.sleep(delay)\n",
    "                delay *= 2  # Backoff exponentiel\n",
    "    \n",
    "    print(f\"  Echec apres {max_retries} tentatives\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b94272a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succes : 100 elements recuperes\n"
     ]
    }
   ],
   "source": [
    "# Test avec une URL valide\n",
    "data = appeler_api_avec_retry(\"https://jsonplaceholder.typicode.com/albums\")\n",
    "if data:\n",
    "    print(f\"Succes : {len(data)} elements recuperes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa9aa82",
   "metadata": {},
   "source": [
    "## 8. Ingerer un fichier CSV depuis le Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c58710c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donnees COVID : 161568 lignes\n",
      "+----------+-----------+---------+---------+------+\n",
      "|      Date|    Country|Confirmed|Recovered|Deaths|\n",
      "+----------+-----------+---------+---------+------+\n",
      "|2020-01-22|Afghanistan|        0|        0|     0|\n",
      "|2020-01-23|Afghanistan|        0|        0|     0|\n",
      "|2020-01-24|Afghanistan|        0|        0|     0|\n",
      "|2020-01-25|Afghanistan|        0|        0|     0|\n",
      "|2020-01-26|Afghanistan|        0|        0|     0|\n",
      "+----------+-----------+---------+---------+------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Telecharger un CSV depuis une URL\n",
    "url_csv = \"https://raw.githubusercontent.com/datasets/covid-19/main/data/countries-aggregated.csv\"\n",
    "\n",
    "# Telecharger et lire le CSV\n",
    "response = requests.get(url_csv)\n",
    "if response.status_code == 200:\n",
    "    # Sauvegarder temporairement\n",
    "    with open(\"/tmp/covid_data.csv\", \"w\") as f:\n",
    "        f.write(response.text)\n",
    "    \n",
    "    # Lire avec Spark\n",
    "    df_covid = spark.read.csv(\"/tmp/covid_data.csv\", header=True, inferSchema=True)\n",
    "    print(f\"Donnees COVID : {df_covid.count()} lignes\")\n",
    "    df_covid.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a507b976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sauvegarde : s3a://bronze/covid_data/2026-01-14\n"
     ]
    }
   ],
   "source": [
    "# Sauvegarder dans Bronze\n",
    "chemin = f\"s3a://bronze/covid_data/{date_ingestion}\"\n",
    "df_covid.write.mode(\"overwrite\").parquet(chemin)\n",
    "print(f\"Sauvegarde : {chemin}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b6a658",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercice\n",
    "\n",
    "**Objectif** : Ingerer des donnees depuis une API publique\n",
    "\n",
    "**Consigne** :\n",
    "1. Utilisez l'API REST Countries : `https://restcountries.com/v3.1/all`\n",
    "2. Extrayez les champs : nom, capitale, region, population, superficie\n",
    "3. Sauvegardez dans Bronze\n",
    "\n",
    "A vous de jouer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25d5cc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données récupérées : 250 pays trouvés.\n",
      "Exemple: {'name': {'common': 'Antigua and Barbuda', 'official': 'Antigua and Barbuda', 'nativeName': {'eng': {'official': 'Antigua and Barbuda', 'common': 'Antigua and Barbuda'}}}, 'capital': [\"Saint John's\"], 'region': 'Americas', 'area': 442.0, 'population': 103603}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Appeler l'API REST Countries\n",
    "import requests\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime\n",
    "\n",
    "url_countries = \"https://restcountries.com/v3.1/all?fields=name,capital,region,population,area\"\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "response = requests.get(url_countries, headers=headers)\n",
    "\n",
    "# Vérification du statut\n",
    "if response.status_code == 200:\n",
    "    data_raw = response.json()\n",
    "    print(f\"Données récupérées : {len(data_raw)} pays trouvés.\")\n",
    "    \n",
    "    if len(data_raw) > 0:\n",
    "        print(\"Exemple:\", data_raw[0])\n",
    "else:\n",
    "    print(f\"Erreur API : {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51337f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame créé avec 250 lignes.\n",
      "+------------+-------------------+----------+--------+----------+\n",
      "|    capitale|                nom|population|  region|superficie|\n",
      "+------------+-------------------+----------+--------+----------+\n",
      "|Saint John's|Antigua and Barbuda|    103603|Americas|     442.0|\n",
      "|     Thimphu|             Bhutan|    784043|    Asia|   38394.0|\n",
      "|        Rome|              Italy|  58927633|  Europe|  301336.0|\n",
      "|    Funafuti|             Tuvalu|     10643| Oceania|      26.0|\n",
      "|  The Valley|           Anguilla|     16010|Americas|      91.0|\n",
      "+------------+-------------------+----------+--------+----------+\n",
      "only showing top 5 rows\n",
      "root\n",
      " |-- capitale: string (nullable = true)\n",
      " |-- nom: string (nullable = true)\n",
      " |-- population: long (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- superficie: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Extraire les champs utiles et creer un DataFrame\n",
    "pays_clean = []\n",
    "\n",
    "if 'data_raw' in locals() and data_raw:\n",
    "    for item in data_raw:\n",
    "        nom = item.get(\"name\", {}).get(\"common\", \"Inconnu\")\n",
    "        capital_list = item.get(\"capital\", [])\n",
    "        capitale = capital_list[0] if capital_list else None\n",
    "        region = item.get(\"region\")\n",
    "        population = item.get(\"population\")\n",
    "        superficie = item.get(\"area\")\n",
    "        \n",
    "        # Ajout à la liste\n",
    "        pays_clean.append({\n",
    "            \"nom\": nom,\n",
    "            \"capitale\": capitale,\n",
    "            \"region\": region,\n",
    "            \"population\": population,\n",
    "            \"superficie\": superficie\n",
    "        })\n",
    "\n",
    "    # Création du DataFrame Spark\n",
    "    df_countries = spark.createDataFrame(pays_clean)\n",
    "\n",
    "    print(f\"DataFrame créé avec {df_countries.count()} lignes.\")\n",
    "    df_countries.show(5)\n",
    "    df_countries.printSchema()\n",
    "\n",
    "else:\n",
    "    print(\"Erreur : La variable 'data_raw' est vide. Vérifiez l'étape 1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d33adf57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sauvegarde réussie dans : s3a://bronze/api_countries/2026-01-15\n",
      "Vérification lecture :\n",
      "+-------------+--------------+---------------+\n",
      "|          nom|      capitale|_ingestion_date|\n",
      "+-------------+--------------+---------------+\n",
      "|        India|     New Delhi|     2026-01-15|\n",
      "|   Martinique|Fort-de-France|     2026-01-15|\n",
      "|New Caledonia|        Nouméa|     2026-01-15|\n",
      "+-------------+--------------+---------------+\n",
      "only showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "# TODO: Sauvegarder dans Bronze\n",
    "if 'df_countries' in locals():\n",
    "    from datetime import datetime\n",
    "    date_ingestion_exo = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    chemin_bronze = f\"s3a://bronze/api_countries/{date_ingestion_exo}\"\n",
    "\n",
    "    # Ajout des métadonnées de traçabilité\n",
    "    df_final = df_countries \\\n",
    "        .withColumn(\"_source\", F.lit(\"restcountries_api\")) \\\n",
    "        .withColumn(\"_ingestion_date\", F.lit(date_ingestion_exo))\n",
    "\n",
    "    # Écriture en mode overwrite\n",
    "    df_final.write.mode(\"overwrite\").parquet(chemin_bronze)\n",
    "\n",
    "    print(f\"✅ Sauvegarde réussie dans : {chemin_bronze}\")\n",
    "    \n",
    "    # Petite vérification de lecture\n",
    "    print(\"Vérification lecture :\")\n",
    "    spark.read.parquet(chemin_bronze).select(\"nom\", \"capitale\", \"_ingestion_date\").show(3)\n",
    "\n",
    "else:\n",
    "    print(\"Erreur : DataFrame 'df_countries' introuvable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16866e60",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Resume\n",
    "\n",
    "Dans ce notebook, vous avez appris :\n",
    "- Comment **appeler une API REST** avec Python requests\n",
    "- Comment **convertir du JSON** en DataFrame Spark\n",
    "- Comment **aplatir** les structures imbriquees\n",
    "- Comment **gerer les erreurs** avec retry\n",
    "- Comment **ingerer des fichiers** CSV depuis le Web\n",
    "\n",
    "### Prochaine etape\n",
    "Dans le prochain notebook, nous apprendrons a nettoyer les donnees pour la couche Silver."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
