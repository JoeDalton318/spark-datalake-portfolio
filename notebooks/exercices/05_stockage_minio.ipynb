{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45b0c894",
   "metadata": {},
   "source": [
    "# Exercice 05 - Stockage dans MinIO (Data Lake)\n",
    "\n",
    "## Objectifs\n",
    "- Configurer Spark pour se connecter a MinIO\n",
    "- Lire et ecrire des donnees dans MinIO\n",
    "- Organiser les donnees en Bronze / Silver / Gold\n",
    "- Comprendre les chemins S3\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcc0ef5",
   "metadata": {},
   "source": [
    "## 1. Rappel : Architecture du Data Lake\n",
    "\n",
    "```\n",
    "+------------------------------------------------------------------+\n",
    "|                         MINIO (S3)                               |\n",
    "+------------------------------------------------------------------+\n",
    "|                                                                  |\n",
    "|  +----------------+  +----------------+  +----------------+      |\n",
    "|  |    BRONZE      |  |    SILVER      |  |     GOLD       |      |\n",
    "|  |                |  |                |  |                |      |\n",
    "|  | Donnees brutes |  | Donnees        |  | Donnees        |      |\n",
    "|  | telles que     |  | nettoyees      |  | agregees       |      |\n",
    "|  | recues         |  | et typees      |  | pret pour      |      |\n",
    "|  |                |  |                |  | l'analyse      |      |\n",
    "|  | s3a://bronze/  |  | s3a://silver/  |  | s3a://gold/    |      |\n",
    "|  +----------------+  +----------------+  +----------------+      |\n",
    "|                                                                  |\n",
    "+------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c7dc99",
   "metadata": {},
   "source": [
    "## 2. Configuration de Spark pour MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5023ba34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark configure pour MinIO !\n",
      "Version: 4.0.1\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Arreter la session existante si elle existe (necessaire pour charger les JARs)\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Configuration de Spark pour se connecter a MinIO\n",
    "# Les JARs AWS sont telecharges automatiquement au premier lancement\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark MinIO\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.6.0,org.apache.hadoop:hadoop-aws:3.4.1,com.amazonaws:aws-java-sdk-bundle:1.12.262\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark configure pour MinIO !\")\n",
    "print(f\"Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c77a45",
   "metadata": {},
   "source": [
    "### Explication de la configuration\n",
    "\n",
    "| Option | Description |\n",
    "|--------|-------------|\n",
    "| `fs.s3a.endpoint` | Adresse du serveur MinIO |\n",
    "| `fs.s3a.access.key` | Identifiant de connexion |\n",
    "| `fs.s3a.secret.key` | Mot de passe |\n",
    "| `fs.s3a.path.style.access` | Necessaire pour MinIO |\n",
    "| `fs.s3a.impl` | Implementation du systeme de fichiers S3 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af5c0af",
   "metadata": {},
   "source": [
    "## 3. Creation des buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb3e145b-9978-450f-a5a4-b54dbc3273e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: minio in /opt/conda/lib/python3.13/site-packages (7.2.20)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.13/site-packages (from minio) (25.1.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.13/site-packages (from minio) (2025.11.12)\n",
      "Requirement already satisfied: pycryptodome in /opt/conda/lib/python3.13/site-packages (from minio) (3.23.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.13/site-packages (from minio) (4.15.0)\n",
      "Requirement already satisfied: urllib3 in /opt/conda/lib/python3.13/site-packages (from minio) (2.5.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /opt/conda/lib/python3.13/site-packages (from argon2-cffi->minio) (25.1.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /opt/conda/lib/python3.13/site-packages (from argon2-cffi-bindings->argon2-cffi->minio) (2.0.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.13/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->minio) (2.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8097a826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 'bronze' existe deja\n",
      "Bucket 'silver' existe deja\n",
      "Bucket 'gold' existe deja\n"
     ]
    }
   ],
   "source": [
    "from minio import Minio\n",
    "\n",
    "# Connexion a MinIO\n",
    "client = Minio(\n",
    "    \"minio:9000\",\n",
    "    access_key=\"minioadmin\",\n",
    "    secret_key=\"minioadmin123\",\n",
    "    secure=False\n",
    ")\n",
    "\n",
    "# Creer les buckets s'ils n'existent pas\n",
    "for bucket in [\"bronze\", \"silver\", \"gold\"]:\n",
    "    if not client.bucket_exists(bucket):\n",
    "        client.make_bucket(bucket)\n",
    "        print(f\"Bucket '{bucket}' cree\")\n",
    "    else:\n",
    "        print(f\"Bucket '{bucket}' existe deja\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcf4553",
   "metadata": {},
   "source": [
    "## 4. Creer des donnees de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6045339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+---------+--------+-------------+\n",
      "|      date|produit|    ville|quantite|prix_unitaire|\n",
      "+----------+-------+---------+--------+-------------+\n",
      "|2025-01-01| Laptop|    Paris|       2|       999.99|\n",
      "|2025-01-01| Souris|     Lyon|      10|        29.99|\n",
      "|2025-01-02|Clavier|    Paris|       5|        79.99|\n",
      "|2025-01-02|  Ecran|Marseille|       3|       299.99|\n",
      "|2025-01-03| Laptop|     Lyon|       1|       999.99|\n",
      "|2025-01-03| Casque|    Paris|       8|       149.99|\n",
      "+----------+-------+---------+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creer un DataFrame de ventes\n",
    "ventes = [\n",
    "    (\"2025-01-01\", \"Laptop\", \"Paris\", 2, 999.99),\n",
    "    (\"2025-01-01\", \"Souris\", \"Lyon\", 10, 29.99),\n",
    "    (\"2025-01-02\", \"Clavier\", \"Paris\", 5, 79.99),\n",
    "    (\"2025-01-02\", \"Ecran\", \"Marseille\", 3, 299.99),\n",
    "    (\"2025-01-03\", \"Laptop\", \"Lyon\", 1, 999.99),\n",
    "    (\"2025-01-03\", \"Casque\", \"Paris\", 8, 149.99)\n",
    "]\n",
    "\n",
    "colonnes = [\"date\", \"produit\", \"ville\", \"quantite\", \"prix_unitaire\"]\n",
    "\n",
    "df_ventes = spark.createDataFrame(ventes, colonnes)\n",
    "df_ventes.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6635f92",
   "metadata": {},
   "source": [
    "## 5. Ecriture dans Bronze\n",
    "\n",
    "Les chemins S3 ont le format : `s3a://bucket/chemin/fichier`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9089a94e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebe98208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donnees ecrites dans : s3a://bronze/ventes/raw\n"
     ]
    }
   ],
   "source": [
    "# Ecrire les donnees brutes dans Bronze\n",
    "chemin_bronze = \"s3a://bronze/ventes/raw\"\n",
    "\n",
    "df_ventes.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(chemin_bronze)\n",
    "\n",
    "print(f\"Donnees ecrites dans : {chemin_bronze}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4f8fc8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lecture depuis Bronze :\n",
      "+----------+-------+---------+--------+-------------+\n",
      "|      date|produit|    ville|quantite|prix_unitaire|\n",
      "+----------+-------+---------+--------+-------------+\n",
      "|2025-01-02|  Ecran|Marseille|       3|       299.99|\n",
      "|2025-01-02|Clavier|    Paris|       5|        79.99|\n",
      "|2025-01-01| Laptop|    Paris|       2|       999.99|\n",
      "|2025-01-03| Casque|    Paris|       8|       149.99|\n",
      "|2025-01-03| Laptop|     Lyon|       1|       999.99|\n",
      "|2025-01-01| Souris|     Lyon|      10|        29.99|\n",
      "+----------+-------+---------+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verifier en relisant\n",
    "df_bronze = spark.read.parquet(chemin_bronze)\n",
    "print(\"Lecture depuis Bronze :\")\n",
    "df_bronze.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b57a9b",
   "metadata": {},
   "source": [
    "## 6. Transformation vers Silver\n",
    "\n",
    "En Silver, on nettoie et on enrichit les donnees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8941d009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+---------+--------+-------------+-------------+\n",
      "|      date|produit|    ville|quantite|prix_unitaire|montant_total|\n",
      "+----------+-------+---------+--------+-------------+-------------+\n",
      "|2025-01-02|  Ecran|Marseille|       3|       299.99|       899.97|\n",
      "|2025-01-02|Clavier|    Paris|       5|        79.99|       399.95|\n",
      "|2025-01-01| Laptop|    Paris|       2|       999.99|      1999.98|\n",
      "|2025-01-03| Casque|    Paris|       8|       149.99|      1199.92|\n",
      "|2025-01-03| Laptop|     Lyon|       1|       999.99|       999.99|\n",
      "|2025-01-01| Souris|     Lyon|      10|        29.99|        299.9|\n",
      "+----------+-------+---------+--------+-------------+-------------+\n",
      "\n",
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- produit: string (nullable = true)\n",
      " |-- ville: string (nullable = true)\n",
      " |-- quantite: long (nullable = true)\n",
      " |-- prix_unitaire: double (nullable = true)\n",
      " |-- montant_total: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "# Transformation : ajouter le montant total\n",
    "df_silver = df_bronze \\\n",
    "    .withColumn(\"date\", to_date(col(\"date\"))) \\\n",
    "    .withColumn(\"montant_total\", col(\"quantite\") * col(\"prix_unitaire\"))\n",
    "\n",
    "df_silver.show()\n",
    "df_silver.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1619ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donnees ecrites dans : s3a://silver/ventes/enriched\n"
     ]
    }
   ],
   "source": [
    "# Ecrire dans Silver\n",
    "chemin_silver = \"s3a://silver/ventes/enriched\"\n",
    "\n",
    "df_silver.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(chemin_silver)\n",
    "\n",
    "print(f\"Donnees ecrites dans : {chemin_silver}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484e19d2",
   "metadata": {},
   "source": [
    "## 7. Agregation vers Gold\n",
    "\n",
    "En Gold, on prepare les donnees pour l'analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a14ddbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+------------------+-----------------+\n",
      "|    ville|nb_ventes|total_quantite|  chiffre_affaires|     panier_moyen|\n",
      "+---------+---------+--------------+------------------+-----------------+\n",
      "|Marseille|        1|             3|            899.97|           899.97|\n",
      "|    Paris|        3|            15|           3599.85|          1199.95|\n",
      "|     Lyon|        2|            11|1299.8899999999999|649.9449999999999|\n",
      "+---------+---------+--------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, count, avg\n",
    "\n",
    "# Lire depuis Silver\n",
    "df_silver = spark.read.parquet(chemin_silver)\n",
    "\n",
    "# Agregation par ville\n",
    "df_gold_ville = df_silver.groupBy(\"ville\").agg(\n",
    "    count(\"*\").alias(\"nb_ventes\"),\n",
    "    sum(\"quantite\").alias(\"total_quantite\"),\n",
    "    sum(\"montant_total\").alias(\"chiffre_affaires\"),\n",
    "    avg(\"montant_total\").alias(\"panier_moyen\")\n",
    ")\n",
    "\n",
    "df_gold_ville.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b068b270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donnees ecrites dans : s3a://gold/ventes/par_ville\n"
     ]
    }
   ],
   "source": [
    "# Ecrire dans Gold\n",
    "chemin_gold = \"s3a://gold/ventes/par_ville\"\n",
    "\n",
    "df_gold_ville.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(chemin_gold)\n",
    "\n",
    "print(f\"Donnees ecrites dans : {chemin_gold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd3a8b5",
   "metadata": {},
   "source": [
    "## 8. Verification du Data Lake\n",
    "\n",
    "Listons les objets dans chaque bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9699a5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Bucket: bronze ===\n",
      "  clients/ (0 octets)\n",
      "  clients/raw/_SUCCESS (0 octets)\n",
      "  clients/raw/part-00000-86c65917-8879-4404-8523-9c31de6d0601-c000.snappy.parquet (649 octets)\n",
      "  clients/raw/part-00002-86c65917-8879-4404-8523-9c31de6d0601-c000.snappy.parquet (1732 octets)\n",
      "  clients/raw/part-00004-86c65917-8879-4404-8523-9c31de6d0601-c000.snappy.parquet (1691 octets)\n",
      "  clients/raw/part-00007-86c65917-8879-4404-8523-9c31de6d0601-c000.snappy.parquet (1761 octets)\n",
      "  clients/raw/part-00009-86c65917-8879-4404-8523-9c31de6d0601-c000.snappy.parquet (1698 octets)\n",
      "  clients/raw/part-00011-86c65917-8879-4404-8523-9c31de6d0601-c000.snappy.parquet (1711 octets)\n",
      "  demo/premier_fichier.json (107 octets)\n",
      "  exercice/mon_profil.json (64 octets)\n",
      "  ventes/ (0 octets)\n",
      "  ventes/raw/_SUCCESS (0 octets)\n",
      "  ventes/raw/part-00000-afd7ff3d-5b75-4e94-b28a-f292741f3b19-c000.snappy.parquet (657 octets)\n",
      "  ventes/raw/part-00001-afd7ff3d-5b75-4e94-b28a-f292741f3b19-c000.snappy.parquet (1641 octets)\n",
      "  ventes/raw/part-00003-afd7ff3d-5b75-4e94-b28a-f292741f3b19-c000.snappy.parquet (1633 octets)\n",
      "  ventes/raw/part-00005-afd7ff3d-5b75-4e94-b28a-f292741f3b19-c000.snappy.parquet (1648 octets)\n",
      "  ventes/raw/part-00007-afd7ff3d-5b75-4e94-b28a-f292741f3b19-c000.snappy.parquet (1662 octets)\n",
      "  ventes/raw/part-00009-afd7ff3d-5b75-4e94-b28a-f292741f3b19-c000.snappy.parquet (1634 octets)\n",
      "  ventes/raw/part-00011-afd7ff3d-5b75-4e94-b28a-f292741f3b19-c000.snappy.parquet (1641 octets)\n",
      "\n",
      "=== Bucket: silver ===\n",
      "  clients/ (0 octets)\n",
      "  clients/enriched/_SUCCESS (0 octets)\n",
      "  clients/enriched/part-00000-943c6d1e-f1f4-4def-99b8-75ff3f5b0dc4-c000.snappy.parquet (1986 octets)\n",
      "  clients/enriched/part-00001-943c6d1e-f1f4-4def-99b8-75ff3f5b0dc4-c000.snappy.parquet (1957 octets)\n",
      "  clients/enriched/part-00002-943c6d1e-f1f4-4def-99b8-75ff3f5b0dc4-c000.snappy.parquet (1936 octets)\n",
      "  clients/enriched/part-00003-943c6d1e-f1f4-4def-99b8-75ff3f5b0dc4-c000.snappy.parquet (1923 octets)\n",
      "  clients/enriched/part-00004-943c6d1e-f1f4-4def-99b8-75ff3f5b0dc4-c000.snappy.parquet (1916 octets)\n",
      "  ventes/ (0 octets)\n",
      "  ventes/enriched/_SUCCESS (0 octets)\n",
      "  ventes/enriched/part-00000-22175a4a-014f-4903-bc6c-d80ee02358cb-c000.snappy.parquet (1902 octets)\n",
      "  ventes/enriched/part-00001-22175a4a-014f-4903-bc6c-d80ee02358cb-c000.snappy.parquet (1888 octets)\n",
      "  ventes/enriched/part-00002-22175a4a-014f-4903-bc6c-d80ee02358cb-c000.snappy.parquet (1881 octets)\n",
      "  ventes/enriched/part-00003-22175a4a-014f-4903-bc6c-d80ee02358cb-c000.snappy.parquet (1881 octets)\n",
      "  ventes/enriched/part-00004-22175a4a-014f-4903-bc6c-d80ee02358cb-c000.snappy.parquet (1874 octets)\n",
      "  ventes/enriched/part-00005-22175a4a-014f-4903-bc6c-d80ee02358cb-c000.snappy.parquet (1873 octets)\n",
      "\n",
      "=== Bucket: gold ===\n",
      "  clients/ (0 octets)\n",
      "  clients/par_pays/_SUCCESS (0 octets)\n",
      "  clients/par_pays/part-00000-46bd1223-f33f-4b32-b224-58a779e54d32-c000.snappy.parquet (1723 octets)\n",
      "  ventes/ (0 octets)\n",
      "  ventes/par_ville/_SUCCESS (0 octets)\n",
      "  ventes/par_ville/part-00000-0efd6111-5636-46e9-a8ec-626ef1876d82-c000.snappy.parquet (1723 octets)\n"
     ]
    }
   ],
   "source": [
    "# Lister les objets dans chaque bucket\n",
    "for bucket in [\"bronze\", \"silver\", \"gold\"]:\n",
    "    print(f\"\\n=== Bucket: {bucket} ===\")\n",
    "    objets = client.list_objects(bucket, recursive=True)\n",
    "    for obj in objets:\n",
    "        print(f\"  {obj.object_name} ({obj.size} octets)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4470c873",
   "metadata": {},
   "source": [
    "## 9. Schema du pipeline\n",
    "\n",
    "```\n",
    "DONNEES SOURCES                      DATA LAKE (MinIO)\n",
    "---------------                      -----------------\n",
    "\n",
    "  [DataFrame]                        +-------------------+\n",
    "       |                             |      BRONZE       |\n",
    "       |  Ingestion brute            |  s3a://bronze/    |\n",
    "       +--------------------------->|  ventes/raw       |\n",
    "                                     +--------+----------+\n",
    "                                              |\n",
    "                                              | Nettoyage\n",
    "                                              | Enrichissement\n",
    "                                              v\n",
    "                                     +-------------------+\n",
    "                                     |      SILVER       |\n",
    "                                     |  s3a://silver/    |\n",
    "                                     |  ventes/enriched  |\n",
    "                                     +--------+----------+\n",
    "                                              |\n",
    "                                              | Agregation\n",
    "                                              | Indicateurs\n",
    "                                              v\n",
    "                                     +-------------------+\n",
    "                                     |       GOLD        |\n",
    "                                     |  s3a://gold/      |\n",
    "                                     |  ventes/par_ville |\n",
    "                                     +-------------------+\n",
    "                                              |\n",
    "                                              v\n",
    "                                        Reporting\n",
    "                                        BI Tools\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18957f7d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercice\n",
    "\n",
    "**Objectif** : Creer votre propre pipeline Bronze / Silver / Gold\n",
    "\n",
    "**Consigne** :\n",
    "1. Creez un DataFrame de clients (id, nom, email, pays, date_inscription)\n",
    "2. Ecrivez-le dans `s3a://bronze/clients/raw`\n",
    "3. Transformez : ajoutez une colonne `annee_inscription` extraite de la date\n",
    "4. Ecrivez dans `s3a://silver/clients/enriched`\n",
    "5. Agregez : comptez les clients par pays\n",
    "6. Ecrivez dans `s3a://gold/clients/par_pays`\n",
    "\n",
    "A vous de jouer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de0a6bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-----------------+------+----------------+\n",
      "| id|           nom|            email|  pays|date_inscription|\n",
      "+---+--------------+-----------------+------+----------------+\n",
      "|  1|  Alice Martin|  alice@email.com|France|      2023-03-15|\n",
      "|  2|   Bob Johnson|    bob@email.com|   USA|      2023-06-22|\n",
      "|  3|Charlie Dupont|charlie@email.com|France|      2024-01-10|\n",
      "|  4|   Diana Smith|  diana@email.com|    UK|      2023-11-05|\n",
      "|  5|   Eve Bernard|    eve@email.com|France|      2024-02-28|\n",
      "+---+--------------+-----------------+------+----------------+\n",
      "\n",
      "Donnees ecrites dans : s3a://bronze/clients/raw\n"
     ]
    }
   ],
   "source": [
    "# Donnees clients\n",
    "clients = [\n",
    "    (1, \"Alice Martin\", \"alice@email.com\", \"France\", \"2023-03-15\"),\n",
    "    (2, \"Bob Johnson\", \"bob@email.com\", \"USA\", \"2023-06-22\"),\n",
    "    (3, \"Charlie Dupont\", \"charlie@email.com\", \"France\", \"2024-01-10\"),\n",
    "    (4, \"Diana Smith\", \"diana@email.com\", \"UK\", \"2023-11-05\"),\n",
    "    (5, \"Eve Bernard\", \"eve@email.com\", \"France\", \"2024-02-28\")\n",
    "]\n",
    "\n",
    "colonnes = [\"id\", \"nom\", \"email\", \"pays\", \"date_inscription\"]\n",
    "\n",
    "# TODO: Creez le DataFrame\n",
    "df_clients = spark.createDataFrame (clients, colonnes)\n",
    "df_clients.show()\n",
    "\n",
    "# TODO: Ecrivez dans Bronze\n",
    "chemin_bronze_exo = \"s3a://bronze/clients/raw\"\n",
    "\n",
    "df_clients.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(chemin_bronze_exo)\n",
    "\n",
    "print(f\"Donnees ecrites dans : {chemin_bronze_exo}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e59a605-f538-4535-9d15-a2eb9bd0a2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lecture depuis Bronze :\n",
      "+---+--------------+-----------------+------+----------------+\n",
      "| id|           nom|            email|  pays|date_inscription|\n",
      "+---+--------------+-----------------+------+----------------+\n",
      "|  3|Charlie Dupont|charlie@email.com|France|      2024-01-10|\n",
      "|  1|  Alice Martin|  alice@email.com|France|      2023-03-15|\n",
      "|  5|   Eve Bernard|    eve@email.com|France|      2024-02-28|\n",
      "|  4|   Diana Smith|  diana@email.com|    UK|      2023-11-05|\n",
      "|  2|   Bob Johnson|    bob@email.com|   USA|      2023-06-22|\n",
      "+---+--------------+-----------------+------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bronze_exo = spark.read.parquet(chemin_bronze_exo)\n",
    "print(\"Lecture depuis Bronze :\")\n",
    "df_bronze_exo.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa12d0fa-d336-476d-a6bd-146462b5617c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- nom: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- pays: string (nullable = true)\n",
      " |-- date_inscription: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bronze_exo.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85238eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-----------------+------+----------------+-----------------+\n",
      "| id|           nom|            email|  pays|date_inscription|annee_inscription|\n",
      "+---+--------------+-----------------+------+----------------+-----------------+\n",
      "|  3|Charlie Dupont|charlie@email.com|France|      2024-01-10|             2024|\n",
      "|  1|  Alice Martin|  alice@email.com|France|      2023-03-15|             2023|\n",
      "|  5|   Eve Bernard|    eve@email.com|France|      2024-02-28|             2024|\n",
      "|  4|   Diana Smith|  diana@email.com|    UK|      2023-11-05|             2023|\n",
      "|  2|   Bob Johnson|    bob@email.com|   USA|      2023-06-22|             2023|\n",
      "+---+--------------+-----------------+------+----------------+-----------------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- nom: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- pays: string (nullable = true)\n",
      " |-- date_inscription: date (nullable = true)\n",
      " |-- annee_inscription: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Transformez et ecrivez dans Silver\n",
    "# Indice : utilisez year(to_date(col(\"date_inscription\"))) pour extraire l'annee\n",
    "\n",
    "from pyspark.sql.functions import year, to_date, col\n",
    "df_silver_exo = df_bronze_exo \\\n",
    "            .withColumn(\"date_inscription\", to_date(col(\"date_inscription\"))) \\\n",
    "            .withColumn(\"annee_inscription\", year(col(\"date_inscription\"))) \\\n",
    "\n",
    "df_silver_exo.show()\n",
    "df_silver_exo.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ff0b63b-1b6c-417e-81e0-40649ee8d22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donnees ecrites dans : s3a://silver/clients/enriched\n"
     ]
    }
   ],
   "source": [
    "#Ecriture dans Silver\n",
    "chemin_silver_exo = \"s3a://silver/clients/enriched\"\n",
    "\n",
    "df_silver_exo.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(chemin_silver_exo)\n",
    "\n",
    "print(f\"Donnees ecrites dans : {chemin_silver_exo}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b9d9f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|  pays|nb_clients|\n",
      "+------+----------+\n",
      "|France|         3|\n",
      "|    UK|         1|\n",
      "|   USA|         1|\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Agregez et ecrivez dans Gold\n",
    "\n",
    "\n",
    "df_silver_clients = spark.read.parquet(chemin_silver_exo)\n",
    "\n",
    "# Agregation par ville\n",
    "df_gold_pays= df_silver_clients.groupBy(\"pays\").agg(\n",
    "    count(\"*\").alias(\"nb_clients\")\n",
    ")\n",
    "\n",
    "df_gold_pays.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cabb9c48-9a38-4ecc-956a-2adf50c0694b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donnees ecrites dans : s3a://gold/clients/par_pays\n"
     ]
    }
   ],
   "source": [
    "# Ecrire dans Gold\n",
    "chemin_gold_exo = \"s3a://gold/clients/par_pays\"\n",
    "\n",
    "df_gold_ville.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(chemin_gold_exo)\n",
    "\n",
    "print(f\"Donnees ecrites dans : {chemin_gold_exo}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d020d72",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Resume\n",
    "\n",
    "Dans ce notebook, vous avez appris :\n",
    "- Comment **configurer Spark pour MinIO**\n",
    "- Le format des chemins S3 : `s3a://bucket/chemin`\n",
    "- Comment **lire et ecrire** dans MinIO avec Spark\n",
    "- L'organisation **Bronze / Silver / Gold**\n",
    "\n",
    "### Prochaine etape\n",
    "Dans le prochain notebook, nous apprendrons a ingerer des donnees depuis **PostgreSQL**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
