{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3adef709",
   "metadata": {},
   "source": [
    "# Exercice 07 - Ingestion PostgreSQL vers Bronze\n",
    "\n",
    "## Objectifs\n",
    "- Extraire des donnees de PostgreSQL\n",
    "- Sauvegarder dans la couche Bronze de MinIO\n",
    "- Organiser les donnees par date d'ingestion\n",
    "- Creer un pipeline d'ingestion reutilisable\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b38a26c",
   "metadata": {},
   "source": [
    "## 1. Architecture d'ingestion\n",
    "\n",
    "```\n",
    "+----------------+                    +------------------------+\n",
    "|                |                    |        MinIO           |\n",
    "|   PostgreSQL   |     SPARK          |                        |\n",
    "|                |  =============>    |  +------------------+  |\n",
    "|  +----------+  |                    |  |     BRONZE       |  |\n",
    "|  | customers|  |                    |  +------------------+  |\n",
    "|  | products |  |                    |  | /customers/      |  |\n",
    "|  | orders   |  |                    |  |   /2024-01-15/   |  |\n",
    "|  | ...      |  |                    |  | /products/       |  |\n",
    "|  +----------+  |                    |  |   /2024-01-15/   |  |\n",
    "|                |                    |  | /orders/         |  |\n",
    "+----------------+                    |  |   /2024-01-15/   |  |\n",
    "                                      |  +------------------+  |\n",
    "                                      +------------------------+\n",
    "\n",
    "Format Bronze : donnees brutes en Parquet\n",
    "Organisation  : /table/date_ingestion/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ec7574",
   "metadata": {},
   "source": [
    "## 2. Configuration Spark pour PostgreSQL et MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6300866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark configure pour PostgreSQL et MinIO\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "\n",
    "# Creer la SparkSession avec les configurations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Ingestion PostgreSQL vers Bronze\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.6.0,org.apache.hadoop:hadoop-aws:3.4.1,com.amazonaws:aws-java-sdk-bundle:1.12.262\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark configure pour PostgreSQL et MinIO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebc0e773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date d'ingestion : 2026-01-14\n"
     ]
    }
   ],
   "source": [
    "# Configuration PostgreSQL\n",
    "jdbc_url = \"jdbc:postgresql://postgres:5432/app\"\n",
    "jdbc_properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"postgres\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# Date d'ingestion pour organiser les fichiers\n",
    "date_ingestion = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "print(f\"Date d'ingestion : {date_ingestion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4361b5d",
   "metadata": {},
   "source": [
    "## 3. Ingerer une table simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdec84e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customers : 91 lignes\n",
      "+-----------+--------------------+------------------+--------------------+--------------------+-----------+------+-----------+-------+--------------+--------------+\n",
      "|customer_id|        company_name|      contact_name|       contact_title|             address|       city|region|postal_code|country|         phone|           fax|\n",
      "+-----------+--------------------+------------------+--------------------+--------------------+-----------+------+-----------+-------+--------------+--------------+\n",
      "|      ALFKI| Alfreds Futterkiste|      Maria Anders|Sales Representative|       Obere Str. 57|     Berlin|  NULL|      12209|Germany|   030-0074321|   030-0076545|\n",
      "|      ANATR|Ana Trujillo Empa...|      Ana Trujillo|               Owner|Avda. de la Const...|México D.F.|  NULL|      05021| Mexico|  (5) 555-4729|  (5) 555-3745|\n",
      "|      ANTON|Antonio Moreno Ta...|    Antonio Moreno|               Owner|     Mataderos  2312|México D.F.|  NULL|      05023| Mexico|  (5) 555-3932|          NULL|\n",
      "|      AROUT|     Around the Horn|      Thomas Hardy|Sales Representative|     120 Hanover Sq.|     London|  NULL|    WA1 1DP|     UK|(171) 555-7788|(171) 555-6750|\n",
      "|      BERGS|  Berglunds snabbköp|Christina Berglund| Order Administrator|     Berguvsvägen  8|      Luleå|  NULL|   S-958 22| Sweden| 0921-12 34 65| 0921-12 34 67|\n",
      "+-----------+--------------------+------------------+--------------------+--------------------+-----------+------+-----------+-------+--------------+--------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Lire la table customers depuis PostgreSQL\n",
    "df_customers = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"customers\",\n",
    "    properties=jdbc_properties\n",
    ")\n",
    "\n",
    "print(f\"Customers : {df_customers.count()} lignes\")\n",
    "df_customers.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d57bbc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sauvegarde reussie : s3a://bronze/customers/2026-01-14\n"
     ]
    }
   ],
   "source": [
    "# Sauvegarder dans Bronze\n",
    "chemin_bronze = f\"s3a://bronze/customers/{date_ingestion}\"\n",
    "\n",
    "df_customers.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(chemin_bronze)\n",
    "\n",
    "print(f\"Sauvegarde reussie : {chemin_bronze}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad43da06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification : 91 lignes lues depuis Bronze\n"
     ]
    }
   ],
   "source": [
    "# Verifier la sauvegarde\n",
    "df_check = spark.read.parquet(chemin_bronze)\n",
    "print(f\"Verification : {df_check.count()} lignes lues depuis Bronze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a6e660",
   "metadata": {},
   "source": [
    "## 4. Fonction d'ingestion reutilisable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fceaa2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingerer_table(nom_table, date=None):\n",
    "    \"\"\"\n",
    "    Ingere une table PostgreSQL vers le bucket Bronze.\n",
    "    \n",
    "    Args:\n",
    "        nom_table: Nom de la table a ingerer\n",
    "        date: Date d'ingestion (defaut: aujourd'hui)\n",
    "    \n",
    "    Returns:\n",
    "        Nombre de lignes ingerees\n",
    "    \"\"\"\n",
    "    if date is None:\n",
    "        date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    # Lire depuis PostgreSQL\n",
    "    df = spark.read.jdbc(\n",
    "        url=\"jdbc:postgresql://postgres:5432/app\",\n",
    "        table=nom_table,\n",
    "        properties={\n",
    "            \"user\": \"postgres\",\n",
    "            \"password\": \"postgres\",\n",
    "            \"driver\": \"org.postgresql.Driver\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Sauvegarder dans Bronze\n",
    "    chemin = f\"s3a://bronze/{nom_table}/{date}\"\n",
    "    df.write.mode(\"overwrite\").parquet(chemin)\n",
    "    \n",
    "    nb_lignes = df.count()\n",
    "    print(f\"[OK] {nom_table} : {nb_lignes} lignes -> {chemin}\")\n",
    "    \n",
    "    return nb_lignes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bde64a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] products : 77 lignes -> s3a://bronze/products/2026-01-14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tester la fonction\n",
    "ingerer_table(\"products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3f5537",
   "metadata": {},
   "source": [
    "## 5. Ingerer toutes les tables Northwind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60f29ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables a ingerer : 10\n"
     ]
    }
   ],
   "source": [
    "# Liste des tables a ingerer\n",
    "tables_northwind = [\n",
    "    \"categories\",\n",
    "    \"customers\",\n",
    "    \"employees\",\n",
    "    \"orders\",\n",
    "    \"order_details\",\n",
    "    \"products\",\n",
    "    \"shippers\",\n",
    "    \"suppliers\",\n",
    "    \"region\",\n",
    "    \"territories\"\n",
    "]\n",
    "\n",
    "print(f\"Tables a ingerer : {len(tables_northwind)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbde5c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "INGESTION NORTHWIND VERS BRONZE\n",
      "==================================================\n",
      "[OK] categories : 8 lignes -> s3a://bronze/categories/2026-01-14\n",
      "[OK] customers : 91 lignes -> s3a://bronze/customers/2026-01-14\n",
      "[OK] employees : 9 lignes -> s3a://bronze/employees/2026-01-14\n",
      "[OK] orders : 830 lignes -> s3a://bronze/orders/2026-01-14\n",
      "[OK] order_details : 2155 lignes -> s3a://bronze/order_details/2026-01-14\n",
      "[OK] products : 77 lignes -> s3a://bronze/products/2026-01-14\n",
      "[OK] shippers : 6 lignes -> s3a://bronze/shippers/2026-01-14\n",
      "[OK] suppliers : 29 lignes -> s3a://bronze/suppliers/2026-01-14\n",
      "[OK] region : 4 lignes -> s3a://bronze/region/2026-01-14\n",
      "[OK] territories : 53 lignes -> s3a://bronze/territories/2026-01-14\n",
      "==================================================\n",
      "INGESTION TERMINEE\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Ingerer toutes les tables\n",
    "resultats = {}\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"INGESTION NORTHWIND VERS BRONZE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for table in tables_northwind:\n",
    "    try:\n",
    "        nb_lignes = ingerer_table(table, date_ingestion)\n",
    "        resultats[table] = {\"status\": \"OK\", \"lignes\": nb_lignes}\n",
    "    except Exception as e:\n",
    "        print(f\"[ERREUR] {table} : {e}\")\n",
    "        resultats[table] = {\"status\": \"ERREUR\", \"erreur\": str(e)}\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"INGESTION TERMINEE\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f4f09ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resume :\n",
      "  categories: 8 lignes\n",
      "  customers: 91 lignes\n",
      "  employees: 9 lignes\n",
      "  orders: 830 lignes\n",
      "  order_details: 2155 lignes\n",
      "  products: 77 lignes\n",
      "  shippers: 6 lignes\n",
      "  suppliers: 29 lignes\n",
      "  region: 4 lignes\n",
      "  territories: 53 lignes\n",
      "\n",
      "Total : 3262 lignes ingerees\n"
     ]
    }
   ],
   "source": [
    "# Resume de l'ingestion\n",
    "print(\"\\nResume :\")\n",
    "total_lignes = 0\n",
    "for table, info in resultats.items():\n",
    "    if info[\"status\"] == \"OK\":\n",
    "        print(f\"  {table}: {info['lignes']} lignes\")\n",
    "        total_lignes += info[\"lignes\"]\n",
    "    else:\n",
    "        print(f\"  {table}: ERREUR\")\n",
    "\n",
    "print(f\"\\nTotal : {total_lignes} lignes ingerees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f7bc62",
   "metadata": {},
   "source": [
    "## 6. Ajouter des metadonnees d'ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b228091",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def ingerer_table_avec_metadata(nom_table, date=None):\n",
    "    \"\"\"\n",
    "    Ingere une table avec des colonnes de metadata.\n",
    "    \"\"\"\n",
    "    if date is None:\n",
    "        date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    timestamp_ingestion = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Lire depuis PostgreSQL\n",
    "    df = spark.read.jdbc(\n",
    "        url=\"jdbc:postgresql://postgres:5432/app\",\n",
    "        table=nom_table,\n",
    "        properties={\n",
    "            \"user\": \"postgres\",\n",
    "            \"password\": \"postgres\",\n",
    "            \"driver\": \"org.postgresql.Driver\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Ajouter les metadonnees\n",
    "    df = df.withColumn(\"_source\", F.lit(\"postgresql\")) \\\n",
    "           .withColumn(\"_table\", F.lit(nom_table)) \\\n",
    "           .withColumn(\"_ingestion_date\", F.lit(date)) \\\n",
    "           .withColumn(\"_ingestion_timestamp\", F.lit(timestamp_ingestion))\n",
    "    \n",
    "    # Sauvegarder dans Bronze\n",
    "    chemin = f\"s3a://bronze/{nom_table}/{date}\"\n",
    "    df.write.mode(\"overwrite\").parquet(chemin)\n",
    "    \n",
    "    nb_lignes = df.count()\n",
    "    print(f\"[OK] {nom_table} : {nb_lignes} lignes (avec metadata) -> {chemin}\")\n",
    "    \n",
    "    return nb_lignes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08d41d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] categories : 8 lignes (avec metadata) -> s3a://bronze/categories/2026-01-14\n",
      "+-----------+--------------+----------+----------+---------------+\n",
      "|category_id| category_name|   _source|    _table|_ingestion_date|\n",
      "+-----------+--------------+----------+----------+---------------+\n",
      "|          1|     Beverages|postgresql|categories|     2026-01-14|\n",
      "|          2|    Condiments|postgresql|categories|     2026-01-14|\n",
      "|          3|   Confections|postgresql|categories|     2026-01-14|\n",
      "|          4|Dairy Products|postgresql|categories|     2026-01-14|\n",
      "|          5|Grains/Cereals|postgresql|categories|     2026-01-14|\n",
      "|          6|  Meat/Poultry|postgresql|categories|     2026-01-14|\n",
      "|          7|       Produce|postgresql|categories|     2026-01-14|\n",
      "|          8|       Seafood|postgresql|categories|     2026-01-14|\n",
      "+-----------+--------------+----------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tester avec metadata\n",
    "ingerer_table_avec_metadata(\"categories\")\n",
    "\n",
    "# Verifier les metadonnees\n",
    "df_test = spark.read.parquet(f\"s3a://bronze/categories/{date_ingestion}\")\n",
    "df_test.select(\"category_id\", \"category_name\", \"_source\", \"_table\", \"_ingestion_date\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1424d22",
   "metadata": {},
   "source": [
    "## 7. Verifier le contenu de Bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f48bb645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contenu du bucket Bronze :\n",
      "==================================================\n",
      "categories: 8 lignes\n",
      "customers: 91 lignes\n",
      "employees: 9 lignes\n",
      "orders: 830 lignes\n",
      "order_details: 2155 lignes\n",
      "products: 77 lignes\n",
      "shippers: 6 lignes\n",
      "suppliers: 29 lignes\n",
      "region: 4 lignes\n",
      "territories: 53 lignes\n"
     ]
    }
   ],
   "source": [
    "# Lister les fichiers dans Bronze\n",
    "print(\"Contenu du bucket Bronze :\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for table in tables_northwind:\n",
    "    try:\n",
    "        chemin = f\"s3a://bronze/{table}/{date_ingestion}\"\n",
    "        df = spark.read.parquet(chemin)\n",
    "        print(f\"{table}: {df.count()} lignes\")\n",
    "    except:\n",
    "        print(f\"{table}: non trouve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b446be",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercice\n",
    "\n",
    "**Objectif** : Creer un script d'ingestion complet\n",
    "\n",
    "**Consigne** :\n",
    "1. Creez une fonction `ingestion_complete()` qui :\n",
    "   - Ingere toutes les tables avec metadonnees\n",
    "   - Affiche un rapport final\n",
    "   - Retourne un dictionnaire avec les statistiques\n",
    "\n",
    "2. Ajoutez une colonne `_nb_colonnes` aux metadonnees\n",
    "\n",
    "A vous de jouer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "319256dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DÉBUT INGESTION (2026-01-14 16:41:40) ===\n",
      "\n",
      "[SUCCÈS] categories      :     8 lignes |  4 cols -> s3a://bronze/categories/2026-01-14\n",
      "[SUCCÈS] customers       :    91 lignes | 11 cols -> s3a://bronze/customers/2026-01-14\n",
      "[SUCCÈS] employees       :     9 lignes | 18 cols -> s3a://bronze/employees/2026-01-14\n",
      "[SUCCÈS] orders          :   830 lignes | 14 cols -> s3a://bronze/orders/2026-01-14\n",
      "[SUCCÈS] products        :    77 lignes | 10 cols -> s3a://bronze/products/2026-01-14\n",
      "\n",
      "=== RAPPORT FINAL ===\n",
      "Tables traitées avec succès : 5 / 5\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime\n",
    "\n",
    "def ingestion_complete(tables_liste, bucket_bronze=\"s3a://bronze\"):\n",
    "    \"\"\"\n",
    "    Ingère une liste de tables avec métadonnées complètes et rapport final.\n",
    "    \"\"\"\n",
    "    date_jour = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    timestamp_now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    stats = {}\n",
    "    \n",
    "    print(f\"=== DÉBUT INGESTION ({timestamp_now}) ===\\n\")\n",
    "\n",
    "    for table in tables_liste:\n",
    "        try:\n",
    "            # 1. Lecture JDBC\n",
    "            df = spark.read.jdbc(\n",
    "                url=\"jdbc:postgresql://postgres:5432/app\",\n",
    "                table=table,\n",
    "                properties={\"user\": \"postgres\", \"password\": \"postgres\", \"driver\": \"org.postgresql.Driver\"}\n",
    "            )\n",
    "            \n",
    "            # Calculs préliminaires pour les métadonnées\n",
    "            nb_lignes = df.count()\n",
    "            nb_cols = len(df.columns)\n",
    "            \n",
    "            # 2. Ajout des métadonnées (dont _nb_colonnes demandé dans l'exercice)\n",
    "            df_final = df.withColumn(\"_source\", F.lit(\"postgresql\")) \\\n",
    "                         .withColumn(\"_table\", F.lit(table)) \\\n",
    "                         .withColumn(\"_ingestion_timestamp\", F.lit(timestamp_now)) \\\n",
    "                         .withColumn(\"_nb_colonnes\", F.lit(nb_cols)) # La consigne est ici\n",
    "            \n",
    "            # 3. Écriture dans Bronze (partitionné par date)\n",
    "            chemin = f\"{bucket_bronze}/{table}/{date_jour}\"\n",
    "            df_final.write.mode(\"overwrite\").parquet(chemin)\n",
    "            \n",
    "            # Enregistrement du succès\n",
    "            stats[table] = {\"status\": \"OK\", \"lignes\": nb_lignes, \"colonnes\": nb_cols}\n",
    "            print(f\"[SUCCÈS] {table:15} : {nb_lignes:5} lignes | {nb_cols:2} cols -> {chemin}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            stats[table] = {\"status\": \"ERREUR\", \"msg\": str(e)}\n",
    "            print(f\"[ERREUR] {table:15} : {str(e)}\")\n",
    "\n",
    "    # 4. Rapport Final\n",
    "    print(\"\\n=== RAPPORT FINAL ===\")\n",
    "    succes = sum(1 for t in stats.values() if t[\"status\"] == \"OK\")\n",
    "    print(f\"Tables traitées avec succès : {succes} / {len(tables_liste)}\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Test de la fonction avec toutes les tables\n",
    "tables_northwind = [\"categories\", \"customers\", \"employees\", \"orders\", \"products\"]\n",
    "resultats = ingestion_complete(tables_northwind)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440420c8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Resume\n",
    "\n",
    "Dans ce notebook, vous avez appris :\n",
    "- Comment **extraire des donnees** de PostgreSQL avec Spark\n",
    "- Comment **sauvegarder** les donnees dans MinIO (Bronze)\n",
    "- Comment **organiser** les donnees par date d'ingestion\n",
    "- Comment **ajouter des metadonnees** pour la tracabilite\n",
    "- Comment creer un **pipeline d'ingestion** reutilisable\n",
    "\n",
    "### Prochaine etape\n",
    "Dans le prochain notebook, nous apprendrons a ingerer des donnees depuis le Web (API REST)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
