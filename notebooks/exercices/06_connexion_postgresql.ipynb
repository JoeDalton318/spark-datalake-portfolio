{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e70fa69b",
   "metadata": {},
   "source": [
    "# Exercice 06 - Connexion a PostgreSQL\n",
    "\n",
    "## Objectifs\n",
    "- Se connecter a PostgreSQL depuis Spark\n",
    "- Lire des tables SQL avec Spark\n",
    "- Decouvrir la base Northwind\n",
    "- Executer des requetes SQL sur les donnees\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e7abad",
   "metadata": {},
   "source": [
    "## 1. La base de donnees Northwind\n",
    "\n",
    "**Northwind** est une base de donnees exemple classique qui simule une entreprise de vente.\n",
    "\n",
    "```\n",
    "+------------------------------------------------------------------+\n",
    "|                    BASE NORTHWIND                                |\n",
    "+------------------------------------------------------------------+\n",
    "|                                                                  |\n",
    "|  +------------+     +------------+     +------------+            |\n",
    "|  | categories |     |  products  |     | suppliers  |            |\n",
    "|  +------------+     +------------+     +------------+            |\n",
    "|        |                  |                  |                   |\n",
    "|        +--------+---------+------------------+                   |\n",
    "|                 |                                                |\n",
    "|                 v                                                |\n",
    "|  +------------+     +------------+     +------------+            |\n",
    "|  |   orders   |---->|order_details|<---|  products  |            |\n",
    "|  +------------+     +------------+     +------------+            |\n",
    "|        |                                                         |\n",
    "|        v                                                         |\n",
    "|  +------------+     +------------+                               |\n",
    "|  | customers  |     | employees  |                               |\n",
    "|  +------------+     +------------+                               |\n",
    "|                                                                  |\n",
    "+------------------------------------------------------------------+\n",
    "\n",
    "Tables principales :\n",
    "- customers    : clients de l'entreprise\n",
    "- products     : catalogue de produits\n",
    "- orders       : commandes passees\n",
    "- order_details: details de chaque commande\n",
    "- employees    : employes de l'entreprise\n",
    "- categories   : categories de produits\n",
    "- suppliers    : fournisseurs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb3e06e",
   "metadata": {},
   "source": [
    "## 2. Configuration de Spark pour PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb0c7eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark pret !\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Creer la SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark PostgreSQL\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.6.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark pret !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34734901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration PostgreSQL :\n",
      "  URL: jdbc:postgresql://postgres:5432/app\n",
      "  User: postgres\n"
     ]
    }
   ],
   "source": [
    "# Configuration de la connexion PostgreSQL\n",
    "jdbc_url = \"jdbc:postgresql://postgres:5432/app\"\n",
    "connection_properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"postgres\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "print(\"Configuration PostgreSQL :\")\n",
    "print(f\"  URL: {jdbc_url}\")\n",
    "print(f\"  User: {connection_properties['user']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82695b11",
   "metadata": {},
   "source": [
    "## 3. Lire une table SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cb85019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table customers chargee !\n",
      "Nombre de clients : 91\n"
     ]
    }
   ],
   "source": [
    "# Lire la table customers\n",
    "df_customers = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"customers\",\n",
    "    properties=connection_properties\n",
    ")\n",
    "\n",
    "print(\"Table customers chargee !\")\n",
    "print(f\"Nombre de clients : {df_customers.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09df9fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+------------------+--------------------+--------------------+-----------+------+-----------+-------+--------------+--------------+\n",
      "|customer_id|        company_name|      contact_name|       contact_title|             address|       city|region|postal_code|country|         phone|           fax|\n",
      "+-----------+--------------------+------------------+--------------------+--------------------+-----------+------+-----------+-------+--------------+--------------+\n",
      "|      ALFKI| Alfreds Futterkiste|      Maria Anders|Sales Representative|       Obere Str. 57|     Berlin|  NULL|      12209|Germany|   030-0074321|   030-0076545|\n",
      "|      ANATR|Ana Trujillo Empa...|      Ana Trujillo|               Owner|Avda. de la Const...|México D.F.|  NULL|      05021| Mexico|  (5) 555-4729|  (5) 555-3745|\n",
      "|      ANTON|Antonio Moreno Ta...|    Antonio Moreno|               Owner|     Mataderos  2312|México D.F.|  NULL|      05023| Mexico|  (5) 555-3932|          NULL|\n",
      "|      AROUT|     Around the Horn|      Thomas Hardy|Sales Representative|     120 Hanover Sq.|     London|  NULL|    WA1 1DP|     UK|(171) 555-7788|(171) 555-6750|\n",
      "|      BERGS|  Berglunds snabbköp|Christina Berglund| Order Administrator|     Berguvsvägen  8|      Luleå|  NULL|   S-958 22| Sweden| 0921-12 34 65| 0921-12 34 67|\n",
      "+-----------+--------------------+------------------+--------------------+--------------------+-----------+------+-----------+-------+--------------+--------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Afficher les premieres lignes\n",
    "df_customers.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49bb7838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- company_name: string (nullable = true)\n",
      " |-- contact_name: string (nullable = true)\n",
      " |-- contact_title: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- postal_code: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- phone: string (nullable = true)\n",
      " |-- fax: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Afficher le schema\n",
    "df_customers.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe7d627",
   "metadata": {},
   "source": [
    "## 4. Lire d'autres tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bae73e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de produits : 77\n",
      "+----------+--------------------+-----------+-----------+-------------------+----------+--------------+--------------+-------------+------------+\n",
      "|product_id|        product_name|supplier_id|category_id|  quantity_per_unit|unit_price|units_in_stock|units_on_order|reorder_level|discontinued|\n",
      "+----------+--------------------+-----------+-----------+-------------------+----------+--------------+--------------+-------------+------------+\n",
      "|         1|                Chai|          8|          1| 10 boxes x 30 bags|      18.0|            39|             0|           10|           1|\n",
      "|         2|               Chang|          1|          1| 24 - 12 oz bottles|      19.0|            17|            40|           25|           1|\n",
      "|         3|       Aniseed Syrup|          1|          2|12 - 550 ml bottles|      10.0|            13|            70|           25|           0|\n",
      "|         4|Chef Anton's Caju...|          2|          2|     48 - 6 oz jars|      22.0|            53|             0|            0|           0|\n",
      "|         5|Chef Anton's Gumb...|          2|          2|           36 boxes|     21.35|             0|             0|            0|           1|\n",
      "+----------+--------------------+-----------+-----------+-------------------+----------+--------------+--------------+-------------+------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Lire la table products\n",
    "df_products = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"products\",\n",
    "    properties=connection_properties\n",
    ")\n",
    "\n",
    "print(f\"Nombre de produits : {df_products.count()}\")\n",
    "df_products.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7663f1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de commandes : 830\n",
      "+--------+-----------+-----------+----------+-------------+------------+--------+-------+--------------------+--------------------+--------------+-----------+----------------+------------+\n",
      "|order_id|customer_id|employee_id|order_date|required_date|shipped_date|ship_via|freight|           ship_name|        ship_address|     ship_city|ship_region|ship_postal_code|ship_country|\n",
      "+--------+-----------+-----------+----------+-------------+------------+--------+-------+--------------------+--------------------+--------------+-----------+----------------+------------+\n",
      "|   10248|      VINET|          5|1996-07-04|   1996-08-01|  1996-07-16|       3|  32.38|Vins et alcools C...|  59 rue de l'Abbaye|         Reims|       NULL|           51100|      France|\n",
      "|   10249|      TOMSP|          6|1996-07-05|   1996-08-16|  1996-07-10|       1|  11.61|  Toms Spezialitäten|       Luisenstr. 48|       Münster|       NULL|           44087|     Germany|\n",
      "|   10250|      HANAR|          4|1996-07-08|   1996-08-05|  1996-07-12|       2|  65.83|       Hanari Carnes|     Rua do Paço, 67|Rio de Janeiro|         RJ|       05454-876|      Brazil|\n",
      "|   10251|      VICTE|          3|1996-07-08|   1996-08-05|  1996-07-15|       1|  41.34|Victuailles en stock|  2, rue du Commerce|          Lyon|       NULL|           69004|      France|\n",
      "|   10252|      SUPRD|          4|1996-07-09|   1996-08-06|  1996-07-11|       2|   51.3|    Suprêmes délices|Boulevard Tirou, 255|     Charleroi|       NULL|          B-6000|     Belgium|\n",
      "+--------+-----------+-----------+----------+-------------+------------+--------+-------+--------------------+--------------------+--------------+-----------+----------------+------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Lire la table orders\n",
    "df_orders = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"orders\",\n",
    "    properties=connection_properties\n",
    ")\n",
    "\n",
    "print(f\"Nombre de commandes : {df_orders.count()}\")\n",
    "df_orders.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef67c436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes de commande : 2155\n",
      "+--------+----------+----------+--------+--------+\n",
      "|order_id|product_id|unit_price|quantity|discount|\n",
      "+--------+----------+----------+--------+--------+\n",
      "|   10248|        11|      14.0|      12|     0.0|\n",
      "|   10248|        42|       9.8|      10|     0.0|\n",
      "|   10248|        72|      34.8|       5|     0.0|\n",
      "|   10249|        14|      18.6|       9|     0.0|\n",
      "|   10249|        51|      42.4|      40|     0.0|\n",
      "+--------+----------+----------+--------+--------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Lire la table order_details\n",
    "df_order_details = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"order_details\",\n",
    "    properties=connection_properties\n",
    ")\n",
    "\n",
    "print(f\"Nombre de lignes de commande : {df_order_details.count()}\")\n",
    "df_order_details.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e75fdba",
   "metadata": {},
   "source": [
    "## 5. Utiliser SQL avec Spark\n",
    "\n",
    "Spark permet d'utiliser du SQL sur les DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bf6da2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vues SQL creees !\n"
     ]
    }
   ],
   "source": [
    "# Enregistrer les DataFrames comme vues temporaires\n",
    "df_customers.createOrReplaceTempView(\"customers\")\n",
    "df_products.createOrReplaceTempView(\"products\")\n",
    "df_orders.createOrReplaceTempView(\"orders\")\n",
    "df_order_details.createOrReplaceTempView(\"order_details\")\n",
    "\n",
    "print(\"Vues SQL creees !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d0c9dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|    country|nb_clients|\n",
      "+-----------+----------+\n",
      "|        USA|        13|\n",
      "|    Germany|        11|\n",
      "|     France|        11|\n",
      "|     Brazil|         9|\n",
      "|         UK|         7|\n",
      "|      Spain|         5|\n",
      "|     Mexico|         5|\n",
      "|  Venezuela|         4|\n",
      "|  Argentina|         3|\n",
      "|      Italy|         3|\n",
      "|     Canada|         3|\n",
      "|     Sweden|         2|\n",
      "|    Belgium|         2|\n",
      "|    Finland|         2|\n",
      "|    Denmark|         2|\n",
      "|Switzerland|         2|\n",
      "|   Portugal|         2|\n",
      "|    Austria|         2|\n",
      "|     Norway|         1|\n",
      "|    Ireland|         1|\n",
      "+-----------+----------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Requete SQL : clients par pays\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT country, COUNT(*) as nb_clients\n",
    "    FROM customers\n",
    "    GROUP BY country\n",
    "    ORDER BY nb_clients DESC\n",
    "\"\"\")\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "761fe0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|        product_name|unit_price|\n",
      "+--------------------+----------+\n",
      "|       Côte de Blaye|     263.5|\n",
      "|Thüringer Rostbra...|    123.79|\n",
      "|     Mishi Kobe Niku|      97.0|\n",
      "|Sir Rodney's Marm...|      81.0|\n",
      "|    Carnarvon Tigers|      62.5|\n",
      "|Raclette Courdavault|      55.0|\n",
      "|Manjimup Dried Ap...|      53.0|\n",
      "|      Tarte au sucre|      49.3|\n",
      "|         Ipoh Coffee|      46.0|\n",
      "|   Rössle Sauerkraut|      45.6|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Requete SQL : produits les plus chers\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT product_name, unit_price\n",
    "    FROM products\n",
    "    ORDER BY unit_price DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3dd58cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 clients par chiffre d'affaires :\n",
      "+----------------------------+------------+-------------+\n",
      "|company_name                |nb_commandes|montant_total|\n",
      "+----------------------------+------------+-------------+\n",
      "|QUICK-Stop                  |28          |117483.39    |\n",
      "|Save-a-lot Markets          |31          |115673.39    |\n",
      "|Ernst Handel                |30          |113236.68    |\n",
      "|Hungry Owl All-Night Grocers|19          |57317.39     |\n",
      "|Rattlesnake Canyon Grocery  |18          |52245.9      |\n",
      "|Hanari Carnes               |14          |34101.15     |\n",
      "|Folk och fä HB              |19          |32555.55     |\n",
      "|Mère Paillarde              |13          |32203.9      |\n",
      "|Königlich Essen             |14          |31745.75     |\n",
      "|Queen Cozinha               |13          |30226.1      |\n",
      "+----------------------------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Requete SQL : montant total des commandes par client\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        c.company_name,\n",
    "        COUNT(DISTINCT o.order_id) as nb_commandes,\n",
    "        ROUND(SUM(od.unit_price * od.quantity), 2) as montant_total\n",
    "    FROM customers c\n",
    "    JOIN orders o ON c.customer_id = o.customer_id\n",
    "    JOIN order_details od ON o.order_id = od.order_id\n",
    "    GROUP BY c.company_name\n",
    "    ORDER BY montant_total DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "print(\"Top 10 clients par chiffre d'affaires :\")\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8acacf5",
   "metadata": {},
   "source": [
    "## 6. Lire avec une requete SQL personnalisee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62279a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clients francais :\n",
      "+-----------+--------------------+-------+\n",
      "|customer_id|        company_name|country|\n",
      "+-----------+--------------------+-------+\n",
      "|      BLONP|Blondesddsl père ...| France|\n",
      "|      BONAP|            Bon app'| France|\n",
      "|      DUMON|     Du monde entier| France|\n",
      "|      FOLIG|   Folies gourmandes| France|\n",
      "|      FRANR| France restauration| France|\n",
      "|      LACOR|La corne d'abondance| France|\n",
      "|      LAMAI|    La maison d'Asie| France|\n",
      "|      PARIS|   Paris spécialités| France|\n",
      "|      SPECD|Spécialités du monde| France|\n",
      "|      VICTE|Victuailles en stock| France|\n",
      "|      VINET|Vins et alcools C...| France|\n",
      "+-----------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Au lieu de lire une table entiere, on peut executer une requete\n",
    "query = \"(SELECT customer_id, company_name, country FROM customers WHERE country = 'France') as french_customers\"\n",
    "\n",
    "df_french = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=query,\n",
    "    properties=connection_properties\n",
    ")\n",
    "\n",
    "print(\"Clients francais :\")\n",
    "df_french.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4efd02",
   "metadata": {},
   "source": [
    "## 7. Fonction utilitaire pour lire les tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3baef987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+--------------------+-------+\n",
      "|category_id| category_name|         description|picture|\n",
      "+-----------+--------------+--------------------+-------+\n",
      "|          1|     Beverages|Soft drinks, coff...|     []|\n",
      "|          2|    Condiments|Sweet and savory ...|     []|\n",
      "|          3|   Confections|Desserts, candies...|     []|\n",
      "|          4|Dairy Products|             Cheeses|     []|\n",
      "|          5|Grains/Cereals|Breads, crackers,...|     []|\n",
      "|          6|  Meat/Poultry|      Prepared meats|     []|\n",
      "|          7|       Produce|Dried fruit and b...|     []|\n",
      "|          8|       Seafood|    Seaweed and fish|     []|\n",
      "+-----------+--------------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def lire_table(nom_table):\n",
    "    \"\"\"Fonction pour lire une table PostgreSQL\"\"\"\n",
    "    return spark.read.jdbc(\n",
    "        url=\"jdbc:postgresql://postgres:5432/app\",\n",
    "        table=nom_table,\n",
    "        properties={\n",
    "            \"user\": \"postgres\",\n",
    "            \"password\": \"postgres\",\n",
    "            \"driver\": \"org.postgresql.Driver\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Exemple d'utilisation\n",
    "df_categories = lire_table(\"categories\")\n",
    "df_categories.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580e1532",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercice\n",
    "\n",
    "**Objectif** : Explorer la base Northwind avec Spark SQL\n",
    "\n",
    "**Consigne** :\n",
    "1. Lisez la table `employees`\n",
    "2. Affichez le schema et les premieres lignes\n",
    "3. Creez une vue temporaire\n",
    "4. Ecrivez une requete SQL pour trouver :\n",
    "   - Le nombre d'employes par ville\n",
    "   - Les employes embauches apres 1993\n",
    "\n",
    "A vous de jouer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c38d16f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Schema ---\n",
      "root\n",
      " |-- employee_id: short (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- title_of_courtesy: string (nullable = true)\n",
      " |-- birth_date: date (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- postal_code: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- home_phone: string (nullable = true)\n",
      " |-- extension: string (nullable = true)\n",
      " |-- photo: binary (nullable = true)\n",
      " |-- notes: string (nullable = true)\n",
      " |-- reports_to: short (nullable = true)\n",
      " |-- photo_path: string (nullable = true)\n",
      "\n",
      "--- Apercu des donnees ---\n",
      "+-----------+---------+----------+--------------------+-----------------+----------+----------+--------------------+--------+------+-----------+-------+--------------+---------+-----+--------------------+----------+--------------------+\n",
      "|employee_id|last_name|first_name|               title|title_of_courtesy|birth_date| hire_date|             address|    city|region|postal_code|country|    home_phone|extension|photo|               notes|reports_to|          photo_path|\n",
      "+-----------+---------+----------+--------------------+-----------------+----------+----------+--------------------+--------+------+-----------+-------+--------------+---------+-----+--------------------+----------+--------------------+\n",
      "|          1|  Davolio|     Nancy|Sales Representative|              Ms.|1948-12-08|1992-05-01|507 - 20th Ave. E...| Seattle|    WA|      98122|    USA|(206) 555-9857|     5467|   []|Education include...|         2|http://accweb/emm...|\n",
      "|          2|   Fuller|    Andrew|Vice President, S...|              Dr.|1952-02-19|1992-08-14|  908 W. Capital Way|  Tacoma|    WA|      98401|    USA|(206) 555-9482|     3457|   []|Andrew received h...|      NULL|http://accweb/emm...|\n",
      "|          3|Leverling|     Janet|Sales Representative|              Ms.|1963-08-30|1992-04-01|  722 Moss Bay Blvd.|Kirkland|    WA|      98033|    USA|(206) 555-3412|     3355|   []|Janet has a BS de...|         2|http://accweb/emm...|\n",
      "|          4|  Peacock|  Margaret|Sales Representative|             Mrs.|1937-09-19|1993-05-03|4110 Old Redmond Rd.| Redmond|    WA|      98052|    USA|(206) 555-8122|     5176|   []|Margaret holds a ...|         2|http://accweb/emm...|\n",
      "|          5| Buchanan|    Steven|       Sales Manager|              Mr.|1955-03-04|1993-10-17|     14 Garrett Hill|  London|  NULL|    SW1 8JR|     UK| (71) 555-4848|     3453|   []|Steven Buchanan g...|         2|http://accweb/emm...|\n",
      "+-----------+---------+----------+--------------------+-----------------+----------+----------+--------------------+--------+------+-----------+-------+--------------+---------+-----+--------------------+----------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# TODO: Lire la table employees\n",
    "df_employees = lire_table(\"employees\")\n",
    "\n",
    "# TODO: Afficher le schema et les premieres lignes\n",
    "print(\"--- Schema ---\")\n",
    "df_employees.printSchema()\n",
    "\n",
    "print(\"--- Apercu des donnees ---\")\n",
    "df_employees.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d686347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|    city|nb_employees|\n",
      "+--------+------------+\n",
      "|  London|           4|\n",
      "| Seattle|           2|\n",
      "|Kirkland|           1|\n",
      "| Redmond|           1|\n",
      "|  Tacoma|           1|\n",
      "+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Creer une vue temporaire\n",
    "df_employees.createOrReplaceTempView(\"employees\")\n",
    "# TODO: Nombre d'employes par ville\n",
    "spark.sql(\"\"\"\n",
    "    SELECT city, COUNT(*) as nb_employees\n",
    "    FROM employees\n",
    "    GROUP BY city\n",
    "    ORDER BY nb_employees DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65489d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+\n",
      "|first_name|last_name| hire_date|\n",
      "+----------+---------+----------+\n",
      "|    Robert|     King|1994-01-02|\n",
      "|     Laura| Callahan|1994-03-05|\n",
      "|      Anne|Dodsworth|1994-11-15|\n",
      "+----------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Employes embauches apres 1993\n",
    "# Indice : utilisez WHERE hire_date > '1993-12-31'\n",
    "spark.sql(\"\"\"\n",
    "    SELECT first_name, last_name, hire_date\n",
    "    FROM employees\n",
    "    WHERE hire_date > '1993-12-31'\n",
    "    ORDER BY hire_date ASC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbcc332",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Resume\n",
    "\n",
    "Dans ce notebook, vous avez appris :\n",
    "- Comment **configurer Spark pour PostgreSQL**\n",
    "- Comment **lire des tables** avec `spark.read.jdbc()`\n",
    "- La structure de la base **Northwind**\n",
    "- Comment utiliser **Spark SQL** pour analyser les donnees\n",
    "- Comment lire avec une **requete personnalisee**\n",
    "\n",
    "### Prochaine etape\n",
    "Dans le prochain notebook, nous apprendrons a ingerer les donnees PostgreSQL vers notre Data Lake MinIO."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
