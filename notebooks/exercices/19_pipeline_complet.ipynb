{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7177147a",
   "metadata": {},
   "source": [
    "# Exercice 19 - Pipeline Complet Bronze/Silver/Gold\n",
    "\n",
    "## Objectifs\n",
    "- Construire un pipeline ETL complet\n",
    "- Implementer l'architecture Medallion (Bronze/Silver/Gold)\n",
    "- Orchestrer les differentes etapes\n",
    "- Produire des donnees prets pour l'analyse\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cda7d4",
   "metadata": {},
   "source": [
    "## 1. Architecture du Pipeline\n",
    "\n",
    "```\n",
    "+=========================================================================+\n",
    "|                    PIPELINE MEDALLION COMPLET                           |\n",
    "+=========================================================================+\n",
    "|                                                                         |\n",
    "|  SOURCES                                                                |\n",
    "|  +-------------+     +-------------+     +-------------+                |\n",
    "|  | PostgreSQL  |     |    Kafka    |     |   Fichiers  |                |\n",
    "|  | (Northwind) |     | (Streaming) |     |   (CSV)     |                |\n",
    "|  +------+------+     +------+------+     +------+------+                |\n",
    "|         |                   |                   |                       |\n",
    "|         +-------------------+-------------------+                       |\n",
    "|                             |                                           |\n",
    "|                             v                                           |\n",
    "|  BRONZE (Donnees brutes)                                                |\n",
    "|  +------------------------------------------------------------------+   |\n",
    "|  |  s3a://bronze/                                                   |   |\n",
    "|  |  - customers/    - orders/    - products/    - kafka/            |   |\n",
    "|  |  Format: Parquet, partition par date d'ingestion                 |   |\n",
    "|  +------------------------------------------------------------------+   |\n",
    "|                             |                                           |\n",
    "|                             v                                           |\n",
    "|  SILVER (Donnees nettoyees)                                             |\n",
    "|  +------------------------------------------------------------------+   |\n",
    "|  |  s3a://silver/                                                   |   |\n",
    "|  |  - dim_customers/  - dim_products/  - fact_orders/               |   |\n",
    "|  |  Nettoyage, deduplication, enrichissement                        |   |\n",
    "|  +------------------------------------------------------------------+   |\n",
    "|                             |                                           |\n",
    "|                             v                                           |\n",
    "|  GOLD (Donnees agregees)                                                |\n",
    "|  +------------------------------------------------------------------+   |\n",
    "|  |  s3a://gold/                                                     |   |\n",
    "|  |  - kpi_ventes/  - analyse_clients/  - rapport_produits/          |   |\n",
    "|  |  KPIs, aggregations, donnees prets pour dashboards               |   |\n",
    "|  +------------------------------------------------------------------+   |\n",
    "|                                                                         |\n",
    "+=========================================================================+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6a712d",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06a53da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session Spark d√©marr√©e (Patch '60s' appliqu√©).\n",
      "Date d'ingestion: 2026-01-18\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, current_timestamp, date_format, year, month, dayofmonth,\n",
    "    when, coalesce, trim, upper, lower, initcap,\n",
    "    count, sum as spark_sum, avg, max as spark_max, min as spark_min,\n",
    "    round as spark_round, datediff, current_date,\n",
    "    row_number, dense_rank, percent_rank\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# --- ETAPE CRITIQUE : Arr√™ter l'ancienne session pour appliquer le patch ---\n",
    "try:\n",
    "    SparkSession.getActiveSession().stop()\n",
    "    print(\"üõë Ancienne session Spark arr√™t√©e avec succ√®s.\")\n",
    "except:\n",
    "    pass\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "# Packages n√©cessaires : PostgreSQL (Source) + S3 (Stockage)\n",
    "PACKAGES = \"org.postgresql:postgresql:42.6.0,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PipelineComplet\") \\\n",
    "    .config(\"spark.jars.packages\", PACKAGES) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.timeout\", \"60000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.establish.timeout\", \"10000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.socket.timeout\", \"60000\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# Configuration BDD et Dossiers\n",
    "JDBC_URL = \"jdbc:postgresql://postgres:5432/app\"\n",
    "JDBC_PROPS = {\"user\": \"postgres\", \"password\": \"postgres\", \"driver\": \"org.postgresql.Driver\"}\n",
    "\n",
    "BRONZE = \"s3a://bronze\"\n",
    "SILVER = \"s3a://silver\"\n",
    "GOLD = \"s3a://gold\"\n",
    "\n",
    "DATE_INGESTION = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "print(f\"Session Spark d√©marr√©e (Patch '60s' appliqu√©).\")\n",
    "print(f\"Date d'ingestion: {DATE_INGESTION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d27bfe",
   "metadata": {},
   "source": [
    "## 3. BRONZE - Ingestion des donnees brutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41f3f263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingerer_table_bronze(table_name):\n",
    "    \"\"\"\n",
    "    Ingere une table PostgreSQL vers Bronze.\n",
    "    Ajoute les metadonnees d'ingestion.\n",
    "    \"\"\"\n",
    "    print(f\"Ingestion de {table_name}...\")\n",
    "    \n",
    "    # Lire depuis PostgreSQL\n",
    "    df = spark.read.jdbc(\n",
    "        url=JDBC_URL,\n",
    "        table=table_name,\n",
    "        properties=JDBC_PROPS\n",
    "    )\n",
    "    \n",
    "    # Ajouter metadonnees\n",
    "    df_bronze = df \\\n",
    "        .withColumn(\"_ingestion_timestamp\", current_timestamp()) \\\n",
    "        .withColumn(\"_source\", lit(\"postgresql\")) \\\n",
    "        .withColumn(\"_source_table\", lit(table_name))\n",
    "    \n",
    "    # Sauvegarder en Bronze\n",
    "    output_path = f\"{BRONZE}/{table_name}/date={DATE_INGESTION}\"\n",
    "    df_bronze.write.mode(\"overwrite\").parquet(output_path)\n",
    "    \n",
    "    nb_lignes = df_bronze.count()\n",
    "    print(f\"  -> {nb_lignes} lignes sauvegardees\")\n",
    "    \n",
    "    return nb_lignes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f75d19be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "ETAPE BRONZE - Ingestion des donnees brutes\n",
      "==================================================\n",
      "Ingestion de customers...\n"
     ]
    },
    {
     "ename": "NumberFormatException",
     "evalue": "For input string: \"60s\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNumberFormatException\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m stats_bronze = {}\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m tables:\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     stats_bronze[table] = \u001b[43mingerer_table_bronze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mResume Bronze:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m table, count \u001b[38;5;129;01min\u001b[39;00m stats_bronze.items():\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mingerer_table_bronze\u001b[39m\u001b[34m(table_name)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Sauvegarder en Bronze\u001b[39;00m\n\u001b[32m     22\u001b[39m output_path = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBRONZE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/date=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATE_INGESTION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43mdf_bronze\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moverwrite\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m nb_lignes = df_bronze.count()\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnb_lignes\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m lignes sauvegardees\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/spark/python/pyspark/sql/readwriter.py:2003\u001b[39m, in \u001b[36mDataFrameWriter.parquet\u001b[39m\u001b[34m(self, path, mode, partitionBy, compression)\u001b[39m\n\u001b[32m   2001\u001b[39m     \u001b[38;5;28mself\u001b[39m.partitionBy(partitionBy)\n\u001b[32m   2002\u001b[39m \u001b[38;5;28mself\u001b[39m._set_opts(compression=compression)\n\u001b[32m-> \u001b[39m\u001b[32m2003\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mNumberFormatException\u001b[39m: For input string: \"60s\""
     ]
    }
   ],
   "source": [
    "# Ingerer les tables principales\n",
    "tables = [\"customers\", \"orders\", \"order_details\", \"products\", \"categories\", \"employees\"]\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"ETAPE BRONZE - Ingestion des donnees brutes\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "stats_bronze = {}\n",
    "for table in tables:\n",
    "    stats_bronze[table] = ingerer_table_bronze(table)\n",
    "\n",
    "print(\"\\nResume Bronze:\")\n",
    "for table, count in stats_bronze.items():\n",
    "    print(f\"  {table}: {count} lignes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d40c76",
   "metadata": {},
   "source": [
    "## 4. SILVER - Nettoyage et transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd4854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lire les donnees Bronze\n",
    "df_customers_raw = spark.read.parquet(f\"{BRONZE}/customers/date={DATE_INGESTION}\")\n",
    "df_orders_raw = spark.read.parquet(f\"{BRONZE}/orders/date={DATE_INGESTION}\")\n",
    "df_order_details_raw = spark.read.parquet(f\"{BRONZE}/order_details/date={DATE_INGESTION}\")\n",
    "df_products_raw = spark.read.parquet(f\"{BRONZE}/products/date={DATE_INGESTION}\")\n",
    "df_categories_raw = spark.read.parquet(f\"{BRONZE}/categories/date={DATE_INGESTION}\")\n",
    "df_employees_raw = spark.read.parquet(f\"{BRONZE}/employees/date={DATE_INGESTION}\")\n",
    "\n",
    "print(\"Donnees Bronze chargees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbb26ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"ETAPE SILVER - Nettoyage et transformation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# --- DIM_CUSTOMERS ---\n",
    "print(\"\\nCreation de dim_customers...\")\n",
    "\n",
    "df_dim_customers = df_customers_raw \\\n",
    "    .select(\n",
    "        col(\"customer_id\"),\n",
    "        initcap(col(\"company_name\")).alias(\"company_name\"),\n",
    "        initcap(col(\"contact_name\")).alias(\"contact_name\"),\n",
    "        col(\"contact_title\"),\n",
    "        trim(col(\"address\")).alias(\"address\"),\n",
    "        initcap(col(\"city\")).alias(\"city\"),\n",
    "        upper(col(\"region\")).alias(\"region\"),\n",
    "        col(\"postal_code\"),\n",
    "        initcap(col(\"country\")).alias(\"country\"),\n",
    "        col(\"phone\"),\n",
    "        col(\"fax\")\n",
    "    ) \\\n",
    "    .withColumn(\"_processed_timestamp\", current_timestamp()) \\\n",
    "    .dropDuplicates([\"customer_id\"])\n",
    "\n",
    "df_dim_customers.write.mode(\"overwrite\").parquet(f\"{SILVER}/dim_customers\")\n",
    "print(f\"  -> {df_dim_customers.count()} clients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b43a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DIM_PRODUCTS ---\n",
    "print(\"\\nCreation de dim_products...\")\n",
    "\n",
    "df_dim_products = df_products_raw \\\n",
    "    .join(df_categories_raw, \"category_id\", \"left\") \\\n",
    "    .select(\n",
    "        col(\"product_id\"),\n",
    "        initcap(df_products_raw[\"product_name\"]).alias(\"product_name\"),\n",
    "        col(\"category_id\"),\n",
    "        initcap(col(\"category_name\")).alias(\"category_name\"),\n",
    "        col(\"quantity_per_unit\"),\n",
    "        col(\"unit_price\"),\n",
    "        col(\"units_in_stock\"),\n",
    "        col(\"units_on_order\"),\n",
    "        col(\"reorder_level\"),\n",
    "        col(\"discontinued\"),\n",
    "        # Indicateurs derives\n",
    "        when(col(\"units_in_stock\") < col(\"reorder_level\"), lit(\"Critique\"))\n",
    "            .when(col(\"units_in_stock\") < col(\"reorder_level\") * 2, lit(\"Bas\"))\n",
    "            .otherwise(lit(\"Normal\")).alias(\"stock_status\"),\n",
    "        when(col(\"unit_price\") >= 50, lit(\"Premium\"))\n",
    "            .when(col(\"unit_price\") >= 20, lit(\"Standard\"))\n",
    "            .otherwise(lit(\"Budget\")).alias(\"price_segment\")\n",
    "    ) \\\n",
    "    .withColumn(\"_processed_timestamp\", current_timestamp()) \\\n",
    "    .dropDuplicates([\"product_id\"])\n",
    "\n",
    "df_dim_products.write.mode(\"overwrite\").parquet(f\"{SILVER}/dim_products\")\n",
    "print(f\"  -> {df_dim_products.count()} produits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789e710f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DIM_EMPLOYEES ---\n",
    "print(\"\\nCreation de dim_employees...\")\n",
    "\n",
    "df_dim_employees = df_employees_raw \\\n",
    "    .select(\n",
    "        col(\"employee_id\"),\n",
    "        initcap(col(\"first_name\")).alias(\"first_name\"),\n",
    "        initcap(col(\"last_name\")).alias(\"last_name\"),\n",
    "        col(\"title\"),\n",
    "        col(\"birth_date\"),\n",
    "        col(\"hire_date\"),\n",
    "        initcap(col(\"city\")).alias(\"city\"),\n",
    "        initcap(col(\"country\")).alias(\"country\"),\n",
    "        col(\"reports_to\")\n",
    "    ) \\\n",
    "    .withColumn(\"full_name\", \n",
    "        concat_ws(\" \", col(\"first_name\"), col(\"last_name\"))\n",
    "    ) \\\n",
    "    .withColumn(\"_processed_timestamp\", current_timestamp())\n",
    "\n",
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "df_dim_employees = df_employees_raw \\\n",
    "    .select(\n",
    "        col(\"employee_id\"),\n",
    "        initcap(col(\"first_name\")).alias(\"first_name\"),\n",
    "        initcap(col(\"last_name\")).alias(\"last_name\"),\n",
    "        col(\"title\"),\n",
    "        col(\"hire_date\"),\n",
    "        initcap(col(\"city\")).alias(\"city\"),\n",
    "        initcap(col(\"country\")).alias(\"country\")\n",
    "    ) \\\n",
    "    .withColumn(\"full_name\", concat_ws(\" \", col(\"first_name\"), col(\"last_name\"))) \\\n",
    "    .withColumn(\"_processed_timestamp\", current_timestamp())\n",
    "\n",
    "df_dim_employees.write.mode(\"overwrite\").parquet(f\"{SILVER}/dim_employees\")\n",
    "print(f\"  -> {df_dim_employees.count()} employes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5633ca75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FACT_ORDERS ---\n",
    "print(\"\\nCreation de fact_orders...\")\n",
    "\n",
    "df_fact_orders = df_orders_raw \\\n",
    "    .join(df_order_details_raw, \"order_id\", \"inner\") \\\n",
    "    .select(\n",
    "        col(\"order_id\"),\n",
    "        col(\"customer_id\"),\n",
    "        col(\"employee_id\"),\n",
    "        col(\"order_date\"),\n",
    "        col(\"required_date\"),\n",
    "        col(\"shipped_date\"),\n",
    "        col(\"product_id\"),\n",
    "        col(\"unit_price\"),\n",
    "        col(\"quantity\"),\n",
    "        col(\"discount\"),\n",
    "        # Montants calcules\n",
    "        (col(\"unit_price\") * col(\"quantity\")).alias(\"montant_brut\"),\n",
    "        (col(\"unit_price\") * col(\"quantity\") * (1 - col(\"discount\"))).alias(\"montant_net\"),\n",
    "        (col(\"unit_price\") * col(\"quantity\") * col(\"discount\")).alias(\"remise\"),\n",
    "        # Dimensions temporelles\n",
    "        year(col(\"order_date\")).alias(\"annee\"),\n",
    "        month(col(\"order_date\")).alias(\"mois\"),\n",
    "        dayofmonth(col(\"order_date\")).alias(\"jour\")\n",
    "    ) \\\n",
    "    .withColumn(\"_processed_timestamp\", current_timestamp())\n",
    "\n",
    "df_fact_orders.write.mode(\"overwrite\").parquet(f\"{SILVER}/fact_orders\")\n",
    "print(f\"  -> {df_fact_orders.count()} lignes de commande\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ecd0d3",
   "metadata": {},
   "source": [
    "## 5. GOLD - Agregations et KPIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6ab0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les donnees Silver\n",
    "df_customers = spark.read.parquet(f\"{SILVER}/dim_customers\")\n",
    "df_products = spark.read.parquet(f\"{SILVER}/dim_products\")\n",
    "df_employees = spark.read.parquet(f\"{SILVER}/dim_employees\")\n",
    "df_orders = spark.read.parquet(f\"{SILVER}/fact_orders\")\n",
    "\n",
    "print(\"Donnees Silver chargees\")\n",
    "print(\"=\" * 50)\n",
    "print(\"ETAPE GOLD - Agregations et KPIs\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521e64ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- KPI VENTES MENSUELLES ---\n",
    "print(\"\\nCreation de kpi_ventes_mensuelles...\")\n",
    "\n",
    "df_kpi_ventes = df_orders \\\n",
    "    .groupBy(\"annee\", \"mois\") \\\n",
    "    .agg(\n",
    "        count(\"order_id\").alias(\"nb_commandes\"),\n",
    "        countDistinct(\"customer_id\").alias(\"nb_clients\"),\n",
    "        spark_sum(\"montant_net\").alias(\"ca_total\"),\n",
    "        avg(\"montant_net\").alias(\"panier_moyen\"),\n",
    "        spark_sum(\"quantity\").alias(\"quantite_totale\"),\n",
    "        spark_sum(\"remise\").alias(\"remises_totales\")\n",
    "    ) \\\n",
    "    .withColumn(\"ca_total\", spark_round(col(\"ca_total\"), 2)) \\\n",
    "    .withColumn(\"panier_moyen\", spark_round(col(\"panier_moyen\"), 2)) \\\n",
    "    .withColumn(\"remises_totales\", spark_round(col(\"remises_totales\"), 2)) \\\n",
    "    .orderBy(\"annee\", \"mois\") \\\n",
    "    .withColumn(\"_generated_timestamp\", current_timestamp())\n",
    "\n",
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "df_kpi_ventes = df_orders \\\n",
    "    .groupBy(\"annee\", \"mois\") \\\n",
    "    .agg(\n",
    "        count(\"order_id\").alias(\"nb_commandes\"),\n",
    "        countDistinct(\"customer_id\").alias(\"nb_clients\"),\n",
    "        spark_sum(\"montant_net\").alias(\"ca_total\"),\n",
    "        avg(\"montant_net\").alias(\"panier_moyen\"),\n",
    "        spark_sum(\"quantity\").alias(\"quantite_totale\"),\n",
    "        spark_sum(\"remise\").alias(\"remises_totales\")\n",
    "    ) \\\n",
    "    .withColumn(\"ca_total\", spark_round(col(\"ca_total\"), 2)) \\\n",
    "    .withColumn(\"panier_moyen\", spark_round(col(\"panier_moyen\"), 2)) \\\n",
    "    .withColumn(\"remises_totales\", spark_round(col(\"remises_totales\"), 2)) \\\n",
    "    .orderBy(\"annee\", \"mois\") \\\n",
    "    .withColumn(\"_generated_timestamp\", current_timestamp())\n",
    "\n",
    "df_kpi_ventes.write.mode(\"overwrite\").parquet(f\"{GOLD}/kpi_ventes_mensuelles\")\n",
    "print(f\"  -> {df_kpi_ventes.count()} mois\")\n",
    "df_kpi_ventes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b6d1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ANALYSE CLIENTS ---\n",
    "print(\"\\nCreation de analyse_clients...\")\n",
    "\n",
    "df_analyse_clients = df_orders \\\n",
    "    .groupBy(\"customer_id\") \\\n",
    "    .agg(\n",
    "        count(\"order_id\").alias(\"nb_commandes\"),\n",
    "        spark_sum(\"montant_net\").alias(\"ca_total\"),\n",
    "        avg(\"montant_net\").alias(\"panier_moyen\"),\n",
    "        spark_min(\"order_date\").alias(\"premiere_commande\"),\n",
    "        spark_max(\"order_date\").alias(\"derniere_commande\")\n",
    "    ) \\\n",
    "    .join(df_customers.select(\"customer_id\", \"company_name\", \"country\", \"city\"), \"customer_id\") \\\n",
    "    .withColumn(\"ca_total\", spark_round(col(\"ca_total\"), 2)) \\\n",
    "    .withColumn(\"panier_moyen\", spark_round(col(\"panier_moyen\"), 2))\n",
    "\n",
    "# Segmentation RFM simplifiee\n",
    "window_ca = Window.orderBy(col(\"ca_total\").desc())\n",
    "\n",
    "df_analyse_clients = df_analyse_clients \\\n",
    "    .withColumn(\"rang_ca\", dense_rank().over(window_ca)) \\\n",
    "    .withColumn(\"segment\",\n",
    "        when(col(\"rang_ca\") <= 10, lit(\"VIP\"))\n",
    "        .when(col(\"rang_ca\") <= 30, lit(\"Premium\"))\n",
    "        .when(col(\"rang_ca\") <= 60, lit(\"Standard\"))\n",
    "        .otherwise(lit(\"Occasionnel\"))\n",
    "    ) \\\n",
    "    .withColumn(\"_generated_timestamp\", current_timestamp())\n",
    "\n",
    "df_analyse_clients.write.mode(\"overwrite\").parquet(f\"{GOLD}/analyse_clients\")\n",
    "print(f\"  -> {df_analyse_clients.count()} clients analyses\")\n",
    "df_analyse_clients.select(\n",
    "    \"company_name\", \"country\", \"nb_commandes\", \"ca_total\", \"segment\"\n",
    ").orderBy(col(\"ca_total\").desc()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c733fe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- RAPPORT PRODUITS ---\n",
    "print(\"\\nCreation de rapport_produits...\")\n",
    "\n",
    "df_rapport_produits = df_orders \\\n",
    "    .groupBy(\"product_id\") \\\n",
    "    .agg(\n",
    "        count(\"order_id\").alias(\"nb_ventes\"),\n",
    "        spark_sum(\"quantity\").alias(\"quantite_vendue\"),\n",
    "        spark_sum(\"montant_net\").alias(\"ca_produit\"),\n",
    "        avg(\"unit_price\").alias(\"prix_moyen\")\n",
    "    ) \\\n",
    "    .join(df_products.select(\n",
    "        \"product_id\", \"product_name\", \"category_name\", \n",
    "        \"stock_status\", \"price_segment\", \"units_in_stock\"\n",
    "    ), \"product_id\") \\\n",
    "    .withColumn(\"ca_produit\", spark_round(col(\"ca_produit\"), 2)) \\\n",
    "    .withColumn(\"prix_moyen\", spark_round(col(\"prix_moyen\"), 2))\n",
    "\n",
    "# Ranking par categorie\n",
    "window_cat = Window.partitionBy(\"category_name\").orderBy(col(\"ca_produit\").desc())\n",
    "\n",
    "df_rapport_produits = df_rapport_produits \\\n",
    "    .withColumn(\"rang_categorie\", row_number().over(window_cat)) \\\n",
    "    .withColumn(\"_generated_timestamp\", current_timestamp())\n",
    "\n",
    "df_rapport_produits.write.mode(\"overwrite\").parquet(f\"{GOLD}/rapport_produits\")\n",
    "print(f\"  -> {df_rapport_produits.count()} produits analyses\")\n",
    "df_rapport_produits.select(\n",
    "    \"product_name\", \"category_name\", \"quantite_vendue\", \"ca_produit\", \"rang_categorie\"\n",
    ").orderBy(col(\"ca_produit\").desc()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9482c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PERFORMANCE EMPLOYES ---\n",
    "print(\"\\nCreation de performance_employes...\")\n",
    "\n",
    "df_perf_employes = df_orders \\\n",
    "    .groupBy(\"employee_id\") \\\n",
    "    .agg(\n",
    "        countDistinct(\"order_id\").alias(\"nb_commandes\"),\n",
    "        countDistinct(\"customer_id\").alias(\"nb_clients\"),\n",
    "        spark_sum(\"montant_net\").alias(\"ca_genere\"),\n",
    "        avg(\"montant_net\").alias(\"ca_moyen_ligne\")\n",
    "    ) \\\n",
    "    .join(df_employees.select(\"employee_id\", \"full_name\", \"title\", \"city\"), \"employee_id\") \\\n",
    "    .withColumn(\"ca_genere\", spark_round(col(\"ca_genere\"), 2)) \\\n",
    "    .withColumn(\"ca_moyen_ligne\", spark_round(col(\"ca_moyen_ligne\"), 2)) \\\n",
    "    .withColumn(\"_generated_timestamp\", current_timestamp())\n",
    "\n",
    "df_perf_employes.write.mode(\"overwrite\").parquet(f\"{GOLD}/performance_employes\")\n",
    "print(f\"  -> {df_perf_employes.count()} employes\")\n",
    "df_perf_employes.orderBy(col(\"ca_genere\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbb4858",
   "metadata": {},
   "source": [
    "## 6. Verification du pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5170ea69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"VERIFICATION DU PIPELINE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Verifier Bronze\n",
    "print(\"\\nBRONZE:\")\n",
    "for table in tables:\n",
    "    try:\n",
    "        count = spark.read.parquet(f\"{BRONZE}/{table}/date={DATE_INGESTION}\").count()\n",
    "        print(f\"  {table}: {count} lignes\")\n",
    "    except:\n",
    "        print(f\"  {table}: ERREUR\")\n",
    "\n",
    "# Verifier Silver\n",
    "print(\"\\nSILVER:\")\n",
    "for dim in [\"dim_customers\", \"dim_products\", \"dim_employees\", \"fact_orders\"]:\n",
    "    try:\n",
    "        count = spark.read.parquet(f\"{SILVER}/{dim}\").count()\n",
    "        print(f\"  {dim}: {count} lignes\")\n",
    "    except:\n",
    "        print(f\"  {dim}: ERREUR\")\n",
    "\n",
    "# Verifier Gold\n",
    "print(\"\\nGOLD:\")\n",
    "for kpi in [\"kpi_ventes_mensuelles\", \"analyse_clients\", \"rapport_produits\", \"performance_employes\"]:\n",
    "    try:\n",
    "        count = spark.read.parquet(f\"{GOLD}/{kpi}\").count()\n",
    "        print(f\"  {kpi}: {count} lignes\")\n",
    "    except:\n",
    "        print(f\"  {kpi}: ERREUR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9855ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume executif\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RESUME EXECUTIF\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# KPIs globaux\n",
    "df_kpi = spark.read.parquet(f\"{GOLD}/kpi_ventes_mensuelles\")\n",
    "total_ca = df_kpi.agg(spark_sum(\"ca_total\")).collect()[0][0]\n",
    "total_commandes = df_kpi.agg(spark_sum(\"nb_commandes\")).collect()[0][0]\n",
    "\n",
    "df_clients = spark.read.parquet(f\"{GOLD}/analyse_clients\")\n",
    "nb_clients = df_clients.count()\n",
    "nb_vip = df_clients.filter(col(\"segment\") == \"VIP\").count()\n",
    "\n",
    "print(f\"\\nChiffre d'affaires total: {total_ca:,.2f} EUR\")\n",
    "print(f\"Nombre de commandes: {total_commandes:,}\")\n",
    "print(f\"Nombre de clients: {nb_clients}\")\n",
    "print(f\"Clients VIP: {nb_vip}\")\n",
    "print(f\"\\nPipeline execute le: {DATE_INGESTION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b182e71",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercice\n",
    "\n",
    "**Objectif** : Etendre le pipeline\n",
    "\n",
    "**Consigne** :\n",
    "1. Ajoutez une table Gold \"analyse_geographique\" avec le CA par pays\n",
    "2. Calculez la croissance mensuelle du CA\n",
    "3. Identifiez le top 3 des pays par CA\n",
    "\n",
    "A vous de jouer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe2de08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Creer analyse_geographique\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import sum as spark_sum, col, desc, lag, round as spark_round, current_timestamp, count\n",
    "\n",
    "# --- 1. Cr√©ation de la table Gold : Analyse G√©ographique ---\n",
    "print(\"Construction de l'analyse g√©ographique...\")\n",
    "\n",
    "# On a besoin de joindre les Ventes (Fact) avec les Clients (Dim) pour r√©cup√©rer le pays\n",
    "df_geo_analysis = df_orders.join(df_customers, \"customer_id\") \\\n",
    "    .groupBy(\"country\") \\\n",
    "    .agg(\n",
    "        spark_sum(\"montant_net\").alias(\"ca_total\"),\n",
    "        count(\"order_id\").alias(\"nb_commandes\"),\n",
    "        avg(\"montant_net\").alias(\"panier_moyen\")\n",
    "    ) \\\n",
    "    .withColumn(\"ca_total\", spark_round(col(\"ca_total\"), 2)) \\\n",
    "    .withColumn(\"panier_moyen\", spark_round(col(\"panier_moyen\"), 2)) \\\n",
    "    .withColumn(\"_generated_at\", current_timestamp())\n",
    "\n",
    "# Sauvegarde dans le Data Lake (Gold)\n",
    "df_geo_analysis.write.mode(\"overwrite\").parquet(f\"{GOLD}/analyse_geographique\")\n",
    "print(f\"Table 'analyse_geographique' sauvegard√©e.\")\n",
    "\n",
    "\n",
    "# --- 2. Top 3 des Pays par CA ---\n",
    "print(\"\\n--- TOP 3 PAYS PAR CHIFFRE D'AFFAIRES ---\")\n",
    "df_geo_analysis.select(\"country\", \"nb_commandes\", \"ca_total\") \\\n",
    "    .orderBy(desc(\"ca_total\")) \\\n",
    "    .show(3)\n",
    "\n",
    "\n",
    "# --- 3. Calcul de la Croissance Mensuelle (Window Function) ---\n",
    "print(\"\\n--- CROISSANCE MENSUELLE DU CA ---\")\n",
    "\n",
    "# Pour cela, on r√©utilise la table 'kpi_ventes_mensuelles' qu'on a cr√©√©e plus haut\n",
    "df_mensuel = spark.read.parquet(f\"{GOLD}/kpi_ventes_mensuelles\")\n",
    "\n",
    "# On d√©finit une fen√™tre ordonn√©e par temps pour comparer avec le mois pr√©c√©dent\n",
    "window_spec = Window.orderBy(\"annee\", \"mois\")\n",
    "\n",
    "df_croissance = df_mensuel.withColumn(\n",
    "    \"ca_precedent\", \n",
    "    lag(\"ca_total\", 1).over(window_spec)\n",
    ").withColumn(\n",
    "    \"croissance_pct\", \n",
    "    spark_round(((col(\"ca_total\") - col(\"ca_precedent\")) / col(\"ca_precedent\")) * 100, 2)\n",
    ").select(\"annee\", \"mois\", \"ca_total\", \"ca_precedent\", \"croissance_pct\")\n",
    "\n",
    "df_croissance.orderBy(\"annee\", \"mois\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9e0283",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Resume\n",
    "\n",
    "Dans ce notebook, vous avez appris :\n",
    "- Comment construire un **pipeline ETL complet**\n",
    "- L'architecture **Medallion** (Bronze/Silver/Gold)\n",
    "- L'**ingestion** depuis PostgreSQL vers Bronze\n",
    "- Le **nettoyage et transformation** vers Silver\n",
    "- La creation de **KPIs et agregations** dans Gold\n",
    "- La **verification** du pipeline\n",
    "\n",
    "Ce pipeline est la base de toute architecture Data Lake moderne."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
