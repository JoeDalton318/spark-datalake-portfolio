{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9364b4b7",
   "metadata": {},
   "source": [
    "# Exercice 03 - Decouverte de Spark\n",
    "\n",
    "## Objectifs\n",
    "- Comprendre ce qu'est Apache Spark\n",
    "- Creer une SparkSession\n",
    "- Comprendre la difference entre Spark et Pandas\n",
    "- Creer votre premier DataFrame Spark\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19f5277",
   "metadata": {},
   "source": [
    "## 1. Qu'est-ce que Apache Spark ?\n",
    "\n",
    "**Apache Spark** est un moteur de traitement de donnees distribue.\n",
    "\n",
    "```\n",
    "+------------------------------------------------------------------+\n",
    "|                    TRAITEMENT PANDAS                             |\n",
    "|                                                                  |\n",
    "|   +----------+          +------------------+                     |\n",
    "|   | Donnees  | -------> | 1 seul processeur| -----> Resultat     |\n",
    "|   +----------+          +------------------+                     |\n",
    "|                                                                  |\n",
    "|   Limite : tout doit tenir en memoire RAM                        |\n",
    "+------------------------------------------------------------------+\n",
    "\n",
    "+------------------------------------------------------------------+\n",
    "|                    TRAITEMENT SPARK                              |\n",
    "|                                                                  |\n",
    "|   +----------+     +-------+  +-------+  +-------+               |\n",
    "|   | Donnees  | --> | CPU 1 |  | CPU 2 |  | CPU 3 | --> Resultat  |\n",
    "|   | (huge)   |     +-------+  +-------+  +-------+               |\n",
    "|   +----------+         |          |          |                   |\n",
    "|                        +----------+----------+                   |\n",
    "|                               |                                  |\n",
    "|                        Coordination                              |\n",
    "|                                                                  |\n",
    "|   Avantage : distribue le travail sur plusieurs machines         |\n",
    "+------------------------------------------------------------------+\n",
    "```\n",
    "\n",
    "### Caracteristiques principales\n",
    "- **Distribue** : repartit le travail sur plusieurs machines\n",
    "- **En memoire** : traite les donnees en RAM (tres rapide)\n",
    "- **Polyvalent** : SQL, Machine Learning, Streaming, Graphes\n",
    "- **Compatible** : lit CSV, JSON, Parquet, bases de donnees..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd55f91",
   "metadata": {},
   "source": [
    "## 2. Spark vs Pandas\n",
    "\n",
    "| Critere | Pandas | Spark |\n",
    "|---------|--------|-------|\n",
    "| Execution | 1 machine | Cluster de machines |\n",
    "| Taille donnees | < 10 Go | Petaoctets |\n",
    "| Vitesse petits fichiers | Tres rapide | Plus lent (overhead) |\n",
    "| Vitesse gros fichiers | Lent/impossible | Tres rapide |\n",
    "| API | DataFrame | DataFrame |\n",
    "| Langage | Python | Python, Scala, Java, R |\n",
    "\n",
    "### Quand utiliser quoi ?\n",
    "- **Pandas** : fichiers < 1 Go, exploration rapide, prototypage\n",
    "- **Spark** : fichiers > 1 Go, production, pipelines Data Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4decfc83",
   "metadata": {},
   "source": [
    "## 3. La SparkSession\n",
    "\n",
    "La **SparkSession** est le point d'entree pour utiliser Spark.\n",
    "\n",
    "C'est comme ouvrir une connexion a Spark avant de pouvoir travailler.\n",
    "\n",
    "```\n",
    "+------------------------------------------------------------------+\n",
    "|                                                                  |\n",
    "|   Votre code Python                                              |\n",
    "|        |                                                         |\n",
    "|        v                                                         |\n",
    "|   +--------------+                                               |\n",
    "|   | SparkSession | <-- Point d'entree unique                     |\n",
    "|   +--------------+                                               |\n",
    "|        |                                                         |\n",
    "|        v                                                         |\n",
    "|   +------------------+                                           |\n",
    "|   | Moteur Spark     |                                           |\n",
    "|   | (distribue)      |                                           |\n",
    "|   +------------------+                                           |\n",
    "|                                                                  |\n",
    "+------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07299241",
   "metadata": {},
   "source": [
    "## 4. Creer une SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3322d5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession creee avec succes !\n",
      "Version Spark : 4.0.1\n"
     ]
    }
   ],
   "source": [
    "# Importer SparkSession depuis le module pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Creer une SparkSession\n",
    "# - appName : nom de votre application (visible dans l'interface Spark)\n",
    "# - getOrCreate : reutilise une session existante ou en cree une nouvelle\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Mon premier Spark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"SparkSession creee avec succes !\")\n",
    "print(\"Version Spark :\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62d87ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Application : Mon premier Spark\n",
      "Master : local[*]\n"
     ]
    }
   ],
   "source": [
    "# Afficher les informations de la session\n",
    "print(\"Application :\", spark.sparkContext.appName)\n",
    "print(\"Master :\", spark.sparkContext.master)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ebe337",
   "metadata": {},
   "source": [
    "## 5. Creer un DataFrame Spark\n",
    "\n",
    "Un **DataFrame Spark** est similaire a un DataFrame Pandas, mais distribue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b37c8967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---------+\n",
      "|    nom|age|    ville|\n",
      "+-------+---+---------+\n",
      "|  Alice| 30|    Paris|\n",
      "|    Bob| 25|     Lyon|\n",
      "|Charlie| 35|Marseille|\n",
      "|  Diana| 28| Bordeaux|\n",
      "+-------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creer un DataFrame a partir d'une liste de tuples\n",
    "donnees = [\n",
    "    (\"Alice\", 30, \"Paris\"),\n",
    "    (\"Bob\", 25, \"Lyon\"),\n",
    "    (\"Charlie\", 35, \"Marseille\"),\n",
    "    (\"Diana\", 28, \"Bordeaux\")\n",
    "]\n",
    "\n",
    "# Definir les noms des colonnes\n",
    "colonnes = [\"nom\", \"age\", \"ville\"]\n",
    "\n",
    "# Creer le DataFrame\n",
    "df = spark.createDataFrame(donnees, colonnes)\n",
    "\n",
    "# Afficher le DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb4f4615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nom: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- ville: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Afficher le schema (structure) du DataFrame\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "210df7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes : 4\n"
     ]
    }
   ],
   "source": [
    "# Compter le nombre de lignes\n",
    "print(\"Nombre de lignes :\", df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b731ac2",
   "metadata": {},
   "source": [
    "## 6. Operations de base sur un DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98cdf261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|    nom|    ville|\n",
      "+-------+---------+\n",
      "|  Alice|    Paris|\n",
      "|    Bob|     Lyon|\n",
      "|Charlie|Marseille|\n",
      "|  Diana| Bordeaux|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selectionner certaines colonnes\n",
    "df.select(\"nom\", \"ville\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60ddb510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---------+\n",
      "|    nom|age|    ville|\n",
      "+-------+---+---------+\n",
      "|  Alice| 30|    Paris|\n",
      "|Charlie| 35|Marseille|\n",
      "|  Diana| 28| Bordeaux|\n",
      "+-------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtrer les lignes\n",
    "df.filter(df.age > 27).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b1b0005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---------+\n",
      "|    nom|age|    ville|\n",
      "+-------+---+---------+\n",
      "|    Bob| 25|     Lyon|\n",
      "|  Diana| 28| Bordeaux|\n",
      "|  Alice| 30|    Paris|\n",
      "|Charlie| 35|Marseille|\n",
      "+-------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Trier par une colonne\n",
    "df.orderBy(\"age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d3f05e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---------+---------------+\n",
      "|    nom|age|    ville|age_dans_10_ans|\n",
      "+-------+---+---------+---------------+\n",
      "|  Alice| 30|    Paris|             40|\n",
      "|    Bob| 25|     Lyon|             35|\n",
      "|Charlie| 35|Marseille|             45|\n",
      "|  Diana| 28| Bordeaux|             38|\n",
      "+-------+---+---------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ajouter une nouvelle colonne\n",
    "df_avec_bonus = df.withColumn(\"age_dans_10_ans\", df.age + 10)\n",
    "df_avec_bonus.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acac3f3",
   "metadata": {},
   "source": [
    "## 7. Difference importante : Lazy Evaluation\n",
    "\n",
    "Spark utilise l'**evaluation paresseuse** (lazy evaluation) :\n",
    "- Les transformations (select, filter, etc.) ne sont PAS executees immediatement\n",
    "- Elles sont executees seulement quand on demande un resultat (show, count, collect)\n",
    "\n",
    "```\n",
    "TRANSFORMATIONS (lazy)          ACTIONS (declenchent l'execution)\n",
    "--------------------            ---------------------------------\n",
    "select()                        show()\n",
    "filter()                        count()\n",
    "withColumn()                    collect()\n",
    "orderBy()                       write()\n",
    "groupBy()                       first()\n",
    "```\n",
    "\n",
    "### Pourquoi ?\n",
    "Spark peut optimiser l'ensemble des transformations avant de les executer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3488d738",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercice\n",
    "\n",
    "**Objectif** : Creer et manipuler un DataFrame Spark\n",
    "\n",
    "**Consigne** :\n",
    "1. Creez un DataFrame avec des donnees de produits (nom, prix, quantite)\n",
    "2. Affichez le schema\n",
    "3. Filtrez les produits dont le prix est superieur a 20\n",
    "4. Ajoutez une colonne \"valeur_stock\" = prix * quantite\n",
    "5. Triez par valeur_stock decroissante\n",
    "\n",
    "A vous de jouer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd0376d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+--------+\n",
      "|      nom|  prix|quantite|\n",
      "+---------+------+--------+\n",
      "|   Laptop|999.99|      10|\n",
      "|   Souris| 29.99|     100|\n",
      "|  Clavier| 79.99|      50|\n",
      "|    Ecran|299.99|      25|\n",
      "|Cable USB|  9.99|     200|\n",
      "+---------+------+--------+\n",
      "\n",
      "root\n",
      " |-- nom: string (nullable = true)\n",
      " |-- prix: double (nullable = true)\n",
      " |-- quantite: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Donnees de produits\n",
    "produits = [\n",
    "    (\"Laptop\", 999.99, 10),\n",
    "    (\"Souris\", 29.99, 100),\n",
    "    (\"Clavier\", 79.99, 50),\n",
    "    (\"Ecran\", 299.99, 25),\n",
    "    (\"Cable USB\", 9.99, 200)\n",
    "]\n",
    "\n",
    "colonnes = [\"nom\", \"prix\", \"quantite\"]\n",
    "\n",
    "# TODO: Creez le DataFrame\n",
    "df_produits = spark.createDataFrame(produits, colonnes)\n",
    "df_produits.show()\n",
    "\n",
    "# TODO: Affichez le schema\n",
    "df_produits.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c27615db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------+\n",
      "|    nom|  prix|quantite|\n",
      "+-------+------+--------+\n",
      "| Laptop|999.99|      10|\n",
      "| Souris| 29.99|     100|\n",
      "|Clavier| 79.99|      50|\n",
      "|  Ecran|299.99|      25|\n",
      "+-------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Filtrez les produits avec prix > 20\n",
    "df_produits.filter(df_produits.prix > 20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23183583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+--------+------------------+\n",
      "|      nom|  prix|quantite|      valeur_stock|\n",
      "+---------+------+--------+------------------+\n",
      "|   Laptop|999.99|      10|            9999.9|\n",
      "|    Ecran|299.99|      25|           7499.75|\n",
      "|  Clavier| 79.99|      50|3999.4999999999995|\n",
      "|   Souris| 29.99|     100|            2999.0|\n",
      "|Cable USB|  9.99|     200|            1998.0|\n",
      "+---------+------+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Ajoutez la colonne valeur_stock et triez\n",
    "# Ajout de la colonne valeur_stock\n",
    "new_df_produits = df_produits.withColumn(\n",
    "    \"valeur_stock\",\n",
    "    df_produits.prix * df_produits.quantite\n",
    ")\n",
    "\n",
    "# Tri d√©croissant\n",
    "new_df_produits_sorted = new_df_produits.orderBy(\"valeur_stock\", ascending=False)\n",
    "\n",
    "# Affichage\n",
    "new_df_produits_sorted.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f38bf7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Resume\n",
    "\n",
    "Dans ce notebook, vous avez appris :\n",
    "- **Apache Spark** est un moteur de traitement distribue\n",
    "- La **SparkSession** est le point d'entree pour utiliser Spark\n",
    "- Un **DataFrame Spark** est similaire a Pandas mais distribue\n",
    "- Les operations de base : select, filter, orderBy, withColumn\n",
    "- La **lazy evaluation** : les transformations ne s'executent qu'a l'action\n",
    "\n",
    "### Prochaine etape\n",
    "Dans le prochain notebook, nous apprendrons a lire et ecrire des fichiers avec Spark."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
